@ARTICLE{Chen2020-uz,
  title         = "A simple framework for contrastive learning of visual
                   representations",
  author        = "Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and
                   Hinton, Geoffrey",
  abstract      = "This paper presents SimCLR: a simple framework for
                   contrastive learning of visual representations. We simplify
                   recently proposed contrastive self-supervised learning
                   algorithms without requiring specialized architectures or a
                   memory bank. In order to understand what enables the
                   contrastive prediction tasks to learn useful
                   representations, we systematically study the major
                   components of our framework. We show that (1) composition of
                   data augmentations plays a critical role in defining
                   effective predictive tasks, (2) introducing a learnable
                   nonlinear transformation between the representation and the
                   contrastive loss substantially improves the quality of the
                   learned representations, and (3) contrastive learning
                   benefits from larger batch sizes and more training steps
                   compared to supervised learning. By combining these
                   findings, we are able to considerably outperform previous
                   methods for self-supervised and semi-supervised learning on
                   ImageNet. A linear classifier trained on self-supervised
                   representations learned by SimCLR achieves 76.5\% top-1
                   accuracy, which is a 7\% relative improvement over previous
                   state-of-the-art, matching the performance of a supervised
                   ResNet-50. When fine-tuned on only 1\% of the labels, we
                   achieve 85.8\% top-5 accuracy, outperforming AlexNet with
                   100X fewer labels.",
  month         =  feb,
  year          =  2020,
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2002.05709"
}

@ARTICLE{Li2017-dk,
  title         = "Visualizing the loss landscape of neural nets",
  author        = "Li, Hao and Xu, Zheng and Taylor, Gavin and Studer,
                   Christoph and Goldstein, Tom",
  abstract      = "Neural network training relies on our ability to find
                   ``good'' minimizers of highly non-convex loss functions. It
                   is well-known that certain network architecture designs
                   (e.g., skip connections) produce loss functions that train
                   easier, and well-chosen training parameters (batch size,
                   learning rate, optimizer) produce minimizers that generalize
                   better. However, the reasons for these differences, and
                   their effects on the underlying loss landscape, are not well
                   understood. In this paper, we explore the structure of
                   neural loss functions, and the effect of loss landscapes on
                   generalization, using a range of visualization methods.
                   First, we introduce a simple ``filter normalization'' method
                   that helps us visualize loss function curvature and make
                   meaningful side-by-side comparisons between loss functions.
                   Then, using a variety of visualizations, we explore how
                   network architecture affects the loss landscape, and how
                   training parameters affect the shape of minimizers.",
  month         =  dec,
  year          =  2017,
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1712.09913"
}

@ARTICLE{Tan2019-az,
  title         = "{EfficientNet}: Rethinking model scaling for convolutional
                   Neural Networks",
  author        = "Tan, Mingxing and Le, Quoc V",
  abstract      = "Convolutional Neural Networks (ConvNets) are commonly
                   developed at a fixed resource budget, and then scaled up for
                   better accuracy if more resources are available. In this
                   paper, we systematically study model scaling and identify
                   that carefully balancing network depth, width, and
                   resolution can lead to better performance. Based on this
                   observation, we propose a new scaling method that uniformly
                   scales all dimensions of depth/width/resolution using a
                   simple yet highly effective compound coefficient. We
                   demonstrate the effectiveness of this method on scaling up
                   MobileNets and ResNet. To go even further, we use neural
                   architecture search to design a new baseline network and
                   scale it up to obtain a family of models, called
                   EfficientNets, which achieve much better accuracy and
                   efficiency than previous ConvNets. In particular, our
                   EfficientNet-B7 achieves state-of-the-art 84.3\% top-1
                   accuracy on ImageNet, while being 8.4x smaller and 6.1x
                   faster on inference than the best existing ConvNet. Our
                   EfficientNets also transfer well and achieve
                   state-of-the-art accuracy on CIFAR-100 (91.7\%), Flowers
                   (98.8\%), and 3 other transfer learning datasets, with an
                   order of magnitude fewer parameters. Source code is at
                   https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet.",
  month         =  may,
  year          =  2019,
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1905.11946"
}


@ARTICLE{He2015-bw,
  title         = "Deep residual learning for image recognition",
  author        = "He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun,
                   Jian",
  abstract      = "Deeper neural networks are more difficult to train. We
                   present a residual learning framework to ease the training
                   of networks that are substantially deeper than those used
                   previously. We explicitly reformulate the layers as learning
                   residual functions with reference to the layer inputs,
                   instead of learning unreferenced functions. We provide
                   comprehensive empirical evidence showing that these residual
                   networks are easier to optimize, and can gain accuracy from
                   considerably increased depth. On the ImageNet dataset we
                   evaluate residual nets with a depth of up to 152 layers---8x
                   deeper than VGG nets but still having lower complexity. An
                   ensemble of these residual nets achieves 3.57\% error on the
                   ImageNet test set. This result won the 1st place on the
                   ILSVRC 2015 classification task. We also present analysis on
                   CIFAR-10 with 100 and 1000 layers. The depth of
                   representations is of central importance for many visual
                   recognition tasks. Solely due to our extremely deep
                   representations, we obtain a 28\% relative improvement on
                   the COCO object detection dataset. Deep residual nets are
                   foundations of our submissions to ILSVRC \& COCO 2015
                   competitions, where we also won the 1st places on the tasks
                   of ImageNet detection, ImageNet localization, COCO
                   detection, and COCO segmentation.",
  month         =  dec,
  year          =  2015,
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "1512.03385"
}

@ARTICLE{Ronneberger2015-ib,
  title         = "{U-Net}: Convolutional Networks for Biomedical Image
                   Segmentation",
  author        = "Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas",
  abstract      = "There is large consent that successful training of deep
                   networks requires many thousand annotated training samples.
                   In this paper, we present a network and training strategy
                   that relies on the strong use of data augmentation to use
                   the available annotated samples more efficiently. The
                   architecture consists of a contracting path to capture
                   context and a symmetric expanding path that enables precise
                   localization. We show that such a network can be trained
                   end-to-end from very few images and outperforms the prior
                   best method (a sliding-window convolutional network) on the
                   ISBI challenge for segmentation of neuronal structures in
                   electron microscopic stacks. Using the same network trained
                   on transmitted light microscopy images (phase contrast and
                   DIC) we won the ISBI cell tracking challenge 2015 in these
                   categories by a large margin. Moreover, the network is fast.
                   Segmentation of a 512x512 image takes less than a second on
                   a recent GPU. The full implementation (based on Caffe) and
                   the trained networks are available at
                   http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net
                   .",
  month         =  may,
  year          =  2015,
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "1505.04597"
}
