{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2617147a-5efd-4572-9ac0-6b48d1269fa3",
   "metadata": {},
   "source": [
    "## Data loading\n",
    "\n",
    "To work with any kind of data we first have to load it. For this we use a dataloader that reads the images as well as their labels and transforms them into pytorch readable tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d80171d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.io import read_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0090711b-668a-4bab-a4d7-686081d239cb",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "class RSiMCCDataset(Dataset):\n",
    "    def __init__(self, transform=None):\n",
    "        super().__init__()\n",
    "        self.transform = transform\n",
    "        # get images\n",
    "        image_files = [x.resolve() for x in pathlib.Path(\".\").glob('data/*/*')]\n",
    "        # Image.open() has a bug, this is a workaround\n",
    "        self.images=[read_image(str(p)) for p in image_files] \n",
    "        # get labels from image path\n",
    "        labels = [x.parts[-2] for x in image_files]\n",
    "        self.classes = sorted(list(set(labels)))\n",
    "        self.labels = [self.label_to_tensor(lbl) for lbl in labels]\n",
    "\n",
    "        assert len(self.labels) == len(self.images), f\"Found {len(self.labels)} labels and {len(self.images)} images\"\n",
    "\n",
    "    def label_to_tensor(self, lbl):\n",
    "        \"\"\"\n",
    "        Converts the string label to a one-hot tensor where every entry is zero except the label which is one.\n",
    "        \n",
    "        \"\"\"\n",
    "        assert lbl in self.classes, f\"Class {lbl} not a valid class (valid classes: {self.classes})\"\n",
    "        t = torch.zeros(len(self.classes))\n",
    "        t[self.classes.index(lbl)] = 1\n",
    "        return t\n",
    "\n",
    "    def tensor_to_label(self, t):\n",
    "        \"\"\"\n",
    "        Returns the classname in string format\n",
    "        \"\"\"\n",
    "        assert len(t.shape) == 1, f\"Can only convert 1-dimensional tensors (shape of tensor: {t.shape})\"\n",
    "        assert len(t) == len(self.classes), f\"Lenght of tensor ({len(t)}) does not match number of classes ({len(self.classes)})\"\n",
    "        return self.classes[t.argmax()]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.images[idx].float()/255\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, self.labels[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685aa618-6d38-4ff5-bac6-2e16f8c7a25e",
   "metadata": {},
   "source": [
    "## Examples\n",
    "Now lets load the dataset and look at some examples by just randomly loading the images and their labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "673f108f-b1c0-4ab8-a331-3a6d038d2cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = RSiMCCDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e8257cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset consists of 11519 RGB images of shape torch.Size([3, 64, 64]) labeled in 10 classes which are AnnualCrop, Forest, HerbaceousVegetation, Highway, Industrial, Pasture, PermanentCrop, Residential, River, SeaLake\n"
     ]
    }
   ],
   "source": [
    "print(f\"The dataset consists of {len(dataset)} RGB images of shape {dataset.images[0].shape} labeled in {dataset.labels[0].shape[0]} classes which are \" + ', '.join(dataset.classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "88aefed8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 48,  47,  48,  ..., 146, 145, 151],\n",
       "         [ 48,  48,  48,  ..., 146, 147, 150],\n",
       "         [ 47,  48,  46,  ..., 150, 150, 151],\n",
       "         ...,\n",
       "         [ 51,  50,  52,  ...,  69,  83,  76],\n",
       "         [ 52,  51,  49,  ...,  91,  94,  92],\n",
       "         [ 50,  51,  54,  ..., 112,  99,  78]],\n",
       "\n",
       "        [[ 75,  74,  75,  ..., 130, 128, 128],\n",
       "         [ 75,  75,  74,  ..., 130, 128, 129],\n",
       "         [ 76,  74,  72,  ..., 131, 129, 128],\n",
       "         ...,\n",
       "         [ 76,  77,  79,  ...,  84,  95,  87],\n",
       "         [ 77,  78,  75,  ..., 104, 105, 100],\n",
       "         [ 75,  75,  78,  ..., 124, 110,  89]],\n",
       "\n",
       "        [[ 96,  95,  96,  ..., 133, 134, 136],\n",
       "         [ 96,  96,  97,  ..., 133, 134, 136],\n",
       "         [ 94,  97,  97,  ..., 135, 134, 136],\n",
       "         ...,\n",
       "         [ 96,  98, 100,  ..., 105, 117, 109],\n",
       "         [ 97,  99,  98,  ..., 121, 125, 123],\n",
       "         [ 95,  99, 102,  ..., 140, 128, 111]]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.images[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a81752",
   "metadata": {},
   "source": [
    "# Create my custom convolution model for the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "609bc17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepCNN(nn.Module):\n",
    "    def __init__(self, nb_classes):\n",
    "        super(DeepCNN, self).__init__()\n",
    "        # inputs (N,3,64,64)\n",
    "\n",
    "        # Classical processing of images with pattern recognition\n",
    "        # Follows ResNET architecture: Conv2d -> BN -> ReLU (-> MaxPool)\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=8)\n",
    "        self.bn1   = nn.BatchNorm2d(64)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        # no pooling in 1st step\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=5, padding=2)\n",
    "        self.bn2   = nn.BatchNorm2d(64)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=32, kernel_size=5)\n",
    "        self.bn3   = nn.BatchNorm2d(32)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        # no pooling in 3rd step\n",
    "\n",
    "        self.conv4 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=5, padding=2)\n",
    "        self.bn4   = nn.BatchNorm2d(32)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv5 = nn.Conv2d(in_channels=32, out_channels=16, kernel_size=5)\n",
    "        self.bn5   = nn.BatchNorm2d(16)\n",
    "        self.relu5 = nn.ReLU()\n",
    "        # no pooling in 5th step\n",
    "\n",
    "        self.conv6 = nn.Conv2d(in_channels=16, out_channels=16, kernel_size=5, padding=2)\n",
    "        self.bn6   = nn.BatchNorm2d(16)\n",
    "        self.relu6 = nn.ReLU()\n",
    "        self.pool6 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "\n",
    "        self.drop7 = nn.Dropout(p=0.05)\n",
    "        self.lin7  = nn.Linear(16*4*4, nb_classes) # adapt here!\n",
    "        \n",
    "    def forward(self, x):        \n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "\n",
    "        x = x + self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu3(x)\n",
    "\n",
    "        x = x + self.conv4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = self.relu4(x)\n",
    "        x = self.pool4(x)\n",
    "\n",
    "        x = self.conv5(x)\n",
    "        x = self.bn5(x)\n",
    "        x = self.relu5(x)\n",
    "\n",
    "        x = x + self.conv6(x)\n",
    "        x = self.bn6(x)\n",
    "        x = self.relu6(x)\n",
    "        x = self.pool6(x)\n",
    "\n",
    "        # outputs (None, 8, 4, 4)\n",
    "        x = self.drop7(x)\n",
    "        x = self.lin7(x.view(-1, 16*4*4))  # adapt here!\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b01bebee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(nb_classes):\n",
    "    return DeepCNN(nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1dea5346",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([42, 10])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_model(len(dataset.classes))\n",
    "dummy_img = torch.rand(42, 3, 64, 64)\n",
    "model(dummy_img).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5ebb7962",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.43.0 (0)\n",
       " -->\n",
       "<!-- Title: DeepCNN Pages: 1 -->\n",
       "<svg width=\"300pt\" height=\"2743pt\"\n",
       " viewBox=\"0.00 0.00 300.14 2743.07\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 2739.07)\">\n",
       "<title>DeepCNN</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-2739.07 296.14,-2739.07 296.14,4 -4,4\"/>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-2720.87\" font-family=\"Times,serif\" font-weight=\"bold\" font-size=\"14.00\">DeepCNN</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-2706.87\" font-family=\"Times,serif\" font-size=\"14.00\">41 tensors total (265.9 MB)</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-2692.87\" font-family=\"Times,serif\" font-size=\"14.00\">213930 params total (838.8 KB)</text>\n",
       "<!-- input_1 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>input_1</title>\n",
       "<ellipse fill=\"#98fb98\" stroke=\"black\" cx=\"158.5\" cy=\"-35.36\" rx=\"120.83\" ry=\"35.21\"/>\n",
       "<text text-anchor=\"start\" x=\"128.5\" y=\"-46.16\" font-family=\"Times,serif\" font-weight=\"bold\" font-size=\"14.00\">input_1</text>\n",
       "<text text-anchor=\"start\" x=\"81\" y=\"-32.16\" font-family=\"Times,serif\" font-size=\"14.00\">42x3x64x64 (2.0 MB)</text>\n",
       "<text text-anchor=\"start\" x=\"126\" y=\"-18.16\" font-family=\"Times,serif\" font-size=\"14.00\">@input.x</text>\n",
       "</g>\n",
       "<!-- conv2d_1_1 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>conv2d_1_1</title>\n",
       "<polygon fill=\"#e6e6e6\" stroke=\"black\" points=\"253,-170.71 64,-170.71 64,-106.71 253,-106.71 253,-170.71\"/>\n",
       "<text text-anchor=\"start\" x=\"114\" y=\"-156.51\" font-family=\"Times,serif\" font-weight=\"bold\" font-size=\"14.00\">conv2d_1_1</text>\n",
       "<text text-anchor=\"start\" x=\"72\" y=\"-142.51\" font-family=\"Times,serif\" font-size=\"14.00\">42x64x57x57 (33.3 MB)</text>\n",
       "<text text-anchor=\"start\" x=\"74.5\" y=\"-128.51\" font-family=\"Times,serif\" font-size=\"14.00\">params: 64x3x8x8, x64</text>\n",
       "<text text-anchor=\"start\" x=\"130\" y=\"-114.51\" font-family=\"Times,serif\" font-size=\"14.00\">@conv1</text>\n",
       "</g>\n",
       "<!-- input_1&#45;&gt;conv2d_1_1 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>input_1&#45;&gt;conv2d_1_1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M158.5,-70.95C158.5,-80.18 158.5,-90.21 158.5,-99.62\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"156.05,-99.67 158.5,-106.67 160.95,-99.67 156.05,-99.67\"/>\n",
       "</g>\n",
       "<!-- batchnorm_1_2 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>batchnorm_1_2</title>\n",
       "<polygon fill=\"#e6e6e6\" stroke=\"black\" points=\"253,-270.71 64,-270.71 64,-206.71 253,-206.71 253,-270.71\"/>\n",
       "<text text-anchor=\"start\" x=\"98.5\" y=\"-256.51\" font-family=\"Times,serif\" font-weight=\"bold\" font-size=\"14.00\">batchnorm_1_2</text>\n",
       "<text text-anchor=\"start\" x=\"72\" y=\"-242.51\" font-family=\"Times,serif\" font-size=\"14.00\">42x64x57x57 (33.3 MB)</text>\n",
       "<text text-anchor=\"start\" x=\"96.5\" y=\"-228.51\" font-family=\"Times,serif\" font-size=\"14.00\">params: x64, x64</text>\n",
       "<text text-anchor=\"start\" x=\"137.5\" y=\"-214.51\" font-family=\"Times,serif\" font-size=\"14.00\">@bn1</text>\n",
       "</g>\n",
       "<!-- conv2d_1_1&#45;&gt;batchnorm_1_2 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>conv2d_1_1&#45;&gt;batchnorm_1_2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M158.5,-170.72C158.5,-179.83 158.5,-189.91 158.5,-199.43\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"156.05,-199.57 158.5,-206.57 160.95,-199.57 156.05,-199.57\"/>\n",
       "</g>\n",
       "<!-- relu_1_3 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>relu_1_3</title>\n",
       "<polygon fill=\"white\" stroke=\"black\" points=\"253,-356.71 64,-356.71 64,-306.71 253,-306.71 253,-356.71\"/>\n",
       "<text text-anchor=\"start\" x=\"125.5\" y=\"-342.51\" font-family=\"Times,serif\" font-weight=\"bold\" font-size=\"14.00\">relu_1_3</text>\n",
       "<text text-anchor=\"start\" x=\"72\" y=\"-328.51\" font-family=\"Times,serif\" font-size=\"14.00\">42x64x57x57 (33.3 MB)</text>\n",
       "<text text-anchor=\"start\" x=\"132.5\" y=\"-314.51\" font-family=\"Times,serif\" font-size=\"14.00\">@relu1</text>\n",
       "</g>\n",
       "<!-- batchnorm_1_2&#45;&gt;relu_1_3 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>batchnorm_1_2&#45;&gt;relu_1_3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M158.5,-270.77C158.5,-280.04 158.5,-290.19 158.5,-299.46\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"156.05,-299.68 158.5,-306.68 160.95,-299.68 156.05,-299.68\"/>\n",
       "</g>\n",
       "<!-- conv2d_2_4 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>conv2d_2_4</title>\n",
       "<polygon fill=\"#e6e6e6\" stroke=\"black\" points=\"193,-456.71 0,-456.71 0,-392.71 193,-392.71 193,-456.71\"/>\n",
       "<text text-anchor=\"start\" x=\"52\" y=\"-442.51\" font-family=\"Times,serif\" font-weight=\"bold\" font-size=\"14.00\">conv2d_2_4</text>\n",
       "<text text-anchor=\"start\" x=\"10\" y=\"-428.51\" font-family=\"Times,serif\" font-size=\"14.00\">42x64x57x57 (33.3 MB)</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-414.51\" font-family=\"Times,serif\" font-size=\"14.00\">params: 64x64x5x5, x64</text>\n",
       "<text text-anchor=\"start\" x=\"68\" y=\"-400.51\" font-family=\"Times,serif\" font-size=\"14.00\">@conv2</text>\n",
       "</g>\n",
       "<!-- relu_1_3&#45;&gt;conv2d_2_4 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>relu_1_3&#45;&gt;conv2d_2_4</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M141.89,-357.1C135.77,-366.08 128.69,-376.47 121.96,-386.33\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"119.75,-385.23 117.84,-392.39 123.8,-387.99 119.75,-385.23\"/>\n",
       "</g>\n",
       "<!-- add_1_5 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>add_1_5</title>\n",
       "<ellipse fill=\"white\" stroke=\"black\" cx=\"158.5\" cy=\"-518.17\" rx=\"133.79\" ry=\"25.41\"/>\n",
       "<text text-anchor=\"start\" x=\"127.5\" y=\"-521.97\" font-family=\"Times,serif\" font-weight=\"bold\" font-size=\"14.00\">add_1_5</text>\n",
       "<text text-anchor=\"start\" x=\"72\" y=\"-507.97\" font-family=\"Times,serif\" font-size=\"14.00\">42x64x57x57 (33.3 MB)</text>\n",
       "</g>\n",
       "<!-- relu_1_3&#45;&gt;add_1_5 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>relu_1_3&#45;&gt;add_1_5</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M181.73,-356.97C189.96,-367.17 198.24,-379.7 202.5,-392.71 211.35,-419.74 211.3,-429.66 202.5,-456.71 198.97,-467.55 192.69,-478.06 185.96,-487.19\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"183.83,-485.95 181.5,-492.99 187.71,-488.94 183.83,-485.95\"/>\n",
       "</g>\n",
       "<!-- conv2d_2_4&#45;&gt;add_1_5 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>conv2d_2_4&#45;&gt;add_1_5</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M117.66,-456.93C124.23,-466.61 131.45,-477.27 137.97,-486.89\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"135.97,-488.31 141.93,-492.72 140.03,-485.56 135.97,-488.31\"/>\n",
       "</g>\n",
       "<!-- batchnorm_2_6 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>batchnorm_2_6</title>\n",
       "<polygon fill=\"#e6e6e6\" stroke=\"black\" points=\"253,-643.62 64,-643.62 64,-579.62 253,-579.62 253,-643.62\"/>\n",
       "<text text-anchor=\"start\" x=\"98.5\" y=\"-629.42\" font-family=\"Times,serif\" font-weight=\"bold\" font-size=\"14.00\">batchnorm_2_6</text>\n",
       "<text text-anchor=\"start\" x=\"72\" y=\"-615.42\" font-family=\"Times,serif\" font-size=\"14.00\">42x64x57x57 (33.3 MB)</text>\n",
       "<text text-anchor=\"start\" x=\"96.5\" y=\"-601.42\" font-family=\"Times,serif\" font-size=\"14.00\">params: x64, x64</text>\n",
       "<text text-anchor=\"start\" x=\"137.5\" y=\"-587.42\" font-family=\"Times,serif\" font-size=\"14.00\">@bn2</text>\n",
       "</g>\n",
       "<!-- add_1_5&#45;&gt;batchnorm_2_6 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>add_1_5&#45;&gt;batchnorm_2_6</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M158.5,-543.68C158.5,-552.44 158.5,-562.55 158.5,-572.21\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"156.05,-572.48 158.5,-579.48 160.95,-572.48 156.05,-572.48\"/>\n",
       "</g>\n",
       "<!-- relu_2_7 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>relu_2_7</title>\n",
       "<polygon fill=\"white\" stroke=\"black\" points=\"253,-729.62 64,-729.62 64,-679.62 253,-679.62 253,-729.62\"/>\n",
       "<text text-anchor=\"start\" x=\"125.5\" y=\"-715.42\" font-family=\"Times,serif\" font-weight=\"bold\" font-size=\"14.00\">relu_2_7</text>\n",
       "<text text-anchor=\"start\" x=\"72\" y=\"-701.42\" font-family=\"Times,serif\" font-size=\"14.00\">42x64x57x57 (33.3 MB)</text>\n",
       "<text text-anchor=\"start\" x=\"132.5\" y=\"-687.42\" font-family=\"Times,serif\" font-size=\"14.00\">@relu2</text>\n",
       "</g>\n",
       "<!-- batchnorm_2_6&#45;&gt;relu_2_7 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>batchnorm_2_6&#45;&gt;relu_2_7</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M158.5,-643.68C158.5,-652.95 158.5,-663.1 158.5,-672.38\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"156.05,-672.59 158.5,-679.59 160.95,-672.59 156.05,-672.59\"/>\n",
       "</g>\n",
       "<!-- maxpool2d_1_8 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>maxpool2d_1_8</title>\n",
       "<polygon fill=\"white\" stroke=\"black\" points=\"248.5,-815.62 68.5,-815.62 68.5,-765.62 248.5,-765.62 248.5,-815.62\"/>\n",
       "<text text-anchor=\"start\" x=\"99.5\" y=\"-801.42\" font-family=\"Times,serif\" font-weight=\"bold\" font-size=\"14.00\">maxpool2d_1_8</text>\n",
       "<text text-anchor=\"start\" x=\"76.5\" y=\"-787.42\" font-family=\"Times,serif\" font-size=\"14.00\">42x64x28x28 (8.0 MB)</text>\n",
       "<text text-anchor=\"start\" x=\"131.5\" y=\"-773.42\" font-family=\"Times,serif\" font-size=\"14.00\">@pool2</text>\n",
       "</g>\n",
       "<!-- relu_2_7&#45;&gt;maxpool2d_1_8 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>relu_2_7&#45;&gt;maxpool2d_1_8</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M158.5,-729.92C158.5,-738.8 158.5,-748.97 158.5,-758.38\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"156.05,-758.38 158.5,-765.38 160.95,-758.38 156.05,-758.38\"/>\n",
       "</g>\n",
       "<!-- conv2d_3_9 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>conv2d_3_9</title>\n",
       "<polygon fill=\"#e6e6e6\" stroke=\"black\" points=\"255,-915.62 62,-915.62 62,-851.62 255,-851.62 255,-915.62\"/>\n",
       "<text text-anchor=\"start\" x=\"114\" y=\"-901.42\" font-family=\"Times,serif\" font-weight=\"bold\" font-size=\"14.00\">conv2d_3_9</text>\n",
       "<text text-anchor=\"start\" x=\"76.5\" y=\"-887.42\" font-family=\"Times,serif\" font-size=\"14.00\">42x32x24x24 (3.0 MB)</text>\n",
       "<text text-anchor=\"start\" x=\"70\" y=\"-873.42\" font-family=\"Times,serif\" font-size=\"14.00\">params: 32x64x5x5, x32</text>\n",
       "<text text-anchor=\"start\" x=\"130\" y=\"-859.42\" font-family=\"Times,serif\" font-size=\"14.00\">@conv3</text>\n",
       "</g>\n",
       "<!-- maxpool2d_1_8&#45;&gt;conv2d_3_9 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>maxpool2d_1_8&#45;&gt;conv2d_3_9</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M158.5,-816.01C158.5,-824.65 158.5,-834.6 158.5,-844.13\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"156.05,-844.3 158.5,-851.3 160.95,-844.3 156.05,-844.3\"/>\n",
       "</g>\n",
       "<!-- batchnorm_3_10 -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>batchnorm_3_10</title>\n",
       "<polygon fill=\"#e6e6e6\" stroke=\"black\" points=\"248.5,-1015.62 68.5,-1015.62 68.5,-951.62 248.5,-951.62 248.5,-1015.62\"/>\n",
       "<text text-anchor=\"start\" x=\"93.5\" y=\"-1001.42\" font-family=\"Times,serif\" font-weight=\"bold\" font-size=\"14.00\">batchnorm_3_10</text>\n",
       "<text text-anchor=\"start\" x=\"76.5\" y=\"-987.42\" font-family=\"Times,serif\" font-size=\"14.00\">42x32x24x24 (3.0 MB)</text>\n",
       "<text text-anchor=\"start\" x=\"96.5\" y=\"-973.42\" font-family=\"Times,serif\" font-size=\"14.00\">params: x32, x32</text>\n",
       "<text text-anchor=\"start\" x=\"137.5\" y=\"-959.42\" font-family=\"Times,serif\" font-size=\"14.00\">@bn3</text>\n",
       "</g>\n",
       "<!-- conv2d_3_9&#45;&gt;batchnorm_3_10 -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>conv2d_3_9&#45;&gt;batchnorm_3_10</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M158.5,-915.63C158.5,-924.75 158.5,-934.83 158.5,-944.34\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"156.05,-944.48 158.5,-951.48 160.95,-944.48 156.05,-944.48\"/>\n",
       "</g>\n",
       "<!-- relu_3_11 -->\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>relu_3_11</title>\n",
       "<polygon fill=\"white\" stroke=\"black\" points=\"248.5,-1101.62 68.5,-1101.62 68.5,-1051.62 248.5,-1051.62 248.5,-1101.62\"/>\n",
       "<text text-anchor=\"start\" x=\"120.5\" y=\"-1087.42\" font-family=\"Times,serif\" font-weight=\"bold\" font-size=\"14.00\">relu_3_11</text>\n",
       "<text text-anchor=\"start\" x=\"76.5\" y=\"-1073.42\" font-family=\"Times,serif\" font-size=\"14.00\">42x32x24x24 (3.0 MB)</text>\n",
       "<text text-anchor=\"start\" x=\"132.5\" y=\"-1059.42\" font-family=\"Times,serif\" font-size=\"14.00\">@relu3</text>\n",
       "</g>\n",
       "<!-- batchnorm_3_10&#45;&gt;relu_3_11 -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>batchnorm_3_10&#45;&gt;relu_3_11</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M158.5,-1015.68C158.5,-1024.95 158.5,-1035.1 158.5,-1044.38\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"156.05,-1044.59 158.5,-1051.59 160.95,-1044.59 156.05,-1044.59\"/>\n",
       "</g>\n",
       "<!-- conv2d_4_12 -->\n",
       "<g id=\"node13\" class=\"node\">\n",
       "<title>conv2d_4_12</title>\n",
       "<polygon fill=\"#e6e6e6\" stroke=\"black\" points=\"193,-1201.62 0,-1201.62 0,-1137.62 193,-1137.62 193,-1201.62\"/>\n",
       "<text text-anchor=\"start\" x=\"47\" y=\"-1187.42\" font-family=\"Times,serif\" font-weight=\"bold\" font-size=\"14.00\">conv2d_4_12</text>\n",
       "<text text-anchor=\"start\" x=\"14.5\" y=\"-1173.42\" font-family=\"Times,serif\" font-size=\"14.00\">42x32x24x24 (3.0 MB)</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-1159.42\" font-family=\"Times,serif\" font-size=\"14.00\">params: 32x32x5x5, x32</text>\n",
       "<text text-anchor=\"start\" x=\"68\" y=\"-1145.42\" font-family=\"Times,serif\" font-size=\"14.00\">@conv4</text>\n",
       "</g>\n",
       "<!-- relu_3_11&#45;&gt;conv2d_4_12 -->\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>relu_3_11&#45;&gt;conv2d_4_12</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M141.89,-1102.01C135.77,-1110.99 128.69,-1121.38 121.96,-1131.25\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"119.75,-1130.14 117.84,-1137.3 123.8,-1132.9 119.75,-1130.14\"/>\n",
       "</g>\n",
       "<!-- add_2_13 -->\n",
       "<g id=\"node14\" class=\"node\">\n",
       "<title>add_2_13</title>\n",
       "<ellipse fill=\"white\" stroke=\"black\" cx=\"158.5\" cy=\"-1263.08\" rx=\"127.06\" ry=\"25.41\"/>\n",
       "<text text-anchor=\"start\" x=\"122.5\" y=\"-1266.88\" font-family=\"Times,serif\" font-weight=\"bold\" font-size=\"14.00\">add_2_13</text>\n",
       "<text text-anchor=\"start\" x=\"76.5\" y=\"-1252.88\" font-family=\"Times,serif\" font-size=\"14.00\">42x32x24x24 (3.0 MB)</text>\n",
       "</g>\n",
       "<!-- relu_3_11&#45;&gt;add_2_13 -->\n",
       "<g id=\"edge14\" class=\"edge\">\n",
       "<title>relu_3_11&#45;&gt;add_2_13</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M181.73,-1101.88C189.96,-1112.09 198.24,-1124.61 202.5,-1137.62 211.35,-1164.66 211.3,-1174.57 202.5,-1201.62 198.97,-1212.46 192.69,-1222.97 185.96,-1232.11\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"183.83,-1230.86 181.5,-1237.9 187.71,-1233.85 183.83,-1230.86\"/>\n",
       "</g>\n",
       "<!-- conv2d_4_12&#45;&gt;add_2_13 -->\n",
       "<g id=\"edge15\" class=\"edge\">\n",
       "<title>conv2d_4_12&#45;&gt;add_2_13</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M117.66,-1201.84C124.23,-1211.53 131.45,-1222.18 137.97,-1231.8\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"135.97,-1233.22 141.93,-1237.64 140.03,-1230.47 135.97,-1233.22\"/>\n",
       "</g>\n",
       "<!-- batchnorm_4_14 -->\n",
       "<g id=\"node15\" class=\"node\">\n",
       "<title>batchnorm_4_14</title>\n",
       "<polygon fill=\"#e6e6e6\" stroke=\"black\" points=\"248.5,-1388.53 68.5,-1388.53 68.5,-1324.53 248.5,-1324.53 248.5,-1388.53\"/>\n",
       "<text text-anchor=\"start\" x=\"93.5\" y=\"-1374.33\" font-family=\"Times,serif\" font-weight=\"bold\" font-size=\"14.00\">batchnorm_4_14</text>\n",
       "<text text-anchor=\"start\" x=\"76.5\" y=\"-1360.33\" font-family=\"Times,serif\" font-size=\"14.00\">42x32x24x24 (3.0 MB)</text>\n",
       "<text text-anchor=\"start\" x=\"96.5\" y=\"-1346.33\" font-family=\"Times,serif\" font-size=\"14.00\">params: x32, x32</text>\n",
       "<text text-anchor=\"start\" x=\"137.5\" y=\"-1332.33\" font-family=\"Times,serif\" font-size=\"14.00\">@bn4</text>\n",
       "</g>\n",
       "<!-- add_2_13&#45;&gt;batchnorm_4_14 -->\n",
       "<g id=\"edge16\" class=\"edge\">\n",
       "<title>add_2_13&#45;&gt;batchnorm_4_14</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M158.5,-1288.59C158.5,-1297.36 158.5,-1307.46 158.5,-1317.13\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"156.05,-1317.4 158.5,-1324.4 160.95,-1317.4 156.05,-1317.4\"/>\n",
       "</g>\n",
       "<!-- relu_4_15 -->\n",
       "<g id=\"node16\" class=\"node\">\n",
       "<title>relu_4_15</title>\n",
       "<polygon fill=\"white\" stroke=\"black\" points=\"248.5,-1474.53 68.5,-1474.53 68.5,-1424.53 248.5,-1424.53 248.5,-1474.53\"/>\n",
       "<text text-anchor=\"start\" x=\"120.5\" y=\"-1460.33\" font-family=\"Times,serif\" font-weight=\"bold\" font-size=\"14.00\">relu_4_15</text>\n",
       "<text text-anchor=\"start\" x=\"76.5\" y=\"-1446.33\" font-family=\"Times,serif\" font-size=\"14.00\">42x32x24x24 (3.0 MB)</text>\n",
       "<text text-anchor=\"start\" x=\"132.5\" y=\"-1432.33\" font-family=\"Times,serif\" font-size=\"14.00\">@relu4</text>\n",
       "</g>\n",
       "<!-- batchnorm_4_14&#45;&gt;relu_4_15 -->\n",
       "<g id=\"edge17\" class=\"edge\">\n",
       "<title>batchnorm_4_14&#45;&gt;relu_4_15</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M158.5,-1388.6C158.5,-1397.86 158.5,-1408.01 158.5,-1417.29\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"156.05,-1417.5 158.5,-1424.5 160.95,-1417.5 156.05,-1417.5\"/>\n",
       "</g>\n",
       "<!-- maxpool2d_2_16 -->\n",
       "<g id=\"node17\" class=\"node\">\n",
       "<title>maxpool2d_2_16</title>\n",
       "<polygon fill=\"white\" stroke=\"black\" points=\"255.5,-1560.53 61.5,-1560.53 61.5,-1510.53 255.5,-1510.53 255.5,-1560.53\"/>\n",
       "<text text-anchor=\"start\" x=\"94.5\" y=\"-1546.33\" font-family=\"Times,serif\" font-weight=\"bold\" font-size=\"14.00\">maxpool2d_2_16</text>\n",
       "<text text-anchor=\"start\" x=\"69.5\" y=\"-1532.33\" font-family=\"Times,serif\" font-size=\"14.00\">42x32x12x12 (756.2 KB)</text>\n",
       "<text text-anchor=\"start\" x=\"131.5\" y=\"-1518.33\" font-family=\"Times,serif\" font-size=\"14.00\">@pool4</text>\n",
       "</g>\n",
       "<!-- relu_4_15&#45;&gt;maxpool2d_2_16 -->\n",
       "<g id=\"edge18\" class=\"edge\">\n",
       "<title>relu_4_15&#45;&gt;maxpool2d_2_16</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M158.5,-1474.83C158.5,-1483.71 158.5,-1493.88 158.5,-1503.29\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"156.05,-1503.29 158.5,-1510.29 160.95,-1503.29 156.05,-1503.29\"/>\n",
       "</g>\n",
       "<!-- conv2d_5_17 -->\n",
       "<g id=\"node18\" class=\"node\">\n",
       "<title>conv2d_5_17</title>\n",
       "<polygon fill=\"#e6e6e6\" stroke=\"black\" points=\"255,-1660.53 62,-1660.53 62,-1596.53 255,-1596.53 255,-1660.53\"/>\n",
       "<text text-anchor=\"start\" x=\"109\" y=\"-1646.33\" font-family=\"Times,serif\" font-weight=\"bold\" font-size=\"14.00\">conv2d_5_17</text>\n",
       "<text text-anchor=\"start\" x=\"78.5\" y=\"-1632.33\" font-family=\"Times,serif\" font-size=\"14.00\">42x16x8x8 (168.2 KB)</text>\n",
       "<text text-anchor=\"start\" x=\"70\" y=\"-1618.33\" font-family=\"Times,serif\" font-size=\"14.00\">params: 16x32x5x5, x16</text>\n",
       "<text text-anchor=\"start\" x=\"130\" y=\"-1604.33\" font-family=\"Times,serif\" font-size=\"14.00\">@conv5</text>\n",
       "</g>\n",
       "<!-- maxpool2d_2_16&#45;&gt;conv2d_5_17 -->\n",
       "<g id=\"edge19\" class=\"edge\">\n",
       "<title>maxpool2d_2_16&#45;&gt;conv2d_5_17</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M158.5,-1560.92C158.5,-1569.56 158.5,-1579.51 158.5,-1589.04\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"156.05,-1589.21 158.5,-1596.22 160.95,-1589.22 156.05,-1589.21\"/>\n",
       "</g>\n",
       "<!-- batchnorm_5_18 -->\n",
       "<g id=\"node19\" class=\"node\">\n",
       "<title>batchnorm_5_18</title>\n",
       "<polygon fill=\"#e6e6e6\" stroke=\"black\" points=\"246.5,-1760.53 70.5,-1760.53 70.5,-1696.53 246.5,-1696.53 246.5,-1760.53\"/>\n",
       "<text text-anchor=\"start\" x=\"93.5\" y=\"-1746.33\" font-family=\"Times,serif\" font-weight=\"bold\" font-size=\"14.00\">batchnorm_5_18</text>\n",
       "<text text-anchor=\"start\" x=\"78.5\" y=\"-1732.33\" font-family=\"Times,serif\" font-size=\"14.00\">42x16x8x8 (168.2 KB)</text>\n",
       "<text text-anchor=\"start\" x=\"96.5\" y=\"-1718.33\" font-family=\"Times,serif\" font-size=\"14.00\">params: x16, x16</text>\n",
       "<text text-anchor=\"start\" x=\"137.5\" y=\"-1704.33\" font-family=\"Times,serif\" font-size=\"14.00\">@bn5</text>\n",
       "</g>\n",
       "<!-- conv2d_5_17&#45;&gt;batchnorm_5_18 -->\n",
       "<g id=\"edge20\" class=\"edge\">\n",
       "<title>conv2d_5_17&#45;&gt;batchnorm_5_18</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M158.5,-1660.54C158.5,-1669.66 158.5,-1679.74 158.5,-1689.25\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"156.05,-1689.39 158.5,-1696.39 160.95,-1689.39 156.05,-1689.39\"/>\n",
       "</g>\n",
       "<!-- relu_5_19 -->\n",
       "<g id=\"node20\" class=\"node\">\n",
       "<title>relu_5_19</title>\n",
       "<polygon fill=\"white\" stroke=\"black\" points=\"246.5,-1846.53 70.5,-1846.53 70.5,-1796.53 246.5,-1796.53 246.5,-1846.53\"/>\n",
       "<text text-anchor=\"start\" x=\"120.5\" y=\"-1832.33\" font-family=\"Times,serif\" font-weight=\"bold\" font-size=\"14.00\">relu_5_19</text>\n",
       "<text text-anchor=\"start\" x=\"78.5\" y=\"-1818.33\" font-family=\"Times,serif\" font-size=\"14.00\">42x16x8x8 (168.2 KB)</text>\n",
       "<text text-anchor=\"start\" x=\"132.5\" y=\"-1804.33\" font-family=\"Times,serif\" font-size=\"14.00\">@relu5</text>\n",
       "</g>\n",
       "<!-- batchnorm_5_18&#45;&gt;relu_5_19 -->\n",
       "<g id=\"edge21\" class=\"edge\">\n",
       "<title>batchnorm_5_18&#45;&gt;relu_5_19</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M158.5,-1760.6C158.5,-1769.86 158.5,-1780.01 158.5,-1789.29\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"156.05,-1789.5 158.5,-1796.5 160.95,-1789.5 156.05,-1789.5\"/>\n",
       "</g>\n",
       "<!-- conv2d_6_20 -->\n",
       "<g id=\"node21\" class=\"node\">\n",
       "<title>conv2d_6_20</title>\n",
       "<polygon fill=\"#e6e6e6\" stroke=\"black\" points=\"193,-1946.53 0,-1946.53 0,-1882.53 193,-1882.53 193,-1946.53\"/>\n",
       "<text text-anchor=\"start\" x=\"47\" y=\"-1932.33\" font-family=\"Times,serif\" font-weight=\"bold\" font-size=\"14.00\">conv2d_6_20</text>\n",
       "<text text-anchor=\"start\" x=\"16.5\" y=\"-1918.33\" font-family=\"Times,serif\" font-size=\"14.00\">42x16x8x8 (168.2 KB)</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-1904.33\" font-family=\"Times,serif\" font-size=\"14.00\">params: 16x16x5x5, x16</text>\n",
       "<text text-anchor=\"start\" x=\"68\" y=\"-1890.33\" font-family=\"Times,serif\" font-size=\"14.00\">@conv6</text>\n",
       "</g>\n",
       "<!-- relu_5_19&#45;&gt;conv2d_6_20 -->\n",
       "<g id=\"edge22\" class=\"edge\">\n",
       "<title>relu_5_19&#45;&gt;conv2d_6_20</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M141.89,-1846.92C135.77,-1855.9 128.69,-1866.29 121.96,-1876.16\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"119.75,-1875.05 117.84,-1882.22 123.8,-1877.81 119.75,-1875.05\"/>\n",
       "</g>\n",
       "<!-- add_3_21 -->\n",
       "<g id=\"node22\" class=\"node\">\n",
       "<title>add_3_21</title>\n",
       "<ellipse fill=\"white\" stroke=\"black\" cx=\"158.5\" cy=\"-2007.99\" rx=\"124.4\" ry=\"25.41\"/>\n",
       "<text text-anchor=\"start\" x=\"122.5\" y=\"-2011.79\" font-family=\"Times,serif\" font-weight=\"bold\" font-size=\"14.00\">add_3_21</text>\n",
       "<text text-anchor=\"start\" x=\"78.5\" y=\"-1997.79\" font-family=\"Times,serif\" font-size=\"14.00\">42x16x8x8 (168.2 KB)</text>\n",
       "</g>\n",
       "<!-- relu_5_19&#45;&gt;add_3_21 -->\n",
       "<g id=\"edge23\" class=\"edge\">\n",
       "<title>relu_5_19&#45;&gt;add_3_21</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M181.73,-1846.79C189.96,-1857 198.24,-1869.52 202.5,-1882.53 211.35,-1909.57 211.3,-1919.49 202.5,-1946.53 198.97,-1957.37 192.69,-1967.88 185.96,-1977.02\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"183.83,-1975.77 181.5,-1982.81 187.71,-1978.76 183.83,-1975.77\"/>\n",
       "</g>\n",
       "<!-- conv2d_6_20&#45;&gt;add_3_21 -->\n",
       "<g id=\"edge24\" class=\"edge\">\n",
       "<title>conv2d_6_20&#45;&gt;add_3_21</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M117.66,-1946.75C124.23,-1956.44 131.45,-1967.09 137.97,-1976.71\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"135.97,-1978.13 141.93,-1982.55 140.03,-1975.38 135.97,-1978.13\"/>\n",
       "</g>\n",
       "<!-- batchnorm_6_22 -->\n",
       "<g id=\"node23\" class=\"node\">\n",
       "<title>batchnorm_6_22</title>\n",
       "<polygon fill=\"#e6e6e6\" stroke=\"black\" points=\"246.5,-2133.45 70.5,-2133.45 70.5,-2069.45 246.5,-2069.45 246.5,-2133.45\"/>\n",
       "<text text-anchor=\"start\" x=\"93.5\" y=\"-2119.25\" font-family=\"Times,serif\" font-weight=\"bold\" font-size=\"14.00\">batchnorm_6_22</text>\n",
       "<text text-anchor=\"start\" x=\"78.5\" y=\"-2105.25\" font-family=\"Times,serif\" font-size=\"14.00\">42x16x8x8 (168.2 KB)</text>\n",
       "<text text-anchor=\"start\" x=\"96.5\" y=\"-2091.25\" font-family=\"Times,serif\" font-size=\"14.00\">params: x16, x16</text>\n",
       "<text text-anchor=\"start\" x=\"137.5\" y=\"-2077.25\" font-family=\"Times,serif\" font-size=\"14.00\">@bn6</text>\n",
       "</g>\n",
       "<!-- add_3_21&#45;&gt;batchnorm_6_22 -->\n",
       "<g id=\"edge25\" class=\"edge\">\n",
       "<title>add_3_21&#45;&gt;batchnorm_6_22</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M158.5,-2033.5C158.5,-2042.27 158.5,-2052.38 158.5,-2062.04\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"156.05,-2062.31 158.5,-2069.31 160.95,-2062.31 156.05,-2062.31\"/>\n",
       "</g>\n",
       "<!-- relu_6_23 -->\n",
       "<g id=\"node24\" class=\"node\">\n",
       "<title>relu_6_23</title>\n",
       "<polygon fill=\"white\" stroke=\"black\" points=\"246.5,-2219.45 70.5,-2219.45 70.5,-2169.45 246.5,-2169.45 246.5,-2219.45\"/>\n",
       "<text text-anchor=\"start\" x=\"120.5\" y=\"-2205.25\" font-family=\"Times,serif\" font-weight=\"bold\" font-size=\"14.00\">relu_6_23</text>\n",
       "<text text-anchor=\"start\" x=\"78.5\" y=\"-2191.25\" font-family=\"Times,serif\" font-size=\"14.00\">42x16x8x8 (168.2 KB)</text>\n",
       "<text text-anchor=\"start\" x=\"132.5\" y=\"-2177.25\" font-family=\"Times,serif\" font-size=\"14.00\">@relu6</text>\n",
       "</g>\n",
       "<!-- batchnorm_6_22&#45;&gt;relu_6_23 -->\n",
       "<g id=\"edge26\" class=\"edge\">\n",
       "<title>batchnorm_6_22&#45;&gt;relu_6_23</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M158.5,-2133.51C158.5,-2142.77 158.5,-2152.93 158.5,-2162.2\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"156.05,-2162.42 158.5,-2169.42 160.95,-2162.42 156.05,-2162.42\"/>\n",
       "</g>\n",
       "<!-- maxpool2d_3_24 -->\n",
       "<g id=\"node25\" class=\"node\">\n",
       "<title>maxpool2d_3_24</title>\n",
       "<polygon fill=\"white\" stroke=\"black\" points=\"242,-2305.45 75,-2305.45 75,-2255.45 242,-2255.45 242,-2305.45\"/>\n",
       "<text text-anchor=\"start\" x=\"94.5\" y=\"-2291.25\" font-family=\"Times,serif\" font-weight=\"bold\" font-size=\"14.00\">maxpool2d_3_24</text>\n",
       "<text text-anchor=\"start\" x=\"83\" y=\"-2277.25\" font-family=\"Times,serif\" font-size=\"14.00\">42x16x4x4 (42.2 KB)</text>\n",
       "<text text-anchor=\"start\" x=\"131.5\" y=\"-2263.25\" font-family=\"Times,serif\" font-size=\"14.00\">@pool6</text>\n",
       "</g>\n",
       "<!-- relu_6_23&#45;&gt;maxpool2d_3_24 -->\n",
       "<g id=\"edge27\" class=\"edge\">\n",
       "<title>relu_6_23&#45;&gt;maxpool2d_3_24</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M158.5,-2219.74C158.5,-2228.62 158.5,-2238.8 158.5,-2248.2\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"156.05,-2248.2 158.5,-2255.2 160.95,-2248.2 156.05,-2248.2\"/>\n",
       "</g>\n",
       "<!-- dropout_1_25 -->\n",
       "<g id=\"node26\" class=\"node\">\n",
       "<title>dropout_1_25</title>\n",
       "<polygon fill=\"white\" stroke=\"black\" points=\"242,-2391.45 75,-2391.45 75,-2341.45 242,-2341.45 242,-2391.45\"/>\n",
       "<text text-anchor=\"start\" x=\"105.5\" y=\"-2377.25\" font-family=\"Times,serif\" font-weight=\"bold\" font-size=\"14.00\">dropout_1_25</text>\n",
       "<text text-anchor=\"start\" x=\"83\" y=\"-2363.25\" font-family=\"Times,serif\" font-size=\"14.00\">42x16x4x4 (42.2 KB)</text>\n",
       "<text text-anchor=\"start\" x=\"130\" y=\"-2349.25\" font-family=\"Times,serif\" font-size=\"14.00\">@drop7</text>\n",
       "</g>\n",
       "<!-- maxpool2d_3_24&#45;&gt;dropout_1_25 -->\n",
       "<g id=\"edge28\" class=\"edge\">\n",
       "<title>maxpool2d_3_24&#45;&gt;dropout_1_25</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M158.5,-2305.74C158.5,-2314.62 158.5,-2324.8 158.5,-2334.2\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"156.05,-2334.2 158.5,-2341.2 160.95,-2334.2 156.05,-2334.2\"/>\n",
       "</g>\n",
       "<!-- view_1_26 -->\n",
       "<g id=\"node27\" class=\"node\">\n",
       "<title>view_1_26</title>\n",
       "<ellipse fill=\"white\" stroke=\"black\" cx=\"158.5\" cy=\"-2452.9\" rx=\"100.32\" ry=\"25.41\"/>\n",
       "<text text-anchor=\"start\" x=\"119.5\" y=\"-2456.7\" font-family=\"Times,serif\" font-weight=\"bold\" font-size=\"14.00\">view_1_26</text>\n",
       "<text text-anchor=\"start\" x=\"95.5\" y=\"-2442.7\" font-family=\"Times,serif\" font-size=\"14.00\">42x256 (42.1 KB)</text>\n",
       "</g>\n",
       "<!-- dropout_1_25&#45;&gt;view_1_26 -->\n",
       "<g id=\"edge29\" class=\"edge\">\n",
       "<title>dropout_1_25&#45;&gt;view_1_26</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M158.5,-2391.87C158.5,-2400.64 158.5,-2410.65 158.5,-2419.96\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"156.05,-2420.24 158.5,-2427.24 160.95,-2420.24 156.05,-2420.24\"/>\n",
       "</g>\n",
       "<!-- linear_1_27 -->\n",
       "<g id=\"node28\" class=\"node\">\n",
       "<title>linear_1_27</title>\n",
       "<polygon fill=\"#e6e6e6\" stroke=\"black\" points=\"242,-2578.36 75,-2578.36 75,-2514.36 242,-2514.36 242,-2578.36\"/>\n",
       "<text text-anchor=\"start\" x=\"113.5\" y=\"-2564.16\" font-family=\"Times,serif\" font-weight=\"bold\" font-size=\"14.00\">linear_1_27</text>\n",
       "<text text-anchor=\"start\" x=\"104.5\" y=\"-2550.16\" font-family=\"Times,serif\" font-size=\"14.00\">42x10 (1.8 KB)</text>\n",
       "<text text-anchor=\"start\" x=\"83\" y=\"-2536.16\" font-family=\"Times,serif\" font-size=\"14.00\">params: 10x256, x10</text>\n",
       "<text text-anchor=\"start\" x=\"137.5\" y=\"-2522.16\" font-family=\"Times,serif\" font-size=\"14.00\">@lin7</text>\n",
       "</g>\n",
       "<!-- view_1_26&#45;&gt;linear_1_27 -->\n",
       "<g id=\"edge30\" class=\"edge\">\n",
       "<title>view_1_26&#45;&gt;linear_1_27</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M158.5,-2478.41C158.5,-2487.18 158.5,-2497.29 158.5,-2506.95\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"156.05,-2507.22 158.5,-2514.22 160.95,-2507.22 156.05,-2507.22\"/>\n",
       "</g>\n",
       "<!-- output_1 -->\n",
       "<g id=\"node29\" class=\"node\">\n",
       "<title>output_1</title>\n",
       "<ellipse fill=\"#ff9999\" stroke=\"black\" cx=\"158.5\" cy=\"-2649.71\" rx=\"87.86\" ry=\"35.21\"/>\n",
       "<text text-anchor=\"start\" x=\"123.5\" y=\"-2660.51\" font-family=\"Times,serif\" font-weight=\"bold\" font-size=\"14.00\">output_1</text>\n",
       "<text text-anchor=\"start\" x=\"104.5\" y=\"-2646.51\" font-family=\"Times,serif\" font-size=\"14.00\">42x10 (1.8 KB)</text>\n",
       "<text text-anchor=\"start\" x=\"127.5\" y=\"-2632.51\" font-family=\"Times,serif\" font-size=\"14.00\">@output</text>\n",
       "</g>\n",
       "<!-- linear_1_27&#45;&gt;output_1 -->\n",
       "<g id=\"edge31\" class=\"edge\">\n",
       "<title>linear_1_27&#45;&gt;output_1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M158.5,-2578.59C158.5,-2587.59 158.5,-2597.54 158.5,-2607.06\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"156.05,-2607.22 158.5,-2614.22 160.95,-2607.23 156.05,-2607.22\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7fadec0c5670>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log of DeepCNN forward pass:\n",
      "\tRandom seed: 147172562\n",
      "\tTime elapsed: 7.389s (7.126s spent logging)\n",
      "\tStructure:\n",
      "\t\t- purely feedforward, no recurrence\n",
      "\t\t- with branching\n",
      "\t\t- no conditional (if-then) branching\n",
      "\t\t- contains 12 buffer layers\n",
      "\t\t- 23 total modules\n",
      "\tTensor info:\n",
      "\t\t- 41 total tensors (265.9 MB) computed in forward pass.\n",
      "\t\t- 41 tensors (265.9 MB) with saved activations.\n",
      "\tParameters: 13 parameter operations (213930 params total; 838.8 KB)\n",
      "\tModule Hierarchy:\n",
      "\t\tconv1\n",
      "\t\tbn1\n",
      "\t\trelu1\n",
      "\t\tconv2\n",
      "\t\tbn2\n",
      "\t\trelu2\n",
      "\t\tpool2\n",
      "\t\tconv3\n",
      "\t\tbn3\n",
      "\t\trelu3\n",
      "\t\tconv4\n",
      "\t\tbn4\n",
      "\t\trelu4\n",
      "\t\tpool4\n",
      "\t\tconv5\n",
      "\t\tbn5\n",
      "\t\trelu5\n",
      "\t\tconv6\n",
      "\t\tbn6\n",
      "\t\trelu6\n",
      "\t\tpool6\n",
      "\t\tdrop7\n",
      "\t\tlin7\n",
      "\tLayers (all have saved activations):\n",
      "\t\t  (0) input_1 \n",
      "\t\t  (1) conv2d_1_1 \n",
      "\t\t  (2) buffer_1 \n",
      "\t\t  (3) buffer_2 \n",
      "\t\t  (4) batchnorm_1_2 \n",
      "\t\t  (5) relu_1_3 \n",
      "\t\t  (6) conv2d_2_4 \n",
      "\t\t  (7) add_1_5 \n",
      "\t\t  (8) buffer_3 \n",
      "\t\t  (9) buffer_4 \n",
      "\t\t  (10) batchnorm_2_6 \n",
      "\t\t  (11) relu_2_7 \n",
      "\t\t  (12) maxpool2d_1_8 \n",
      "\t\t  (13) conv2d_3_9 \n",
      "\t\t  (14) buffer_5 \n",
      "\t\t  (15) buffer_6 \n",
      "\t\t  (16) batchnorm_3_10 \n",
      "\t\t  (17) relu_3_11 \n",
      "\t\t  (18) conv2d_4_12 \n",
      "\t\t  (19) add_2_13 \n",
      "\t\t  (20) buffer_7 \n",
      "\t\t  (21) buffer_8 \n",
      "\t\t  (22) batchnorm_4_14 \n",
      "\t\t  (23) relu_4_15 \n",
      "\t\t  (24) maxpool2d_2_16 \n",
      "\t\t  (25) conv2d_5_17 \n",
      "\t\t  (26) buffer_9 \n",
      "\t\t  (27) buffer_10 \n",
      "\t\t  (28) batchnorm_5_18 \n",
      "\t\t  (29) relu_5_19 \n",
      "\t\t  (30) conv2d_6_20 \n",
      "\t\t  (31) add_3_21 \n",
      "\t\t  (32) buffer_11 \n",
      "\t\t  (33) buffer_12 \n",
      "\t\t  (34) batchnorm_6_22 \n",
      "\t\t  (35) relu_6_23 \n",
      "\t\t  (36) maxpool2d_3_24 \n",
      "\t\t  (37) dropout_1_25 \n",
      "\t\t  (38) view_1_26 \n",
      "\t\t  (39) linear_1_27 \n",
      "\t\t  (40) output_1 \n"
     ]
    }
   ],
   "source": [
    "import torchlens as tl\n",
    "model_history = tl.log_forward_pass(model, dummy_img, layers_to_save='all', vis_opt='rolled')\n",
    "print(model_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d82760",
   "metadata": {},
   "source": [
    "### Initialize the model and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6b095176",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' from sklearn.model_selection import train_test_split\\n\\n# Split dataset into training and validation sets\\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\\n\\n# Create data loaders for training and validation sets\\ntrain_dataset = MyDataset(X_train, y_train)\\nval_dataset = MyDataset(X_val, y_val)\\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False) '"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split dataset into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create data loaders for training and validation sets\n",
    "train_dataset = MyDataset(X_train, y_train)\n",
    "val_dataset = MyDataset(X_val, y_val)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "37dce276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([50, 3, 64, 64])\n",
      "Labels batch shape: torch.Size([50, 10])\n",
      "torch.Size([20, 10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "DeepCNN                                  --\n",
       "├─Conv2d: 1-1                            12,352\n",
       "├─BatchNorm2d: 1-2                       128\n",
       "├─ReLU: 1-3                              --\n",
       "├─Conv2d: 1-4                            102,464\n",
       "├─BatchNorm2d: 1-5                       128\n",
       "├─ReLU: 1-6                              --\n",
       "├─MaxPool2d: 1-7                         --\n",
       "├─Conv2d: 1-8                            51,232\n",
       "├─BatchNorm2d: 1-9                       64\n",
       "├─ReLU: 1-10                             --\n",
       "├─Conv2d: 1-11                           25,632\n",
       "├─BatchNorm2d: 1-12                      64\n",
       "├─ReLU: 1-13                             --\n",
       "├─MaxPool2d: 1-14                        --\n",
       "├─Conv2d: 1-15                           12,816\n",
       "├─BatchNorm2d: 1-16                      32\n",
       "├─ReLU: 1-17                             --\n",
       "├─Conv2d: 1-18                           6,416\n",
       "├─BatchNorm2d: 1-19                      32\n",
       "├─ReLU: 1-20                             --\n",
       "├─MaxPool2d: 1-21                        --\n",
       "├─Dropout: 1-22                          --\n",
       "├─Linear: 1-23                           2,570\n",
       "=================================================================\n",
       "Total params: 213,930\n",
       "Trainable params: 213,930\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create training and validation datasets\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "batch_size = 128\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Create data loaders for training and validation sets\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "data_loader = DataLoader(dataset, batch_size=50, shuffle=True)\n",
    "\n",
    "train_features, train_labels = next(iter(data_loader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "\n",
    "\n",
    "num_classes = len(dataset.classes)\n",
    "model = get_model(num_classes) # define the model\n",
    "# model.load_state_dict(torch.load(\"best_model.pt\"))\n",
    "\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss() # CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "\n",
    "image = torch.randn(20, 3, 64, 64) # create dummy image\n",
    "output = model(image)\n",
    "\n",
    "print(model(image).shape) # show the output of one batch of images\n",
    "\n",
    "from torchinfo import summary\n",
    "summary(model) # display summary of the model and number of trainable parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520830e6",
   "metadata": {},
   "source": [
    "### Send the model to the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "76203502",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeepCNN(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(8, 8), stride=(1, 1))\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu1): ReLU()\n",
       "  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu2): ReLU()\n",
       "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv3): Conv2d(64, 32, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu3): ReLU()\n",
       "  (conv4): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (bn4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu4): ReLU()\n",
       "  (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv5): Conv2d(32, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (bn5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu5): ReLU()\n",
       "  (conv6): Conv2d(16, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (bn6): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu6): ReLU()\n",
       "  (pool6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (drop7): Dropout(p=0.05, inplace=False)\n",
       "  (lin7): Linear(in_features=256, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a8b5dc",
   "metadata": {},
   "source": [
    "### Define the training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bbb08e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, loss_fn, optimizer, device, num_epochs):\n",
    "    train_losses = []\n",
    "    train_accs = []\n",
    "    val_losses = []\n",
    "    val_accs = []\n",
    "    best_val_acc = 0.0\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for batch, (X, y) in enumerate(train_loader):\n",
    "            # Move tensors to the configured device\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Compute prediction and loss\n",
    "            pred = model(X)\n",
    "\n",
    "            # Compute loss and its gradients\n",
    "            loss = loss_fn(pred, y)\n",
    "            loss.backward()\n",
    "            \n",
    "\n",
    "            # Backpropagation step\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(pred.data, 1)\n",
    "            total += y.size(0)\n",
    "            correct += (predicted == y.argmax(1)).sum().item()\n",
    "\n",
    "            # Display progress\n",
    "            if batch % 5 == 0:\n",
    "                loss, current = loss.item(), batch * len(X)\n",
    "                print(f\"Epoch {epoch+1}, batch {batch+1}/{len(train_loader)}, loss: {loss:.4f}\")\n",
    "        \n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_acc = 100 * correct / total\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        print(f\"Epoch {epoch+1}, train loss: {train_loss:.4f}, train accuracy: {train_acc:.2f}%\")\n",
    "\n",
    "        # Validation\n",
    "\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for batch, (X, y) in enumerate(val_loader):\n",
    "                X, y = X.to(device), y.to(device)\n",
    "                pred = model(X)\n",
    "                loss = loss_fn(pred, y)\n",
    "                running_loss += loss.item()\n",
    "                _, predicted = torch.max(pred.data, 1)\n",
    "                total += y.size(0)\n",
    "                correct += (predicted == y.argmax(1)).sum().item()\n",
    "            val_loss = running_loss / len(val_loader)\n",
    "            val_acc = 100 * correct / total\n",
    "            val_losses.append(val_loss)\n",
    "            val_accs.append(val_acc)\n",
    "            print(f\"Epoch {epoch+1}, val loss: {val_loss:.4f}, val accuracy: {val_acc:.2f}%\")\n",
    "        \n",
    "            # # Save best model\n",
    "            # if val_acc > best_val_acc:\n",
    "            #     best_val_acc = val_acc\n",
    "            #     torch.save(model.state_dict(), \"best_model.pt\")\n",
    "\n",
    "    return train_losses, train_accs, val_losses, val_accs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f224c264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, batch 1/72, loss: 2.6216\n",
      "Epoch 1, batch 6/72, loss: 2.0553\n",
      "Epoch 1, batch 11/72, loss: 2.0569\n",
      "Epoch 1, batch 16/72, loss: 1.8719\n",
      "Epoch 1, batch 21/72, loss: 1.7563\n",
      "Epoch 1, batch 26/72, loss: 1.6705\n",
      "Epoch 1, batch 31/72, loss: 1.5646\n",
      "Epoch 1, batch 36/72, loss: 1.5316\n",
      "Epoch 1, batch 41/72, loss: 1.4335\n",
      "Epoch 1, batch 46/72, loss: 1.3437\n",
      "Epoch 1, batch 51/72, loss: 1.4611\n",
      "Epoch 1, batch 56/72, loss: 1.4378\n",
      "Epoch 1, batch 61/72, loss: 1.3372\n",
      "Epoch 1, batch 66/72, loss: 1.3022\n",
      "Epoch 1, batch 71/72, loss: 1.3021\n",
      "Epoch 1, train loss: 1.6205, train accuracy: 45.07%\n",
      "Epoch 1, val loss: 1.1851, val accuracy: 61.28%\n",
      "Epoch 2, batch 1/72, loss: 1.1426\n",
      "Epoch 2, batch 6/72, loss: 1.0588\n",
      "Epoch 2, batch 11/72, loss: 1.1969\n",
      "Epoch 2, batch 16/72, loss: 1.0425\n",
      "Epoch 2, batch 21/72, loss: 1.1251\n",
      "Epoch 2, batch 26/72, loss: 1.0377\n",
      "Epoch 2, batch 31/72, loss: 0.9624\n",
      "Epoch 2, batch 36/72, loss: 1.0313\n",
      "Epoch 2, batch 41/72, loss: 0.9759\n",
      "Epoch 2, batch 46/72, loss: 0.8843\n",
      "Epoch 2, batch 51/72, loss: 0.9253\n",
      "Epoch 2, batch 56/72, loss: 0.9604\n",
      "Epoch 2, batch 61/72, loss: 0.9502\n",
      "Epoch 2, batch 66/72, loss: 0.8141\n",
      "Epoch 2, batch 71/72, loss: 0.9887\n",
      "Epoch 2, train loss: 1.0240, train accuracy: 66.91%\n",
      "Epoch 2, val loss: 0.8754, val accuracy: 71.35%\n",
      "Epoch 3, batch 1/72, loss: 0.8642\n",
      "Epoch 3, batch 6/72, loss: 0.9491\n",
      "Epoch 3, batch 11/72, loss: 0.8405\n",
      "Epoch 3, batch 16/72, loss: 0.7933\n",
      "Epoch 3, batch 21/72, loss: 0.9705\n",
      "Epoch 3, batch 26/72, loss: 0.6948\n",
      "Epoch 3, batch 31/72, loss: 0.8067\n",
      "Epoch 3, batch 36/72, loss: 0.7632\n",
      "Epoch 3, batch 41/72, loss: 0.7282\n",
      "Epoch 3, batch 46/72, loss: 0.7681\n",
      "Epoch 3, batch 51/72, loss: 0.8267\n",
      "Epoch 3, batch 56/72, loss: 0.9028\n",
      "Epoch 3, batch 61/72, loss: 0.8424\n",
      "Epoch 3, batch 66/72, loss: 0.8285\n",
      "Epoch 3, batch 71/72, loss: 0.6521\n",
      "Epoch 3, train loss: 0.8105, train accuracy: 73.77%\n",
      "Epoch 3, val loss: 0.7583, val accuracy: 75.56%\n",
      "Epoch 4, batch 1/72, loss: 0.6344\n",
      "Epoch 4, batch 6/72, loss: 0.6428\n",
      "Epoch 4, batch 11/72, loss: 0.8252\n",
      "Epoch 4, batch 16/72, loss: 0.8135\n",
      "Epoch 4, batch 21/72, loss: 0.7164\n",
      "Epoch 4, batch 26/72, loss: 0.6674\n",
      "Epoch 4, batch 31/72, loss: 0.7359\n",
      "Epoch 4, batch 36/72, loss: 0.7233\n",
      "Epoch 4, batch 41/72, loss: 0.7596\n",
      "Epoch 4, batch 46/72, loss: 0.7023\n",
      "Epoch 4, batch 51/72, loss: 0.7688\n",
      "Epoch 4, batch 56/72, loss: 0.6823\n",
      "Epoch 4, batch 61/72, loss: 0.6556\n",
      "Epoch 4, batch 66/72, loss: 0.6510\n",
      "Epoch 4, batch 71/72, loss: 0.7045\n",
      "Epoch 4, train loss: 0.7140, train accuracy: 76.20%\n",
      "Epoch 4, val loss: 0.7072, val accuracy: 76.30%\n",
      "Epoch 5, batch 1/72, loss: 0.5322\n",
      "Epoch 5, batch 6/72, loss: 0.6537\n",
      "Epoch 5, batch 11/72, loss: 0.6307\n",
      "Epoch 5, batch 16/72, loss: 0.6337\n",
      "Epoch 5, batch 21/72, loss: 0.6422\n",
      "Epoch 5, batch 26/72, loss: 0.7333\n",
      "Epoch 5, batch 31/72, loss: 0.7534\n",
      "Epoch 5, batch 36/72, loss: 0.6365\n",
      "Epoch 5, batch 41/72, loss: 0.8579\n",
      "Epoch 5, batch 46/72, loss: 0.5561\n",
      "Epoch 5, batch 51/72, loss: 0.6931\n",
      "Epoch 5, batch 56/72, loss: 0.6930\n",
      "Epoch 5, batch 61/72, loss: 0.6914\n",
      "Epoch 5, batch 66/72, loss: 0.5600\n",
      "Epoch 5, batch 71/72, loss: 0.5752\n",
      "Epoch 5, train loss: 0.6386, train accuracy: 78.44%\n",
      "Epoch 5, val loss: 0.6817, val accuracy: 76.82%\n",
      "Epoch 6, batch 1/72, loss: 0.6930\n",
      "Epoch 6, batch 6/72, loss: 0.6169\n",
      "Epoch 6, batch 11/72, loss: 0.5680\n",
      "Epoch 6, batch 16/72, loss: 0.7181\n",
      "Epoch 6, batch 21/72, loss: 0.5678\n",
      "Epoch 6, batch 26/72, loss: 0.5066\n",
      "Epoch 6, batch 31/72, loss: 0.6845\n",
      "Epoch 6, batch 36/72, loss: 0.6754\n",
      "Epoch 6, batch 41/72, loss: 0.5735\n",
      "Epoch 6, batch 46/72, loss: 0.5228\n",
      "Epoch 6, batch 51/72, loss: 0.4534\n",
      "Epoch 6, batch 56/72, loss: 0.6689\n",
      "Epoch 6, batch 61/72, loss: 0.7895\n",
      "Epoch 6, batch 66/72, loss: 0.5903\n",
      "Epoch 6, batch 71/72, loss: 0.4504\n",
      "Epoch 6, train loss: 0.5722, train accuracy: 80.78%\n",
      "Epoch 6, val loss: 0.6201, val accuracy: 79.21%\n",
      "Epoch 7, batch 1/72, loss: 0.5042\n",
      "Epoch 7, batch 6/72, loss: 0.5402\n",
      "Epoch 7, batch 11/72, loss: 0.5794\n",
      "Epoch 7, batch 16/72, loss: 0.5210\n",
      "Epoch 7, batch 21/72, loss: 0.5763\n",
      "Epoch 7, batch 26/72, loss: 0.5568\n",
      "Epoch 7, batch 31/72, loss: 0.5308\n",
      "Epoch 7, batch 36/72, loss: 0.5102\n",
      "Epoch 7, batch 41/72, loss: 0.6173\n",
      "Epoch 7, batch 46/72, loss: 0.5772\n",
      "Epoch 7, batch 51/72, loss: 0.5753\n",
      "Epoch 7, batch 56/72, loss: 0.7618\n",
      "Epoch 7, batch 61/72, loss: 0.5074\n",
      "Epoch 7, batch 66/72, loss: 0.4780\n",
      "Epoch 7, batch 71/72, loss: 0.4633\n",
      "Epoch 7, train loss: 0.5362, train accuracy: 81.82%\n",
      "Epoch 7, val loss: 0.6036, val accuracy: 79.51%\n",
      "Epoch 8, batch 1/72, loss: 0.5475\n",
      "Epoch 8, batch 6/72, loss: 0.5917\n",
      "Epoch 8, batch 11/72, loss: 0.5500\n",
      "Epoch 8, batch 16/72, loss: 0.5516\n",
      "Epoch 8, batch 21/72, loss: 0.4598\n",
      "Epoch 8, batch 26/72, loss: 0.5767\n",
      "Epoch 8, batch 31/72, loss: 0.4952\n",
      "Epoch 8, batch 36/72, loss: 0.4592\n",
      "Epoch 8, batch 41/72, loss: 0.5093\n",
      "Epoch 8, batch 46/72, loss: 0.4841\n",
      "Epoch 8, batch 51/72, loss: 0.4596\n",
      "Epoch 8, batch 56/72, loss: 0.4346\n",
      "Epoch 8, batch 61/72, loss: 0.6032\n",
      "Epoch 8, batch 66/72, loss: 0.4033\n",
      "Epoch 8, batch 71/72, loss: 0.4281\n",
      "Epoch 8, train loss: 0.5019, train accuracy: 82.59%\n",
      "Epoch 8, val loss: 0.5575, val accuracy: 81.03%\n",
      "Epoch 9, batch 1/72, loss: 0.5274\n",
      "Epoch 9, batch 6/72, loss: 0.4064\n",
      "Epoch 9, batch 11/72, loss: 0.4392\n",
      "Epoch 9, batch 16/72, loss: 0.5181\n",
      "Epoch 9, batch 21/72, loss: 0.3401\n",
      "Epoch 9, batch 26/72, loss: 0.4643\n",
      "Epoch 9, batch 31/72, loss: 0.4580\n",
      "Epoch 9, batch 36/72, loss: 0.4234\n",
      "Epoch 9, batch 41/72, loss: 0.4720\n",
      "Epoch 9, batch 46/72, loss: 0.4752\n",
      "Epoch 9, batch 51/72, loss: 0.5784\n",
      "Epoch 9, batch 56/72, loss: 0.5137\n",
      "Epoch 9, batch 61/72, loss: 0.4970\n",
      "Epoch 9, batch 66/72, loss: 0.4935\n",
      "Epoch 9, batch 71/72, loss: 0.4120\n",
      "Epoch 9, train loss: 0.4653, train accuracy: 83.84%\n",
      "Epoch 9, val loss: 0.5425, val accuracy: 81.51%\n",
      "Epoch 10, batch 1/72, loss: 0.5352\n",
      "Epoch 10, batch 6/72, loss: 0.4474\n",
      "Epoch 10, batch 11/72, loss: 0.3405\n",
      "Epoch 10, batch 16/72, loss: 0.4628\n",
      "Epoch 10, batch 21/72, loss: 0.4655\n",
      "Epoch 10, batch 26/72, loss: 0.4417\n",
      "Epoch 10, batch 31/72, loss: 0.4164\n",
      "Epoch 10, batch 36/72, loss: 0.3668\n",
      "Epoch 10, batch 41/72, loss: 0.3760\n",
      "Epoch 10, batch 46/72, loss: 0.3695\n",
      "Epoch 10, batch 51/72, loss: 0.4076\n",
      "Epoch 10, batch 56/72, loss: 0.4172\n",
      "Epoch 10, batch 61/72, loss: 0.4317\n",
      "Epoch 10, batch 66/72, loss: 0.5237\n",
      "Epoch 10, batch 71/72, loss: 0.5673\n",
      "Epoch 10, train loss: 0.4451, train accuracy: 84.73%\n",
      "Epoch 10, val loss: 0.5255, val accuracy: 81.51%\n",
      "Epoch 11, batch 1/72, loss: 0.4386\n",
      "Epoch 11, batch 6/72, loss: 0.3175\n",
      "Epoch 11, batch 11/72, loss: 0.4599\n",
      "Epoch 11, batch 16/72, loss: 0.5418\n",
      "Epoch 11, batch 21/72, loss: 0.7210\n",
      "Epoch 11, batch 26/72, loss: 0.3809\n",
      "Epoch 11, batch 31/72, loss: 0.4424\n",
      "Epoch 11, batch 36/72, loss: 0.4625\n",
      "Epoch 11, batch 41/72, loss: 0.4094\n",
      "Epoch 11, batch 46/72, loss: 0.4226\n",
      "Epoch 11, batch 51/72, loss: 0.2125\n",
      "Epoch 11, batch 56/72, loss: 0.3869\n",
      "Epoch 11, batch 61/72, loss: 0.5113\n",
      "Epoch 11, batch 66/72, loss: 0.3792\n",
      "Epoch 11, batch 71/72, loss: 0.3801\n",
      "Epoch 11, train loss: 0.4260, train accuracy: 85.45%\n",
      "Epoch 11, val loss: 0.5076, val accuracy: 83.03%\n",
      "Epoch 12, batch 1/72, loss: 0.4800\n",
      "Epoch 12, batch 6/72, loss: 0.2774\n",
      "Epoch 12, batch 11/72, loss: 0.3607\n",
      "Epoch 12, batch 16/72, loss: 0.3164\n",
      "Epoch 12, batch 21/72, loss: 0.4291\n",
      "Epoch 12, batch 26/72, loss: 0.3397\n",
      "Epoch 12, batch 31/72, loss: 0.3691\n",
      "Epoch 12, batch 36/72, loss: 0.4472\n",
      "Epoch 12, batch 41/72, loss: 0.3858\n",
      "Epoch 12, batch 46/72, loss: 0.5396\n",
      "Epoch 12, batch 51/72, loss: 0.5915\n",
      "Epoch 12, batch 56/72, loss: 0.3834\n",
      "Epoch 12, batch 61/72, loss: 0.3024\n",
      "Epoch 12, batch 66/72, loss: 0.3225\n",
      "Epoch 12, batch 71/72, loss: 0.4640\n",
      "Epoch 12, train loss: 0.3770, train accuracy: 87.26%\n",
      "Epoch 12, val loss: 0.5136, val accuracy: 82.77%\n",
      "Epoch 13, batch 1/72, loss: 0.2951\n",
      "Epoch 13, batch 6/72, loss: 0.2958\n",
      "Epoch 13, batch 11/72, loss: 0.3060\n",
      "Epoch 13, batch 16/72, loss: 0.2832\n",
      "Epoch 13, batch 21/72, loss: 0.3334\n",
      "Epoch 13, batch 26/72, loss: 0.2748\n",
      "Epoch 13, batch 31/72, loss: 0.3738\n",
      "Epoch 13, batch 36/72, loss: 0.5423\n",
      "Epoch 13, batch 41/72, loss: 0.4307\n",
      "Epoch 13, batch 46/72, loss: 0.4270\n",
      "Epoch 13, batch 51/72, loss: 0.4383\n",
      "Epoch 13, batch 56/72, loss: 0.4099\n",
      "Epoch 13, batch 61/72, loss: 0.3256\n",
      "Epoch 13, batch 66/72, loss: 0.4142\n",
      "Epoch 13, batch 71/72, loss: 0.3396\n",
      "Epoch 13, train loss: 0.3522, train accuracy: 88.27%\n",
      "Epoch 13, val loss: 0.4951, val accuracy: 82.68%\n",
      "Epoch 14, batch 1/72, loss: 0.4549\n",
      "Epoch 14, batch 6/72, loss: 0.3649\n",
      "Epoch 14, batch 11/72, loss: 0.3645\n",
      "Epoch 14, batch 16/72, loss: 0.4560\n",
      "Epoch 14, batch 21/72, loss: 0.3478\n",
      "Epoch 14, batch 26/72, loss: 0.2943\n",
      "Epoch 14, batch 31/72, loss: 0.2902\n",
      "Epoch 14, batch 36/72, loss: 0.2114\n",
      "Epoch 14, batch 41/72, loss: 0.4592\n",
      "Epoch 14, batch 46/72, loss: 0.4497\n",
      "Epoch 14, batch 51/72, loss: 0.5236\n",
      "Epoch 14, batch 56/72, loss: 0.3472\n",
      "Epoch 14, batch 61/72, loss: 0.2523\n",
      "Epoch 14, batch 66/72, loss: 0.4638\n",
      "Epoch 14, batch 71/72, loss: 0.3304\n",
      "Epoch 14, train loss: 0.3554, train accuracy: 87.90%\n",
      "Epoch 14, val loss: 0.4753, val accuracy: 83.85%\n",
      "Epoch 15, batch 1/72, loss: 0.3793\n",
      "Epoch 15, batch 6/72, loss: 0.2995\n",
      "Epoch 15, batch 11/72, loss: 0.3210\n",
      "Epoch 15, batch 16/72, loss: 0.3709\n",
      "Epoch 15, batch 21/72, loss: 0.2915\n",
      "Epoch 15, batch 26/72, loss: 0.3207\n",
      "Epoch 15, batch 31/72, loss: 0.3152\n",
      "Epoch 15, batch 36/72, loss: 0.2824\n",
      "Epoch 15, batch 41/72, loss: 0.3925\n",
      "Epoch 15, batch 46/72, loss: 0.4833\n",
      "Epoch 15, batch 51/72, loss: 0.2932\n",
      "Epoch 15, batch 56/72, loss: 0.2158\n",
      "Epoch 15, batch 61/72, loss: 0.3450\n",
      "Epoch 15, batch 66/72, loss: 0.3678\n",
      "Epoch 15, batch 71/72, loss: 0.3372\n",
      "Epoch 15, train loss: 0.3279, train accuracy: 89.32%\n",
      "Epoch 15, val loss: 0.5286, val accuracy: 82.60%\n",
      "Epoch 16, batch 1/72, loss: 0.3998\n",
      "Epoch 16, batch 6/72, loss: 0.3300\n",
      "Epoch 16, batch 11/72, loss: 0.2760\n",
      "Epoch 16, batch 16/72, loss: 0.3625\n",
      "Epoch 16, batch 21/72, loss: 0.4132\n",
      "Epoch 16, batch 26/72, loss: 0.3449\n",
      "Epoch 16, batch 31/72, loss: 0.3810\n",
      "Epoch 16, batch 36/72, loss: 0.2344\n",
      "Epoch 16, batch 41/72, loss: 0.3429\n",
      "Epoch 16, batch 46/72, loss: 0.3396\n",
      "Epoch 16, batch 51/72, loss: 0.2996\n",
      "Epoch 16, batch 56/72, loss: 0.2856\n",
      "Epoch 16, batch 61/72, loss: 0.2160\n",
      "Epoch 16, batch 66/72, loss: 0.2609\n",
      "Epoch 16, batch 71/72, loss: 0.2315\n",
      "Epoch 16, train loss: 0.3050, train accuracy: 89.99%\n",
      "Epoch 16, val loss: 0.4281, val accuracy: 85.68%\n",
      "Epoch 17, batch 1/72, loss: 0.2409\n",
      "Epoch 17, batch 6/72, loss: 0.2177\n",
      "Epoch 17, batch 11/72, loss: 0.4049\n",
      "Epoch 17, batch 16/72, loss: 0.1947\n",
      "Epoch 17, batch 21/72, loss: 0.3113\n",
      "Epoch 17, batch 26/72, loss: 0.2865\n",
      "Epoch 17, batch 31/72, loss: 0.1792\n",
      "Epoch 17, batch 36/72, loss: 0.2060\n",
      "Epoch 17, batch 41/72, loss: 0.3369\n",
      "Epoch 17, batch 46/72, loss: 0.2877\n",
      "Epoch 17, batch 51/72, loss: 0.3300\n",
      "Epoch 17, batch 56/72, loss: 0.3439\n",
      "Epoch 17, batch 61/72, loss: 0.3864\n",
      "Epoch 17, batch 66/72, loss: 0.2918\n",
      "Epoch 17, batch 71/72, loss: 0.3076\n",
      "Epoch 17, train loss: 0.2716, train accuracy: 91.10%\n",
      "Epoch 17, val loss: 0.4405, val accuracy: 84.81%\n",
      "Epoch 18, batch 1/72, loss: 0.2261\n",
      "Epoch 18, batch 6/72, loss: 0.2596\n",
      "Epoch 18, batch 11/72, loss: 0.2530\n",
      "Epoch 18, batch 16/72, loss: 0.2539\n",
      "Epoch 18, batch 21/72, loss: 0.3449\n",
      "Epoch 18, batch 26/72, loss: 0.2654\n",
      "Epoch 18, batch 31/72, loss: 0.3279\n",
      "Epoch 18, batch 36/72, loss: 0.2659\n",
      "Epoch 18, batch 41/72, loss: 0.2412\n",
      "Epoch 18, batch 46/72, loss: 0.3875\n",
      "Epoch 18, batch 51/72, loss: 0.2595\n",
      "Epoch 18, batch 56/72, loss: 0.2807\n",
      "Epoch 18, batch 61/72, loss: 0.1803\n",
      "Epoch 18, batch 66/72, loss: 0.2638\n",
      "Epoch 18, batch 71/72, loss: 0.2353\n",
      "Epoch 18, train loss: 0.2764, train accuracy: 90.92%\n",
      "Epoch 18, val loss: 0.4247, val accuracy: 86.24%\n",
      "Epoch 19, batch 1/72, loss: 0.2083\n",
      "Epoch 19, batch 6/72, loss: 0.3109\n",
      "Epoch 19, batch 11/72, loss: 0.2002\n",
      "Epoch 19, batch 16/72, loss: 0.2339\n",
      "Epoch 19, batch 21/72, loss: 0.2140\n",
      "Epoch 19, batch 26/72, loss: 0.2262\n",
      "Epoch 19, batch 31/72, loss: 0.1983\n",
      "Epoch 19, batch 36/72, loss: 0.2477\n",
      "Epoch 19, batch 41/72, loss: 0.2868\n",
      "Epoch 19, batch 46/72, loss: 0.2953\n",
      "Epoch 19, batch 51/72, loss: 0.1201\n",
      "Epoch 19, batch 56/72, loss: 0.2555\n",
      "Epoch 19, batch 61/72, loss: 0.2281\n",
      "Epoch 19, batch 66/72, loss: 0.3089\n",
      "Epoch 19, batch 71/72, loss: 0.2691\n",
      "Epoch 19, train loss: 0.2536, train accuracy: 91.61%\n",
      "Epoch 19, val loss: 0.3954, val accuracy: 86.85%\n",
      "Epoch 20, batch 1/72, loss: 0.1966\n",
      "Epoch 20, batch 6/72, loss: 0.2027\n",
      "Epoch 20, batch 11/72, loss: 0.2610\n",
      "Epoch 20, batch 16/72, loss: 0.1095\n",
      "Epoch 20, batch 21/72, loss: 0.2313\n",
      "Epoch 20, batch 26/72, loss: 0.2153\n",
      "Epoch 20, batch 31/72, loss: 0.2170\n",
      "Epoch 20, batch 36/72, loss: 0.1334\n",
      "Epoch 20, batch 41/72, loss: 0.2445\n",
      "Epoch 20, batch 46/72, loss: 0.1929\n",
      "Epoch 20, batch 51/72, loss: 0.3177\n",
      "Epoch 20, batch 56/72, loss: 0.2942\n",
      "Epoch 20, batch 61/72, loss: 0.2490\n",
      "Epoch 20, batch 66/72, loss: 0.2690\n",
      "Epoch 20, batch 71/72, loss: 0.3348\n",
      "Epoch 20, train loss: 0.2371, train accuracy: 92.14%\n",
      "Epoch 20, val loss: 0.5182, val accuracy: 82.73%\n",
      "Epoch 21, batch 1/72, loss: 0.2228\n",
      "Epoch 21, batch 6/72, loss: 0.2782\n",
      "Epoch 21, batch 11/72, loss: 0.1966\n",
      "Epoch 21, batch 16/72, loss: 0.1834\n",
      "Epoch 21, batch 21/72, loss: 0.2595\n",
      "Epoch 21, batch 26/72, loss: 0.2900\n",
      "Epoch 21, batch 31/72, loss: 0.2310\n",
      "Epoch 21, batch 36/72, loss: 0.1864\n",
      "Epoch 21, batch 41/72, loss: 0.2965\n",
      "Epoch 21, batch 46/72, loss: 0.2743\n",
      "Epoch 21, batch 51/72, loss: 0.2781\n",
      "Epoch 21, batch 56/72, loss: 0.2936\n",
      "Epoch 21, batch 61/72, loss: 0.2400\n",
      "Epoch 21, batch 66/72, loss: 0.2274\n",
      "Epoch 21, batch 71/72, loss: 0.1706\n",
      "Epoch 21, train loss: 0.2420, train accuracy: 91.94%\n",
      "Epoch 21, val loss: 0.3957, val accuracy: 86.76%\n",
      "Epoch 22, batch 1/72, loss: 0.1857\n",
      "Epoch 22, batch 6/72, loss: 0.3080\n",
      "Epoch 22, batch 11/72, loss: 0.1346\n",
      "Epoch 22, batch 16/72, loss: 0.2369\n",
      "Epoch 22, batch 21/72, loss: 0.2289\n",
      "Epoch 22, batch 26/72, loss: 0.1420\n",
      "Epoch 22, batch 31/72, loss: 0.2073\n",
      "Epoch 22, batch 36/72, loss: 0.2582\n",
      "Epoch 22, batch 41/72, loss: 0.2367\n",
      "Epoch 22, batch 46/72, loss: 0.2359\n",
      "Epoch 22, batch 51/72, loss: 0.2424\n",
      "Epoch 22, batch 56/72, loss: 0.2050\n",
      "Epoch 22, batch 61/72, loss: 0.2898\n",
      "Epoch 22, batch 66/72, loss: 0.3535\n",
      "Epoch 22, batch 71/72, loss: 0.2965\n",
      "Epoch 22, train loss: 0.2209, train accuracy: 92.81%\n",
      "Epoch 22, val loss: 0.4365, val accuracy: 85.07%\n",
      "Epoch 23, batch 1/72, loss: 0.1780\n",
      "Epoch 23, batch 6/72, loss: 0.1987\n",
      "Epoch 23, batch 11/72, loss: 0.1300\n",
      "Epoch 23, batch 16/72, loss: 0.1224\n",
      "Epoch 23, batch 21/72, loss: 0.1388\n",
      "Epoch 23, batch 26/72, loss: 0.1959\n",
      "Epoch 23, batch 31/72, loss: 0.2111\n",
      "Epoch 23, batch 36/72, loss: 0.1285\n",
      "Epoch 23, batch 41/72, loss: 0.2338\n",
      "Epoch 23, batch 46/72, loss: 0.1220\n",
      "Epoch 23, batch 51/72, loss: 0.2365\n",
      "Epoch 23, batch 56/72, loss: 0.1753\n",
      "Epoch 23, batch 61/72, loss: 0.1589\n",
      "Epoch 23, batch 66/72, loss: 0.2103\n",
      "Epoch 23, batch 71/72, loss: 0.1377\n",
      "Epoch 23, train loss: 0.1903, train accuracy: 93.85%\n",
      "Epoch 23, val loss: 0.3840, val accuracy: 87.80%\n",
      "Epoch 24, batch 1/72, loss: 0.1465\n",
      "Epoch 24, batch 6/72, loss: 0.1105\n",
      "Epoch 24, batch 11/72, loss: 0.0945\n",
      "Epoch 24, batch 16/72, loss: 0.1262\n",
      "Epoch 24, batch 21/72, loss: 0.2120\n",
      "Epoch 24, batch 26/72, loss: 0.1077\n",
      "Epoch 24, batch 31/72, loss: 0.1372\n",
      "Epoch 24, batch 36/72, loss: 0.2272\n",
      "Epoch 24, batch 41/72, loss: 0.2849\n",
      "Epoch 24, batch 46/72, loss: 0.2185\n",
      "Epoch 24, batch 51/72, loss: 0.2445\n",
      "Epoch 24, batch 56/72, loss: 0.1418\n",
      "Epoch 24, batch 61/72, loss: 0.1624\n",
      "Epoch 24, batch 66/72, loss: 0.1930\n",
      "Epoch 24, batch 71/72, loss: 0.2724\n",
      "Epoch 24, train loss: 0.1729, train accuracy: 94.73%\n",
      "Epoch 24, val loss: 0.4016, val accuracy: 87.02%\n",
      "Epoch 25, batch 1/72, loss: 0.1581\n",
      "Epoch 25, batch 6/72, loss: 0.2355\n",
      "Epoch 25, batch 11/72, loss: 0.1488\n",
      "Epoch 25, batch 16/72, loss: 0.1234\n",
      "Epoch 25, batch 21/72, loss: 0.2150\n",
      "Epoch 25, batch 26/72, loss: 0.1669\n",
      "Epoch 25, batch 31/72, loss: 0.1465\n",
      "Epoch 25, batch 36/72, loss: 0.0831\n",
      "Epoch 25, batch 41/72, loss: 0.1883\n",
      "Epoch 25, batch 46/72, loss: 0.1556\n",
      "Epoch 25, batch 51/72, loss: 0.1774\n",
      "Epoch 25, batch 56/72, loss: 0.1842\n",
      "Epoch 25, batch 61/72, loss: 0.2540\n",
      "Epoch 25, batch 66/72, loss: 0.1827\n",
      "Epoch 25, batch 71/72, loss: 0.1657\n",
      "Epoch 25, train loss: 0.1578, train accuracy: 95.27%\n",
      "Epoch 25, val loss: 0.3891, val accuracy: 86.98%\n",
      "Epoch 26, batch 1/72, loss: 0.1202\n",
      "Epoch 26, batch 6/72, loss: 0.1150\n",
      "Epoch 26, batch 11/72, loss: 0.1173\n",
      "Epoch 26, batch 16/72, loss: 0.1362\n",
      "Epoch 26, batch 21/72, loss: 0.0851\n",
      "Epoch 26, batch 26/72, loss: 0.1411\n",
      "Epoch 26, batch 31/72, loss: 0.1408\n",
      "Epoch 26, batch 36/72, loss: 0.1444\n",
      "Epoch 26, batch 41/72, loss: 0.2014\n",
      "Epoch 26, batch 46/72, loss: 0.2194\n",
      "Epoch 26, batch 51/72, loss: 0.1762\n",
      "Epoch 26, batch 56/72, loss: 0.1640\n",
      "Epoch 26, batch 61/72, loss: 0.2210\n",
      "Epoch 26, batch 66/72, loss: 0.1751\n",
      "Epoch 26, batch 71/72, loss: 0.1109\n",
      "Epoch 26, train loss: 0.1517, train accuracy: 95.38%\n",
      "Epoch 26, val loss: 0.4045, val accuracy: 86.20%\n",
      "Epoch 27, batch 1/72, loss: 0.2072\n",
      "Epoch 27, batch 6/72, loss: 0.1514\n",
      "Epoch 27, batch 11/72, loss: 0.1394\n",
      "Epoch 27, batch 16/72, loss: 0.2116\n",
      "Epoch 27, batch 21/72, loss: 0.1408\n",
      "Epoch 27, batch 26/72, loss: 0.1884\n",
      "Epoch 27, batch 31/72, loss: 0.1312\n",
      "Epoch 27, batch 36/72, loss: 0.2156\n",
      "Epoch 27, batch 41/72, loss: 0.1234\n",
      "Epoch 27, batch 46/72, loss: 0.1918\n",
      "Epoch 27, batch 51/72, loss: 0.1549\n",
      "Epoch 27, batch 56/72, loss: 0.1587\n",
      "Epoch 27, batch 61/72, loss: 0.2810\n",
      "Epoch 27, batch 66/72, loss: 0.1507\n",
      "Epoch 27, batch 71/72, loss: 0.0694\n",
      "Epoch 27, train loss: 0.1528, train accuracy: 95.23%\n",
      "Epoch 27, val loss: 0.3685, val accuracy: 87.15%\n",
      "Epoch 28, batch 1/72, loss: 0.0887\n",
      "Epoch 28, batch 6/72, loss: 0.1221\n",
      "Epoch 28, batch 11/72, loss: 0.1551\n",
      "Epoch 28, batch 16/72, loss: 0.0968\n",
      "Epoch 28, batch 21/72, loss: 0.1837\n",
      "Epoch 28, batch 26/72, loss: 0.1047\n",
      "Epoch 28, batch 31/72, loss: 0.1971\n",
      "Epoch 28, batch 36/72, loss: 0.1529\n",
      "Epoch 28, batch 41/72, loss: 0.1701\n",
      "Epoch 28, batch 46/72, loss: 0.1677\n",
      "Epoch 28, batch 51/72, loss: 0.1772\n",
      "Epoch 28, batch 56/72, loss: 0.1821\n",
      "Epoch 28, batch 61/72, loss: 0.1261\n",
      "Epoch 28, batch 66/72, loss: 0.1352\n",
      "Epoch 28, batch 71/72, loss: 0.1712\n",
      "Epoch 28, train loss: 0.1469, train accuracy: 95.75%\n",
      "Epoch 28, val loss: 0.4004, val accuracy: 86.33%\n",
      "Epoch 29, batch 1/72, loss: 0.1737\n",
      "Epoch 29, batch 6/72, loss: 0.1411\n",
      "Epoch 29, batch 11/72, loss: 0.1383\n",
      "Epoch 29, batch 16/72, loss: 0.1659\n",
      "Epoch 29, batch 21/72, loss: 0.0923\n",
      "Epoch 29, batch 26/72, loss: 0.1362\n",
      "Epoch 29, batch 31/72, loss: 0.1069\n",
      "Epoch 29, batch 36/72, loss: 0.2362\n",
      "Epoch 29, batch 41/72, loss: 0.1796\n",
      "Epoch 29, batch 46/72, loss: 0.2516\n",
      "Epoch 29, batch 51/72, loss: 0.1782\n",
      "Epoch 29, batch 56/72, loss: 0.1396\n",
      "Epoch 29, batch 61/72, loss: 0.1908\n",
      "Epoch 29, batch 66/72, loss: 0.1398\n",
      "Epoch 29, batch 71/72, loss: 0.2185\n",
      "Epoch 29, train loss: 0.1602, train accuracy: 94.81%\n",
      "Epoch 29, val loss: 0.4146, val accuracy: 86.24%\n",
      "Epoch 30, batch 1/72, loss: 0.1324\n",
      "Epoch 30, batch 6/72, loss: 0.1322\n",
      "Epoch 30, batch 11/72, loss: 0.1471\n",
      "Epoch 30, batch 16/72, loss: 0.1547\n",
      "Epoch 30, batch 21/72, loss: 0.2106\n",
      "Epoch 30, batch 26/72, loss: 0.1095\n",
      "Epoch 30, batch 31/72, loss: 0.1930\n",
      "Epoch 30, batch 36/72, loss: 0.1779\n",
      "Epoch 30, batch 41/72, loss: 0.1534\n",
      "Epoch 30, batch 46/72, loss: 0.1006\n",
      "Epoch 30, batch 51/72, loss: 0.1211\n",
      "Epoch 30, batch 56/72, loss: 0.1512\n",
      "Epoch 30, batch 61/72, loss: 0.1126\n",
      "Epoch 30, batch 66/72, loss: 0.1399\n",
      "Epoch 30, batch 71/72, loss: 0.1319\n",
      "Epoch 30, train loss: 0.1483, train accuracy: 95.41%\n",
      "Epoch 30, val loss: 0.3896, val accuracy: 87.67%\n",
      "Epoch 31, batch 1/72, loss: 0.0721\n",
      "Epoch 31, batch 6/72, loss: 0.1709\n",
      "Epoch 31, batch 11/72, loss: 0.1525\n",
      "Epoch 31, batch 16/72, loss: 0.1592\n",
      "Epoch 31, batch 21/72, loss: 0.1390\n",
      "Epoch 31, batch 26/72, loss: 0.1802\n",
      "Epoch 31, batch 31/72, loss: 0.1550\n",
      "Epoch 31, batch 36/72, loss: 0.1708\n",
      "Epoch 31, batch 41/72, loss: 0.1523\n",
      "Epoch 31, batch 46/72, loss: 0.1859\n",
      "Epoch 31, batch 51/72, loss: 0.0866\n",
      "Epoch 31, batch 56/72, loss: 0.1504\n",
      "Epoch 31, batch 61/72, loss: 0.1211\n",
      "Epoch 31, batch 66/72, loss: 0.1020\n",
      "Epoch 31, batch 71/72, loss: 0.2238\n",
      "Epoch 31, train loss: 0.1287, train accuracy: 96.17%\n",
      "Epoch 31, val loss: 0.3609, val accuracy: 88.06%\n",
      "Epoch 32, batch 1/72, loss: 0.0972\n",
      "Epoch 32, batch 6/72, loss: 0.1425\n",
      "Epoch 32, batch 11/72, loss: 0.0520\n",
      "Epoch 32, batch 16/72, loss: 0.1412\n",
      "Epoch 32, batch 21/72, loss: 0.1333\n",
      "Epoch 32, batch 26/72, loss: 0.1500\n",
      "Epoch 32, batch 31/72, loss: 0.1132\n",
      "Epoch 32, batch 36/72, loss: 0.1772\n",
      "Epoch 32, batch 41/72, loss: 0.1054\n",
      "Epoch 32, batch 46/72, loss: 0.0647\n",
      "Epoch 32, batch 51/72, loss: 0.0733\n",
      "Epoch 32, batch 56/72, loss: 0.1315\n",
      "Epoch 32, batch 61/72, loss: 0.0956\n",
      "Epoch 32, batch 66/72, loss: 0.1662\n",
      "Epoch 32, batch 71/72, loss: 0.1476\n",
      "Epoch 32, train loss: 0.1190, train accuracy: 96.47%\n",
      "Epoch 32, val loss: 0.3730, val accuracy: 87.37%\n",
      "Epoch 33, batch 1/72, loss: 0.0777\n",
      "Epoch 33, batch 6/72, loss: 0.1541\n",
      "Epoch 33, batch 11/72, loss: 0.0825\n",
      "Epoch 33, batch 16/72, loss: 0.1201\n",
      "Epoch 33, batch 21/72, loss: 0.1063\n",
      "Epoch 33, batch 26/72, loss: 0.1548\n",
      "Epoch 33, batch 31/72, loss: 0.0667\n",
      "Epoch 33, batch 36/72, loss: 0.0916\n",
      "Epoch 33, batch 41/72, loss: 0.0894\n",
      "Epoch 33, batch 46/72, loss: 0.1048\n",
      "Epoch 33, batch 51/72, loss: 0.1351\n",
      "Epoch 33, batch 56/72, loss: 0.0795\n",
      "Epoch 33, batch 61/72, loss: 0.0589\n",
      "Epoch 33, batch 66/72, loss: 0.0651\n",
      "Epoch 33, batch 71/72, loss: 0.2179\n",
      "Epoch 33, train loss: 0.1025, train accuracy: 97.19%\n",
      "Epoch 33, val loss: 0.3341, val accuracy: 89.19%\n",
      "Epoch 34, batch 1/72, loss: 0.0552\n",
      "Epoch 34, batch 6/72, loss: 0.1221\n",
      "Epoch 34, batch 11/72, loss: 0.1687\n",
      "Epoch 34, batch 16/72, loss: 0.0721\n",
      "Epoch 34, batch 21/72, loss: 0.0794\n",
      "Epoch 34, batch 26/72, loss: 0.0971\n",
      "Epoch 34, batch 31/72, loss: 0.0943\n",
      "Epoch 34, batch 36/72, loss: 0.0582\n",
      "Epoch 34, batch 41/72, loss: 0.0697\n",
      "Epoch 34, batch 46/72, loss: 0.0933\n",
      "Epoch 34, batch 51/72, loss: 0.1015\n",
      "Epoch 34, batch 56/72, loss: 0.1083\n",
      "Epoch 34, batch 61/72, loss: 0.2028\n",
      "Epoch 34, batch 66/72, loss: 0.0774\n",
      "Epoch 34, batch 71/72, loss: 0.0783\n",
      "Epoch 34, train loss: 0.0940, train accuracy: 97.53%\n",
      "Epoch 34, val loss: 0.3688, val accuracy: 87.80%\n",
      "Epoch 35, batch 1/72, loss: 0.0853\n",
      "Epoch 35, batch 6/72, loss: 0.0796\n",
      "Epoch 35, batch 11/72, loss: 0.1675\n",
      "Epoch 35, batch 16/72, loss: 0.1171\n",
      "Epoch 35, batch 21/72, loss: 0.1073\n",
      "Epoch 35, batch 26/72, loss: 0.0754\n",
      "Epoch 35, batch 31/72, loss: 0.0771\n",
      "Epoch 35, batch 36/72, loss: 0.0711\n",
      "Epoch 35, batch 41/72, loss: 0.0839\n",
      "Epoch 35, batch 46/72, loss: 0.0643\n",
      "Epoch 35, batch 51/72, loss: 0.1161\n",
      "Epoch 35, batch 56/72, loss: 0.0401\n",
      "Epoch 35, batch 61/72, loss: 0.0973\n",
      "Epoch 35, batch 66/72, loss: 0.0864\n",
      "Epoch 35, batch 71/72, loss: 0.0711\n",
      "Epoch 35, train loss: 0.0917, train accuracy: 97.49%\n",
      "Epoch 35, val loss: 0.3570, val accuracy: 88.45%\n",
      "Epoch 36, batch 1/72, loss: 0.0908\n",
      "Epoch 36, batch 6/72, loss: 0.1007\n",
      "Epoch 36, batch 11/72, loss: 0.0814\n",
      "Epoch 36, batch 16/72, loss: 0.0869\n",
      "Epoch 36, batch 21/72, loss: 0.0502\n",
      "Epoch 36, batch 26/72, loss: 0.0820\n",
      "Epoch 36, batch 31/72, loss: 0.0548\n",
      "Epoch 36, batch 36/72, loss: 0.0743\n",
      "Epoch 36, batch 41/72, loss: 0.1164\n",
      "Epoch 36, batch 46/72, loss: 0.0883\n",
      "Epoch 36, batch 51/72, loss: 0.0620\n",
      "Epoch 36, batch 56/72, loss: 0.0881\n",
      "Epoch 36, batch 61/72, loss: 0.0816\n",
      "Epoch 36, batch 66/72, loss: 0.1107\n",
      "Epoch 36, batch 71/72, loss: 0.0657\n",
      "Epoch 36, train loss: 0.0841, train accuracy: 97.75%\n",
      "Epoch 36, val loss: 0.3659, val accuracy: 87.85%\n",
      "Epoch 37, batch 1/72, loss: 0.1067\n",
      "Epoch 37, batch 6/72, loss: 0.1036\n",
      "Epoch 37, batch 11/72, loss: 0.1086\n",
      "Epoch 37, batch 16/72, loss: 0.0736\n",
      "Epoch 37, batch 21/72, loss: 0.0457\n",
      "Epoch 37, batch 26/72, loss: 0.0737\n",
      "Epoch 37, batch 31/72, loss: 0.0665\n",
      "Epoch 37, batch 36/72, loss: 0.0830\n",
      "Epoch 37, batch 41/72, loss: 0.0486\n",
      "Epoch 37, batch 46/72, loss: 0.1584\n",
      "Epoch 37, batch 51/72, loss: 0.0734\n",
      "Epoch 37, batch 56/72, loss: 0.0978\n",
      "Epoch 37, batch 61/72, loss: 0.1360\n",
      "Epoch 37, batch 66/72, loss: 0.1390\n",
      "Epoch 37, batch 71/72, loss: 0.0800\n",
      "Epoch 37, train loss: 0.0967, train accuracy: 97.16%\n",
      "Epoch 37, val loss: 0.3717, val accuracy: 87.76%\n",
      "Epoch 38, batch 1/72, loss: 0.0596\n",
      "Epoch 38, batch 6/72, loss: 0.1173\n",
      "Epoch 38, batch 11/72, loss: 0.2021\n",
      "Epoch 38, batch 16/72, loss: 0.0632\n",
      "Epoch 38, batch 21/72, loss: 0.0685\n",
      "Epoch 38, batch 26/72, loss: 0.1131\n",
      "Epoch 38, batch 31/72, loss: 0.1263\n",
      "Epoch 38, batch 36/72, loss: 0.1041\n",
      "Epoch 38, batch 41/72, loss: 0.0709\n",
      "Epoch 38, batch 46/72, loss: 0.1093\n",
      "Epoch 38, batch 51/72, loss: 0.0593\n",
      "Epoch 38, batch 56/72, loss: 0.0714\n",
      "Epoch 38, batch 61/72, loss: 0.0592\n",
      "Epoch 38, batch 66/72, loss: 0.0528\n",
      "Epoch 38, batch 71/72, loss: 0.0633\n",
      "Epoch 38, train loss: 0.0907, train accuracy: 97.43%\n",
      "Epoch 38, val loss: 0.3862, val accuracy: 87.50%\n",
      "Epoch 39, batch 1/72, loss: 0.0866\n",
      "Epoch 39, batch 6/72, loss: 0.0343\n",
      "Epoch 39, batch 11/72, loss: 0.0274\n",
      "Epoch 39, batch 16/72, loss: 0.0921\n",
      "Epoch 39, batch 21/72, loss: 0.0294\n",
      "Epoch 39, batch 26/72, loss: 0.0703\n",
      "Epoch 39, batch 31/72, loss: 0.0880\n",
      "Epoch 39, batch 36/72, loss: 0.0958\n",
      "Epoch 39, batch 41/72, loss: 0.0616\n",
      "Epoch 39, batch 46/72, loss: 0.1007\n",
      "Epoch 39, batch 51/72, loss: 0.0875\n",
      "Epoch 39, batch 56/72, loss: 0.0804\n",
      "Epoch 39, batch 61/72, loss: 0.0985\n",
      "Epoch 39, batch 66/72, loss: 0.0503\n",
      "Epoch 39, batch 71/72, loss: 0.0628\n",
      "Epoch 39, train loss: 0.0656, train accuracy: 98.50%\n",
      "Epoch 39, val loss: 0.3508, val accuracy: 88.19%\n",
      "Epoch 40, batch 1/72, loss: 0.0774\n",
      "Epoch 40, batch 6/72, loss: 0.0334\n",
      "Epoch 40, batch 11/72, loss: 0.1443\n",
      "Epoch 40, batch 16/72, loss: 0.1575\n",
      "Epoch 40, batch 21/72, loss: 0.0523\n",
      "Epoch 40, batch 26/72, loss: 0.0357\n",
      "Epoch 40, batch 31/72, loss: 0.0828\n",
      "Epoch 40, batch 36/72, loss: 0.0576\n",
      "Epoch 40, batch 41/72, loss: 0.0476\n",
      "Epoch 40, batch 46/72, loss: 0.0835\n",
      "Epoch 40, batch 51/72, loss: 0.0786\n",
      "Epoch 40, batch 56/72, loss: 0.0389\n",
      "Epoch 40, batch 61/72, loss: 0.0862\n",
      "Epoch 40, batch 66/72, loss: 0.0301\n",
      "Epoch 40, batch 71/72, loss: 0.0478\n",
      "Epoch 40, train loss: 0.0691, train accuracy: 98.26%\n",
      "Epoch 40, val loss: 0.3676, val accuracy: 88.24%\n",
      "Epoch 41, batch 1/72, loss: 0.0385\n",
      "Epoch 41, batch 6/72, loss: 0.1039\n",
      "Epoch 41, batch 11/72, loss: 0.0509\n",
      "Epoch 41, batch 16/72, loss: 0.0634\n",
      "Epoch 41, batch 21/72, loss: 0.0514\n",
      "Epoch 41, batch 26/72, loss: 0.0551\n",
      "Epoch 41, batch 31/72, loss: 0.0747\n",
      "Epoch 41, batch 36/72, loss: 0.0540\n",
      "Epoch 41, batch 41/72, loss: 0.0481\n",
      "Epoch 41, batch 46/72, loss: 0.0683\n",
      "Epoch 41, batch 51/72, loss: 0.1183\n",
      "Epoch 41, batch 56/72, loss: 0.0625\n",
      "Epoch 41, batch 61/72, loss: 0.0560\n",
      "Epoch 41, batch 66/72, loss: 0.0773\n",
      "Epoch 41, batch 71/72, loss: 0.0754\n",
      "Epoch 41, train loss: 0.0675, train accuracy: 98.26%\n",
      "Epoch 41, val loss: 0.3578, val accuracy: 89.24%\n",
      "Epoch 42, batch 1/72, loss: 0.1067\n",
      "Epoch 42, batch 6/72, loss: 0.0514\n",
      "Epoch 42, batch 11/72, loss: 0.0406\n",
      "Epoch 42, batch 16/72, loss: 0.0871\n",
      "Epoch 42, batch 21/72, loss: 0.0432\n",
      "Epoch 42, batch 26/72, loss: 0.0612\n",
      "Epoch 42, batch 31/72, loss: 0.0577\n",
      "Epoch 42, batch 36/72, loss: 0.0619\n",
      "Epoch 42, batch 41/72, loss: 0.0331\n",
      "Epoch 42, batch 46/72, loss: 0.0721\n",
      "Epoch 42, batch 51/72, loss: 0.0386\n",
      "Epoch 42, batch 56/72, loss: 0.0928\n",
      "Epoch 42, batch 61/72, loss: 0.0509\n",
      "Epoch 42, batch 66/72, loss: 0.0323\n",
      "Epoch 42, batch 71/72, loss: 0.0550\n",
      "Epoch 42, train loss: 0.0599, train accuracy: 98.60%\n",
      "Epoch 42, val loss: 0.3613, val accuracy: 88.72%\n",
      "Epoch 43, batch 1/72, loss: 0.0409\n",
      "Epoch 43, batch 6/72, loss: 0.0387\n",
      "Epoch 43, batch 11/72, loss: 0.0646\n",
      "Epoch 43, batch 16/72, loss: 0.0725\n",
      "Epoch 43, batch 21/72, loss: 0.0487\n",
      "Epoch 43, batch 26/72, loss: 0.0223\n",
      "Epoch 43, batch 31/72, loss: 0.0411\n",
      "Epoch 43, batch 36/72, loss: 0.1058\n",
      "Epoch 43, batch 41/72, loss: 0.0549\n",
      "Epoch 43, batch 46/72, loss: 0.1278\n",
      "Epoch 43, batch 51/72, loss: 0.1251\n",
      "Epoch 43, batch 56/72, loss: 0.0727\n",
      "Epoch 43, batch 61/72, loss: 0.0373\n",
      "Epoch 43, batch 66/72, loss: 0.0483\n",
      "Epoch 43, batch 71/72, loss: 0.0774\n",
      "Epoch 43, train loss: 0.0720, train accuracy: 97.97%\n",
      "Epoch 43, val loss: 0.3454, val accuracy: 89.02%\n",
      "Epoch 44, batch 1/72, loss: 0.0296\n",
      "Epoch 44, batch 6/72, loss: 0.0564\n",
      "Epoch 44, batch 11/72, loss: 0.0301\n",
      "Epoch 44, batch 16/72, loss: 0.0385\n",
      "Epoch 44, batch 21/72, loss: 0.0601\n",
      "Epoch 44, batch 26/72, loss: 0.0376\n",
      "Epoch 44, batch 31/72, loss: 0.0580\n",
      "Epoch 44, batch 36/72, loss: 0.0198\n",
      "Epoch 44, batch 41/72, loss: 0.0433\n",
      "Epoch 44, batch 46/72, loss: 0.0562\n",
      "Epoch 44, batch 51/72, loss: 0.0720\n",
      "Epoch 44, batch 56/72, loss: 0.0345\n",
      "Epoch 44, batch 61/72, loss: 0.0878\n",
      "Epoch 44, batch 66/72, loss: 0.0600\n",
      "Epoch 44, batch 71/72, loss: 0.0529\n",
      "Epoch 44, train loss: 0.0516, train accuracy: 98.74%\n",
      "Epoch 44, val loss: 0.3417, val accuracy: 88.93%\n",
      "Epoch 45, batch 1/72, loss: 0.0383\n",
      "Epoch 45, batch 6/72, loss: 0.0718\n",
      "Epoch 45, batch 11/72, loss: 0.0347\n",
      "Epoch 45, batch 16/72, loss: 0.0365\n",
      "Epoch 45, batch 21/72, loss: 0.0414\n",
      "Epoch 45, batch 26/72, loss: 0.0686\n",
      "Epoch 45, batch 31/72, loss: 0.0352\n",
      "Epoch 45, batch 36/72, loss: 0.0267\n",
      "Epoch 45, batch 41/72, loss: 0.0231\n",
      "Epoch 45, batch 46/72, loss: 0.0549\n",
      "Epoch 45, batch 51/72, loss: 0.0310\n",
      "Epoch 45, batch 56/72, loss: 0.0433\n",
      "Epoch 45, batch 61/72, loss: 0.0676\n",
      "Epoch 45, batch 66/72, loss: 0.0285\n",
      "Epoch 45, batch 71/72, loss: 0.0329\n",
      "Epoch 45, train loss: 0.0510, train accuracy: 98.95%\n",
      "Epoch 45, val loss: 0.3496, val accuracy: 89.15%\n",
      "Epoch 46, batch 1/72, loss: 0.0399\n",
      "Epoch 46, batch 6/72, loss: 0.0189\n",
      "Epoch 46, batch 11/72, loss: 0.0549\n",
      "Epoch 46, batch 16/72, loss: 0.1461\n",
      "Epoch 46, batch 21/72, loss: 0.0656\n",
      "Epoch 46, batch 26/72, loss: 0.0708\n",
      "Epoch 46, batch 31/72, loss: 0.0573\n",
      "Epoch 46, batch 36/72, loss: 0.0435\n",
      "Epoch 46, batch 41/72, loss: 0.0625\n",
      "Epoch 46, batch 46/72, loss: 0.0275\n",
      "Epoch 46, batch 51/72, loss: 0.0543\n",
      "Epoch 46, batch 56/72, loss: 0.1084\n",
      "Epoch 46, batch 61/72, loss: 0.0810\n",
      "Epoch 46, batch 66/72, loss: 0.0394\n",
      "Epoch 46, batch 71/72, loss: 0.0595\n",
      "Epoch 46, train loss: 0.0566, train accuracy: 98.71%\n",
      "Epoch 46, val loss: 0.3566, val accuracy: 88.76%\n",
      "Epoch 47, batch 1/72, loss: 0.0556\n",
      "Epoch 47, batch 6/72, loss: 0.0522\n",
      "Epoch 47, batch 11/72, loss: 0.0557\n",
      "Epoch 47, batch 16/72, loss: 0.0516\n",
      "Epoch 47, batch 21/72, loss: 0.0372\n",
      "Epoch 47, batch 26/72, loss: 0.0334\n",
      "Epoch 47, batch 31/72, loss: 0.0442\n",
      "Epoch 47, batch 36/72, loss: 0.0680\n",
      "Epoch 47, batch 41/72, loss: 0.0604\n",
      "Epoch 47, batch 46/72, loss: 0.0579\n",
      "Epoch 47, batch 51/72, loss: 0.0469\n",
      "Epoch 47, batch 56/72, loss: 0.0379\n",
      "Epoch 47, batch 61/72, loss: 0.0317\n",
      "Epoch 47, batch 66/72, loss: 0.0704\n",
      "Epoch 47, batch 71/72, loss: 0.0618\n",
      "Epoch 47, train loss: 0.0476, train accuracy: 98.85%\n",
      "Epoch 47, val loss: 0.3934, val accuracy: 88.37%\n",
      "Epoch 48, batch 1/72, loss: 0.0398\n",
      "Epoch 48, batch 6/72, loss: 0.0696\n",
      "Epoch 48, batch 11/72, loss: 0.0605\n",
      "Epoch 48, batch 16/72, loss: 0.0446\n",
      "Epoch 48, batch 21/72, loss: 0.0348\n",
      "Epoch 48, batch 26/72, loss: 0.1601\n",
      "Epoch 48, batch 31/72, loss: 0.0200\n",
      "Epoch 48, batch 36/72, loss: 0.0425\n",
      "Epoch 48, batch 41/72, loss: 0.0909\n",
      "Epoch 48, batch 46/72, loss: 0.0408\n",
      "Epoch 48, batch 51/72, loss: 0.0406\n",
      "Epoch 48, batch 56/72, loss: 0.0279\n",
      "Epoch 48, batch 61/72, loss: 0.0505\n",
      "Epoch 48, batch 66/72, loss: 0.0598\n",
      "Epoch 48, batch 71/72, loss: 0.0366\n",
      "Epoch 48, train loss: 0.0539, train accuracy: 98.58%\n",
      "Epoch 48, val loss: 0.3645, val accuracy: 88.85%\n",
      "Epoch 49, batch 1/72, loss: 0.0406\n",
      "Epoch 49, batch 6/72, loss: 0.0504\n",
      "Epoch 49, batch 11/72, loss: 0.0284\n",
      "Epoch 49, batch 16/72, loss: 0.0422\n",
      "Epoch 49, batch 21/72, loss: 0.0419\n",
      "Epoch 49, batch 26/72, loss: 0.0493\n",
      "Epoch 49, batch 31/72, loss: 0.0392\n",
      "Epoch 49, batch 36/72, loss: 0.0554\n",
      "Epoch 49, batch 41/72, loss: 0.0692\n",
      "Epoch 49, batch 46/72, loss: 0.0662\n",
      "Epoch 49, batch 51/72, loss: 0.0338\n",
      "Epoch 49, batch 56/72, loss: 0.0505\n",
      "Epoch 49, batch 61/72, loss: 0.0344\n",
      "Epoch 49, batch 66/72, loss: 0.1133\n",
      "Epoch 49, batch 71/72, loss: 0.0632\n",
      "Epoch 49, train loss: 0.0458, train accuracy: 98.97%\n",
      "Epoch 49, val loss: 0.3402, val accuracy: 89.37%\n",
      "Epoch 50, batch 1/72, loss: 0.0816\n",
      "Epoch 50, batch 6/72, loss: 0.0307\n",
      "Epoch 50, batch 11/72, loss: 0.0218\n",
      "Epoch 50, batch 16/72, loss: 0.0274\n",
      "Epoch 50, batch 21/72, loss: 0.0828\n",
      "Epoch 50, batch 26/72, loss: 0.0493\n",
      "Epoch 50, batch 31/72, loss: 0.0312\n",
      "Epoch 50, batch 36/72, loss: 0.0307\n",
      "Epoch 50, batch 41/72, loss: 0.0155\n",
      "Epoch 50, batch 46/72, loss: 0.0783\n",
      "Epoch 50, batch 51/72, loss: 0.0658\n",
      "Epoch 50, batch 56/72, loss: 0.0421\n",
      "Epoch 50, batch 61/72, loss: 0.0521\n",
      "Epoch 50, batch 66/72, loss: 0.0502\n",
      "Epoch 50, batch 71/72, loss: 0.0576\n",
      "Epoch 50, train loss: 0.0518, train accuracy: 98.77%\n",
      "Epoch 50, val loss: 0.4203, val accuracy: 86.07%\n",
      "Epoch 51, batch 1/72, loss: 0.1829\n",
      "Epoch 51, batch 6/72, loss: 0.1142\n",
      "Epoch 51, batch 11/72, loss: 0.0836\n",
      "Epoch 51, batch 16/72, loss: 0.0937\n",
      "Epoch 51, batch 21/72, loss: 0.1016\n",
      "Epoch 51, batch 26/72, loss: 0.1148\n",
      "Epoch 51, batch 31/72, loss: 0.1227\n",
      "Epoch 51, batch 36/72, loss: 0.0377\n",
      "Epoch 51, batch 41/72, loss: 0.0603\n",
      "Epoch 51, batch 46/72, loss: 0.0512\n",
      "Epoch 51, batch 51/72, loss: 0.1011\n",
      "Epoch 51, batch 56/72, loss: 0.0421\n",
      "Epoch 51, batch 61/72, loss: 0.0435\n",
      "Epoch 51, batch 66/72, loss: 0.0712\n",
      "Epoch 51, batch 71/72, loss: 0.1873\n",
      "Epoch 51, train loss: 0.0915, train accuracy: 97.18%\n",
      "Epoch 51, val loss: 0.4086, val accuracy: 87.11%\n",
      "Epoch 52, batch 1/72, loss: 0.0715\n",
      "Epoch 52, batch 6/72, loss: 0.0730\n",
      "Epoch 52, batch 11/72, loss: 0.0332\n",
      "Epoch 52, batch 16/72, loss: 0.0698\n",
      "Epoch 52, batch 21/72, loss: 0.0590\n",
      "Epoch 52, batch 26/72, loss: 0.1013\n",
      "Epoch 52, batch 31/72, loss: 0.0811\n",
      "Epoch 52, batch 36/72, loss: 0.0508\n",
      "Epoch 52, batch 41/72, loss: 0.0445\n",
      "Epoch 52, batch 46/72, loss: 0.0341\n",
      "Epoch 52, batch 51/72, loss: 0.0318\n",
      "Epoch 52, batch 56/72, loss: 0.0281\n",
      "Epoch 52, batch 61/72, loss: 0.0416\n",
      "Epoch 52, batch 66/72, loss: 0.0427\n",
      "Epoch 52, batch 71/72, loss: 0.0609\n",
      "Epoch 52, train loss: 0.0611, train accuracy: 98.32%\n",
      "Epoch 52, val loss: 0.3684, val accuracy: 88.85%\n",
      "Epoch 53, batch 1/72, loss: 0.0469\n",
      "Epoch 53, batch 6/72, loss: 0.0429\n",
      "Epoch 53, batch 11/72, loss: 0.0413\n",
      "Epoch 53, batch 16/72, loss: 0.0503\n",
      "Epoch 53, batch 21/72, loss: 0.0312\n",
      "Epoch 53, batch 26/72, loss: 0.0365\n",
      "Epoch 53, batch 31/72, loss: 0.0277\n",
      "Epoch 53, batch 36/72, loss: 0.0453\n",
      "Epoch 53, batch 41/72, loss: 0.0773\n",
      "Epoch 53, batch 46/72, loss: 0.0304\n",
      "Epoch 53, batch 51/72, loss: 0.0371\n",
      "Epoch 53, batch 56/72, loss: 0.0212\n",
      "Epoch 53, batch 61/72, loss: 0.0174\n",
      "Epoch 53, batch 66/72, loss: 0.0347\n",
      "Epoch 53, batch 71/72, loss: 0.0480\n",
      "Epoch 53, train loss: 0.0399, train accuracy: 99.18%\n",
      "Epoch 53, val loss: 0.3592, val accuracy: 88.93%\n",
      "Epoch 54, batch 1/72, loss: 0.0520\n",
      "Epoch 54, batch 6/72, loss: 0.0365\n",
      "Epoch 54, batch 11/72, loss: 0.0534\n",
      "Epoch 54, batch 16/72, loss: 0.0224\n",
      "Epoch 54, batch 21/72, loss: 0.0246\n",
      "Epoch 54, batch 26/72, loss: 0.0558\n",
      "Epoch 54, batch 31/72, loss: 0.0775\n",
      "Epoch 54, batch 36/72, loss: 0.0460\n",
      "Epoch 54, batch 41/72, loss: 0.0426\n",
      "Epoch 54, batch 46/72, loss: 0.0539\n",
      "Epoch 54, batch 51/72, loss: 0.0634\n",
      "Epoch 54, batch 56/72, loss: 0.0295\n",
      "Epoch 54, batch 61/72, loss: 0.0248\n",
      "Epoch 54, batch 66/72, loss: 0.0265\n",
      "Epoch 54, batch 71/72, loss: 0.0893\n",
      "Epoch 54, train loss: 0.0435, train accuracy: 98.91%\n",
      "Epoch 54, val loss: 0.3592, val accuracy: 88.80%\n",
      "Epoch 55, batch 1/72, loss: 0.0302\n",
      "Epoch 55, batch 6/72, loss: 0.0300\n",
      "Epoch 55, batch 11/72, loss: 0.0233\n",
      "Epoch 55, batch 16/72, loss: 0.0167\n",
      "Epoch 55, batch 21/72, loss: 0.0254\n",
      "Epoch 55, batch 26/72, loss: 0.0202\n",
      "Epoch 55, batch 31/72, loss: 0.0325\n",
      "Epoch 55, batch 36/72, loss: 0.0349\n",
      "Epoch 55, batch 41/72, loss: 0.0212\n",
      "Epoch 55, batch 46/72, loss: 0.0410\n",
      "Epoch 55, batch 51/72, loss: 0.0336\n",
      "Epoch 55, batch 56/72, loss: 0.0154\n",
      "Epoch 55, batch 61/72, loss: 0.0245\n",
      "Epoch 55, batch 66/72, loss: 0.0359\n",
      "Epoch 55, batch 71/72, loss: 0.0281\n",
      "Epoch 55, train loss: 0.0321, train accuracy: 99.31%\n",
      "Epoch 55, val loss: 0.3693, val accuracy: 89.19%\n",
      "Epoch 56, batch 1/72, loss: 0.0346\n",
      "Epoch 56, batch 6/72, loss: 0.0175\n",
      "Epoch 56, batch 11/72, loss: 0.0223\n",
      "Epoch 56, batch 16/72, loss: 0.0266\n",
      "Epoch 56, batch 21/72, loss: 0.0127\n",
      "Epoch 56, batch 26/72, loss: 0.0230\n",
      "Epoch 56, batch 31/72, loss: 0.0103\n",
      "Epoch 56, batch 36/72, loss: 0.0485\n",
      "Epoch 56, batch 41/72, loss: 0.0373\n",
      "Epoch 56, batch 46/72, loss: 0.0204\n",
      "Epoch 56, batch 51/72, loss: 0.0269\n",
      "Epoch 56, batch 56/72, loss: 0.0165\n",
      "Epoch 56, batch 61/72, loss: 0.0224\n",
      "Epoch 56, batch 66/72, loss: 0.0339\n",
      "Epoch 56, batch 71/72, loss: 0.0470\n",
      "Epoch 56, train loss: 0.0279, train accuracy: 99.54%\n",
      "Epoch 56, val loss: 0.3638, val accuracy: 89.02%\n",
      "Epoch 57, batch 1/72, loss: 0.0498\n",
      "Epoch 57, batch 6/72, loss: 0.0313\n",
      "Epoch 57, batch 11/72, loss: 0.0327\n",
      "Epoch 57, batch 16/72, loss: 0.0211\n",
      "Epoch 57, batch 21/72, loss: 0.0582\n",
      "Epoch 57, batch 26/72, loss: 0.0195\n",
      "Epoch 57, batch 31/72, loss: 0.0553\n",
      "Epoch 57, batch 36/72, loss: 0.0479\n",
      "Epoch 57, batch 41/72, loss: 0.0311\n",
      "Epoch 57, batch 46/72, loss: 0.0612\n",
      "Epoch 57, batch 51/72, loss: 0.0218\n",
      "Epoch 57, batch 56/72, loss: 0.0600\n",
      "Epoch 57, batch 61/72, loss: 0.0236\n",
      "Epoch 57, batch 66/72, loss: 0.0378\n",
      "Epoch 57, batch 71/72, loss: 0.0265\n",
      "Epoch 57, train loss: 0.0333, train accuracy: 99.21%\n",
      "Epoch 57, val loss: 0.3677, val accuracy: 88.76%\n",
      "Epoch 58, batch 1/72, loss: 0.0303\n",
      "Epoch 58, batch 6/72, loss: 0.0155\n",
      "Epoch 58, batch 11/72, loss: 0.0226\n",
      "Epoch 58, batch 16/72, loss: 0.0170\n",
      "Epoch 58, batch 21/72, loss: 0.0630\n",
      "Epoch 58, batch 26/72, loss: 0.0363\n",
      "Epoch 58, batch 31/72, loss: 0.0296\n",
      "Epoch 58, batch 36/72, loss: 0.0189\n",
      "Epoch 58, batch 41/72, loss: 0.0503\n",
      "Epoch 58, batch 46/72, loss: 0.0445\n",
      "Epoch 58, batch 51/72, loss: 0.0663\n",
      "Epoch 58, batch 56/72, loss: 0.0433\n",
      "Epoch 58, batch 61/72, loss: 0.0438\n",
      "Epoch 58, batch 66/72, loss: 0.0607\n",
      "Epoch 58, batch 71/72, loss: 0.0420\n",
      "Epoch 58, train loss: 0.0370, train accuracy: 99.10%\n",
      "Epoch 58, val loss: 0.3461, val accuracy: 89.37%\n",
      "Epoch 59, batch 1/72, loss: 0.0340\n",
      "Epoch 59, batch 6/72, loss: 0.0511\n",
      "Epoch 59, batch 11/72, loss: 0.0491\n",
      "Epoch 59, batch 16/72, loss: 0.0356\n",
      "Epoch 59, batch 21/72, loss: 0.0488\n",
      "Epoch 59, batch 26/72, loss: 0.0263\n",
      "Epoch 59, batch 31/72, loss: 0.0496\n",
      "Epoch 59, batch 36/72, loss: 0.0264\n",
      "Epoch 59, batch 41/72, loss: 0.0170\n",
      "Epoch 59, batch 46/72, loss: 0.0281\n",
      "Epoch 59, batch 51/72, loss: 0.0091\n",
      "Epoch 59, batch 56/72, loss: 0.0173\n",
      "Epoch 59, batch 61/72, loss: 0.0147\n",
      "Epoch 59, batch 66/72, loss: 0.0122\n",
      "Epoch 59, batch 71/72, loss: 0.0185\n",
      "Epoch 59, train loss: 0.0290, train accuracy: 99.45%\n",
      "Epoch 59, val loss: 0.3590, val accuracy: 88.85%\n",
      "Epoch 60, batch 1/72, loss: 0.0307\n",
      "Epoch 60, batch 6/72, loss: 0.0094\n",
      "Epoch 60, batch 11/72, loss: 0.0275\n",
      "Epoch 60, batch 16/72, loss: 0.0105\n",
      "Epoch 60, batch 21/72, loss: 0.0153\n",
      "Epoch 60, batch 26/72, loss: 0.0208\n",
      "Epoch 60, batch 31/72, loss: 0.0174\n",
      "Epoch 60, batch 36/72, loss: 0.0260\n",
      "Epoch 60, batch 41/72, loss: 0.0149\n",
      "Epoch 60, batch 46/72, loss: 0.0277\n",
      "Epoch 60, batch 51/72, loss: 0.0133\n",
      "Epoch 60, batch 56/72, loss: 0.0273\n",
      "Epoch 60, batch 61/72, loss: 0.0128\n",
      "Epoch 60, batch 66/72, loss: 0.0229\n",
      "Epoch 60, batch 71/72, loss: 0.0260\n",
      "Epoch 60, train loss: 0.0228, train accuracy: 99.62%\n",
      "Epoch 60, val loss: 0.3544, val accuracy: 89.11%\n",
      "Epoch 61, batch 1/72, loss: 0.0253\n",
      "Epoch 61, batch 6/72, loss: 0.0188\n",
      "Epoch 61, batch 11/72, loss: 0.0137\n",
      "Epoch 61, batch 16/72, loss: 0.0136\n",
      "Epoch 61, batch 21/72, loss: 0.0230\n",
      "Epoch 61, batch 26/72, loss: 0.0136\n",
      "Epoch 61, batch 31/72, loss: 0.0112\n",
      "Epoch 61, batch 36/72, loss: 0.0181\n",
      "Epoch 61, batch 41/72, loss: 0.0122\n",
      "Epoch 61, batch 46/72, loss: 0.0215\n",
      "Epoch 61, batch 51/72, loss: 0.0180\n",
      "Epoch 61, batch 56/72, loss: 0.0441\n",
      "Epoch 61, batch 61/72, loss: 0.0589\n",
      "Epoch 61, batch 66/72, loss: 0.0210\n",
      "Epoch 61, batch 71/72, loss: 0.0136\n",
      "Epoch 61, train loss: 0.0244, train accuracy: 99.56%\n",
      "Epoch 61, val loss: 0.3829, val accuracy: 88.63%\n",
      "Epoch 62, batch 1/72, loss: 0.0165\n",
      "Epoch 62, batch 6/72, loss: 0.0150\n",
      "Epoch 62, batch 11/72, loss: 0.0300\n",
      "Epoch 62, batch 16/72, loss: 0.0159\n",
      "Epoch 62, batch 21/72, loss: 0.0191\n",
      "Epoch 62, batch 26/72, loss: 0.0125\n",
      "Epoch 62, batch 31/72, loss: 0.0622\n",
      "Epoch 62, batch 36/72, loss: 0.0114\n",
      "Epoch 62, batch 41/72, loss: 0.0161\n",
      "Epoch 62, batch 46/72, loss: 0.0127\n",
      "Epoch 62, batch 51/72, loss: 0.0103\n",
      "Epoch 62, batch 56/72, loss: 0.0083\n",
      "Epoch 62, batch 61/72, loss: 0.0178\n",
      "Epoch 62, batch 66/72, loss: 0.0414\n",
      "Epoch 62, batch 71/72, loss: 0.0132\n",
      "Epoch 62, train loss: 0.0260, train accuracy: 99.49%\n",
      "Epoch 62, val loss: 0.3494, val accuracy: 89.71%\n",
      "Epoch 63, batch 1/72, loss: 0.0066\n",
      "Epoch 63, batch 6/72, loss: 0.0220\n",
      "Epoch 63, batch 11/72, loss: 0.0100\n",
      "Epoch 63, batch 16/72, loss: 0.0157\n",
      "Epoch 63, batch 21/72, loss: 0.0243\n",
      "Epoch 63, batch 26/72, loss: 0.0168\n",
      "Epoch 63, batch 31/72, loss: 0.0243\n",
      "Epoch 63, batch 36/72, loss: 0.0358\n",
      "Epoch 63, batch 41/72, loss: 0.0539\n",
      "Epoch 63, batch 46/72, loss: 0.0199\n",
      "Epoch 63, batch 51/72, loss: 0.0541\n",
      "Epoch 63, batch 56/72, loss: 0.0150\n",
      "Epoch 63, batch 61/72, loss: 0.0249\n",
      "Epoch 63, batch 66/72, loss: 0.0148\n",
      "Epoch 63, batch 71/72, loss: 0.0208\n",
      "Epoch 63, train loss: 0.0253, train accuracy: 99.53%\n",
      "Epoch 63, val loss: 0.3673, val accuracy: 88.98%\n",
      "Epoch 64, batch 1/72, loss: 0.0203\n",
      "Epoch 64, batch 6/72, loss: 0.0287\n",
      "Epoch 64, batch 11/72, loss: 0.1026\n",
      "Epoch 64, batch 16/72, loss: 0.0490\n",
      "Epoch 64, batch 21/72, loss: 0.0341\n",
      "Epoch 64, batch 26/72, loss: 0.0097\n",
      "Epoch 64, batch 31/72, loss: 0.0145\n",
      "Epoch 64, batch 36/72, loss: 0.0237\n",
      "Epoch 64, batch 41/72, loss: 0.0166\n",
      "Epoch 64, batch 46/72, loss: 0.0125\n",
      "Epoch 64, batch 51/72, loss: 0.0343\n",
      "Epoch 64, batch 56/72, loss: 0.0278\n",
      "Epoch 64, batch 61/72, loss: 0.0212\n",
      "Epoch 64, batch 66/72, loss: 0.0157\n",
      "Epoch 64, batch 71/72, loss: 0.0168\n",
      "Epoch 64, train loss: 0.0264, train accuracy: 99.53%\n",
      "Epoch 64, val loss: 0.3763, val accuracy: 88.63%\n",
      "Epoch 65, batch 1/72, loss: 0.0351\n",
      "Epoch 65, batch 6/72, loss: 0.0502\n",
      "Epoch 65, batch 11/72, loss: 0.0236\n",
      "Epoch 65, batch 16/72, loss: 0.0230\n",
      "Epoch 65, batch 21/72, loss: 0.0237\n",
      "Epoch 65, batch 26/72, loss: 0.0297\n",
      "Epoch 65, batch 31/72, loss: 0.0298\n",
      "Epoch 65, batch 36/72, loss: 0.0316\n",
      "Epoch 65, batch 41/72, loss: 0.0359\n",
      "Epoch 65, batch 46/72, loss: 0.0288\n",
      "Epoch 65, batch 51/72, loss: 0.0479\n",
      "Epoch 65, batch 56/72, loss: 0.0129\n",
      "Epoch 65, batch 61/72, loss: 0.0216\n",
      "Epoch 65, batch 66/72, loss: 0.0158\n",
      "Epoch 65, batch 71/72, loss: 0.0112\n",
      "Epoch 65, train loss: 0.0284, train accuracy: 99.27%\n",
      "Epoch 65, val loss: 0.3622, val accuracy: 89.11%\n",
      "Epoch 66, batch 1/72, loss: 0.0454\n",
      "Epoch 66, batch 6/72, loss: 0.0319\n",
      "Epoch 66, batch 11/72, loss: 0.0135\n",
      "Epoch 66, batch 16/72, loss: 0.0213\n",
      "Epoch 66, batch 21/72, loss: 0.0362\n",
      "Epoch 66, batch 26/72, loss: 0.0402\n",
      "Epoch 66, batch 31/72, loss: 0.0185\n",
      "Epoch 66, batch 36/72, loss: 0.0253\n",
      "Epoch 66, batch 41/72, loss: 0.0137\n",
      "Epoch 66, batch 46/72, loss: 0.0182\n",
      "Epoch 66, batch 51/72, loss: 0.0216\n",
      "Epoch 66, batch 56/72, loss: 0.0134\n",
      "Epoch 66, batch 61/72, loss: 0.0107\n",
      "Epoch 66, batch 66/72, loss: 0.0177\n",
      "Epoch 66, batch 71/72, loss: 0.0195\n",
      "Epoch 66, train loss: 0.0216, train accuracy: 99.60%\n",
      "Epoch 66, val loss: 0.3485, val accuracy: 89.28%\n",
      "Epoch 67, batch 1/72, loss: 0.0194\n",
      "Epoch 67, batch 6/72, loss: 0.0177\n",
      "Epoch 67, batch 11/72, loss: 0.0132\n",
      "Epoch 67, batch 16/72, loss: 0.0088\n",
      "Epoch 67, batch 21/72, loss: 0.0059\n",
      "Epoch 67, batch 26/72, loss: 0.0087\n",
      "Epoch 67, batch 31/72, loss: 0.0058\n",
      "Epoch 67, batch 36/72, loss: 0.0078\n",
      "Epoch 67, batch 41/72, loss: 0.0130\n",
      "Epoch 67, batch 46/72, loss: 0.0158\n",
      "Epoch 67, batch 51/72, loss: 0.0070\n",
      "Epoch 67, batch 56/72, loss: 0.0161\n",
      "Epoch 67, batch 61/72, loss: 0.0110\n",
      "Epoch 67, batch 66/72, loss: 0.0250\n",
      "Epoch 67, batch 71/72, loss: 0.0157\n",
      "Epoch 67, train loss: 0.0159, train accuracy: 99.79%\n",
      "Epoch 67, val loss: 0.3482, val accuracy: 88.89%\n",
      "Epoch 68, batch 1/72, loss: 0.0291\n",
      "Epoch 68, batch 6/72, loss: 0.0175\n",
      "Epoch 68, batch 11/72, loss: 0.0062\n",
      "Epoch 68, batch 16/72, loss: 0.0165\n",
      "Epoch 68, batch 21/72, loss: 0.0330\n",
      "Epoch 68, batch 26/72, loss: 0.0123\n",
      "Epoch 68, batch 31/72, loss: 0.0090\n",
      "Epoch 68, batch 36/72, loss: 0.0146\n",
      "Epoch 68, batch 41/72, loss: 0.0154\n",
      "Epoch 68, batch 46/72, loss: 0.0080\n",
      "Epoch 68, batch 51/72, loss: 0.0139\n",
      "Epoch 68, batch 56/72, loss: 0.0229\n",
      "Epoch 68, batch 61/72, loss: 0.0126\n",
      "Epoch 68, batch 66/72, loss: 0.0071\n",
      "Epoch 68, batch 71/72, loss: 0.0422\n",
      "Epoch 68, train loss: 0.0168, train accuracy: 99.73%\n",
      "Epoch 68, val loss: 0.3709, val accuracy: 89.11%\n",
      "Epoch 69, batch 1/72, loss: 0.0212\n",
      "Epoch 69, batch 6/72, loss: 0.0086\n",
      "Epoch 69, batch 11/72, loss: 0.0051\n",
      "Epoch 69, batch 16/72, loss: 0.0078\n",
      "Epoch 69, batch 21/72, loss: 0.0479\n",
      "Epoch 69, batch 26/72, loss: 0.0194\n",
      "Epoch 69, batch 31/72, loss: 0.0092\n",
      "Epoch 69, batch 36/72, loss: 0.0177\n",
      "Epoch 69, batch 41/72, loss: 0.0233\n",
      "Epoch 69, batch 46/72, loss: 0.0233\n",
      "Epoch 69, batch 51/72, loss: 0.0190\n",
      "Epoch 69, batch 56/72, loss: 0.0278\n",
      "Epoch 69, batch 61/72, loss: 0.0081\n",
      "Epoch 69, batch 66/72, loss: 0.0226\n",
      "Epoch 69, batch 71/72, loss: 0.0097\n",
      "Epoch 69, train loss: 0.0179, train accuracy: 99.67%\n",
      "Epoch 69, val loss: 0.3813, val accuracy: 88.76%\n",
      "Epoch 70, batch 1/72, loss: 0.0162\n",
      "Epoch 70, batch 6/72, loss: 0.0098\n",
      "Epoch 70, batch 11/72, loss: 0.0185\n",
      "Epoch 70, batch 16/72, loss: 0.0274\n",
      "Epoch 70, batch 21/72, loss: 0.0131\n",
      "Epoch 70, batch 26/72, loss: 0.0454\n",
      "Epoch 70, batch 31/72, loss: 0.0117\n",
      "Epoch 70, batch 36/72, loss: 0.0221\n",
      "Epoch 70, batch 41/72, loss: 0.0157\n",
      "Epoch 70, batch 46/72, loss: 0.0503\n",
      "Epoch 70, batch 51/72, loss: 0.0251\n",
      "Epoch 70, batch 56/72, loss: 0.0417\n",
      "Epoch 70, batch 61/72, loss: 0.0414\n",
      "Epoch 70, batch 66/72, loss: 0.0101\n",
      "Epoch 70, batch 71/72, loss: 0.0288\n",
      "Epoch 70, train loss: 0.0238, train accuracy: 99.37%\n",
      "Epoch 70, val loss: 0.3603, val accuracy: 89.15%\n",
      "Epoch 71, batch 1/72, loss: 0.0278\n",
      "Epoch 71, batch 6/72, loss: 0.0272\n",
      "Epoch 71, batch 11/72, loss: 0.0385\n",
      "Epoch 71, batch 16/72, loss: 0.0149\n",
      "Epoch 71, batch 21/72, loss: 0.0070\n",
      "Epoch 71, batch 26/72, loss: 0.0382\n",
      "Epoch 71, batch 31/72, loss: 0.0162\n",
      "Epoch 71, batch 36/72, loss: 0.0084\n",
      "Epoch 71, batch 41/72, loss: 0.0115\n",
      "Epoch 71, batch 46/72, loss: 0.0132\n",
      "Epoch 71, batch 51/72, loss: 0.0172\n",
      "Epoch 71, batch 56/72, loss: 0.0137\n",
      "Epoch 71, batch 61/72, loss: 0.0112\n",
      "Epoch 71, batch 66/72, loss: 0.0093\n",
      "Epoch 71, batch 71/72, loss: 0.0170\n",
      "Epoch 71, train loss: 0.0178, train accuracy: 99.70%\n",
      "Epoch 71, val loss: 0.3562, val accuracy: 89.89%\n",
      "Epoch 72, batch 1/72, loss: 0.0149\n",
      "Epoch 72, batch 6/72, loss: 0.0436\n",
      "Epoch 72, batch 11/72, loss: 0.0172\n",
      "Epoch 72, batch 16/72, loss: 0.0149\n",
      "Epoch 72, batch 21/72, loss: 0.0193\n",
      "Epoch 72, batch 26/72, loss: 0.0147\n",
      "Epoch 72, batch 31/72, loss: 0.0221\n",
      "Epoch 72, batch 36/72, loss: 0.0348\n",
      "Epoch 72, batch 41/72, loss: 0.0137\n",
      "Epoch 72, batch 46/72, loss: 0.0256\n",
      "Epoch 72, batch 51/72, loss: 0.0241\n",
      "Epoch 72, batch 56/72, loss: 0.0245\n",
      "Epoch 72, batch 61/72, loss: 0.0177\n",
      "Epoch 72, batch 66/72, loss: 0.0131\n",
      "Epoch 72, batch 71/72, loss: 0.0142\n",
      "Epoch 72, train loss: 0.0224, train accuracy: 99.45%\n",
      "Epoch 72, val loss: 0.4114, val accuracy: 88.11%\n",
      "Epoch 73, batch 1/72, loss: 0.0244\n",
      "Epoch 73, batch 6/72, loss: 0.0543\n",
      "Epoch 73, batch 11/72, loss: 0.0080\n",
      "Epoch 73, batch 16/72, loss: 0.0597\n",
      "Epoch 73, batch 21/72, loss: 0.0334\n",
      "Epoch 73, batch 26/72, loss: 0.0253\n",
      "Epoch 73, batch 31/72, loss: 0.0306\n",
      "Epoch 73, batch 36/72, loss: 0.0100\n",
      "Epoch 73, batch 41/72, loss: 0.0158\n",
      "Epoch 73, batch 46/72, loss: 0.0095\n",
      "Epoch 73, batch 51/72, loss: 0.0092\n",
      "Epoch 73, batch 56/72, loss: 0.0257\n",
      "Epoch 73, batch 61/72, loss: 0.0214\n",
      "Epoch 73, batch 66/72, loss: 0.0466\n",
      "Epoch 73, batch 71/72, loss: 0.0781\n",
      "Epoch 73, train loss: 0.0263, train accuracy: 99.36%\n",
      "Epoch 73, val loss: 0.3893, val accuracy: 88.72%\n",
      "Epoch 74, batch 1/72, loss: 0.0681\n",
      "Epoch 74, batch 6/72, loss: 0.0189\n",
      "Epoch 74, batch 11/72, loss: 0.0172\n",
      "Epoch 74, batch 16/72, loss: 0.0113\n",
      "Epoch 74, batch 21/72, loss: 0.0121\n",
      "Epoch 74, batch 26/72, loss: 0.0108\n",
      "Epoch 74, batch 31/72, loss: 0.0247\n",
      "Epoch 74, batch 36/72, loss: 0.0154\n",
      "Epoch 74, batch 41/72, loss: 0.0052\n",
      "Epoch 74, batch 46/72, loss: 0.0149\n",
      "Epoch 74, batch 51/72, loss: 0.0384\n",
      "Epoch 74, batch 56/72, loss: 0.0170\n",
      "Epoch 74, batch 61/72, loss: 0.0149\n",
      "Epoch 74, batch 66/72, loss: 0.0067\n",
      "Epoch 74, batch 71/72, loss: 0.0077\n",
      "Epoch 74, train loss: 0.0206, train accuracy: 99.66%\n",
      "Epoch 74, val loss: 0.3684, val accuracy: 89.32%\n",
      "Epoch 75, batch 1/72, loss: 0.0138\n",
      "Epoch 75, batch 6/72, loss: 0.0245\n",
      "Epoch 75, batch 11/72, loss: 0.0554\n",
      "Epoch 75, batch 16/72, loss: 0.0151\n",
      "Epoch 75, batch 21/72, loss: 0.0113\n",
      "Epoch 75, batch 26/72, loss: 0.0172\n",
      "Epoch 75, batch 31/72, loss: 0.0124\n",
      "Epoch 75, batch 36/72, loss: 0.0163\n",
      "Epoch 75, batch 41/72, loss: 0.0095\n",
      "Epoch 75, batch 46/72, loss: 0.0061\n",
      "Epoch 75, batch 51/72, loss: 0.0117\n",
      "Epoch 75, batch 56/72, loss: 0.0207\n",
      "Epoch 75, batch 61/72, loss: 0.0106\n",
      "Epoch 75, batch 66/72, loss: 0.0079\n",
      "Epoch 75, batch 71/72, loss: 0.0405\n",
      "Epoch 75, train loss: 0.0166, train accuracy: 99.67%\n",
      "Epoch 75, val loss: 0.3513, val accuracy: 89.93%\n",
      "Epoch 76, batch 1/72, loss: 0.0133\n",
      "Epoch 76, batch 6/72, loss: 0.0083\n",
      "Epoch 76, batch 11/72, loss: 0.0050\n",
      "Epoch 76, batch 16/72, loss: 0.0275\n",
      "Epoch 76, batch 21/72, loss: 0.0086\n",
      "Epoch 76, batch 26/72, loss: 0.0086\n",
      "Epoch 76, batch 31/72, loss: 0.0351\n",
      "Epoch 76, batch 36/72, loss: 0.0108\n",
      "Epoch 76, batch 41/72, loss: 0.0156\n",
      "Epoch 76, batch 46/72, loss: 0.0149\n",
      "Epoch 76, batch 51/72, loss: 0.0270\n",
      "Epoch 76, batch 56/72, loss: 0.0347\n",
      "Epoch 76, batch 61/72, loss: 0.0096\n",
      "Epoch 76, batch 66/72, loss: 0.0196\n",
      "Epoch 76, batch 71/72, loss: 0.0126\n",
      "Epoch 76, train loss: 0.0174, train accuracy: 99.65%\n",
      "Epoch 76, val loss: 0.3605, val accuracy: 89.24%\n",
      "Epoch 77, batch 1/72, loss: 0.0222\n",
      "Epoch 77, batch 6/72, loss: 0.0180\n",
      "Epoch 77, batch 11/72, loss: 0.0121\n",
      "Epoch 77, batch 16/72, loss: 0.0161\n",
      "Epoch 77, batch 21/72, loss: 0.0085\n",
      "Epoch 77, batch 26/72, loss: 0.0095\n",
      "Epoch 77, batch 31/72, loss: 0.0208\n",
      "Epoch 77, batch 36/72, loss: 0.0114\n",
      "Epoch 77, batch 41/72, loss: 0.0132\n",
      "Epoch 77, batch 46/72, loss: 0.0114\n",
      "Epoch 77, batch 51/72, loss: 0.0142\n",
      "Epoch 77, batch 56/72, loss: 0.0165\n",
      "Epoch 77, batch 61/72, loss: 0.0044\n",
      "Epoch 77, batch 66/72, loss: 0.0059\n",
      "Epoch 77, batch 71/72, loss: 0.0139\n",
      "Epoch 77, train loss: 0.0146, train accuracy: 99.78%\n",
      "Epoch 77, val loss: 0.3860, val accuracy: 88.54%\n",
      "Epoch 78, batch 1/72, loss: 0.0488\n",
      "Epoch 78, batch 6/72, loss: 0.0667\n",
      "Epoch 78, batch 11/72, loss: 0.0336\n",
      "Epoch 78, batch 16/72, loss: 0.0087\n",
      "Epoch 78, batch 21/72, loss: 0.0332\n",
      "Epoch 78, batch 26/72, loss: 0.0226\n",
      "Epoch 78, batch 31/72, loss: 0.0327\n",
      "Epoch 78, batch 36/72, loss: 0.0351\n",
      "Epoch 78, batch 41/72, loss: 0.0414\n",
      "Epoch 78, batch 46/72, loss: 0.0655\n",
      "Epoch 78, batch 51/72, loss: 0.0381\n",
      "Epoch 78, batch 56/72, loss: 0.0231\n",
      "Epoch 78, batch 61/72, loss: 0.0334\n",
      "Epoch 78, batch 66/72, loss: 0.0292\n",
      "Epoch 78, batch 71/72, loss: 0.0138\n",
      "Epoch 78, train loss: 0.0304, train accuracy: 99.18%\n",
      "Epoch 78, val loss: 0.3713, val accuracy: 88.93%\n",
      "Epoch 79, batch 1/72, loss: 0.0185\n",
      "Epoch 79, batch 6/72, loss: 0.0102\n",
      "Epoch 79, batch 11/72, loss: 0.0162\n",
      "Epoch 79, batch 16/72, loss: 0.0320\n",
      "Epoch 79, batch 21/72, loss: 0.0064\n",
      "Epoch 79, batch 26/72, loss: 0.0105\n",
      "Epoch 79, batch 31/72, loss: 0.0263\n",
      "Epoch 79, batch 36/72, loss: 0.0103\n",
      "Epoch 79, batch 41/72, loss: 0.0048\n",
      "Epoch 79, batch 46/72, loss: 0.0253\n",
      "Epoch 79, batch 51/72, loss: 0.0079\n",
      "Epoch 79, batch 56/72, loss: 0.0154\n",
      "Epoch 79, batch 61/72, loss: 0.0084\n",
      "Epoch 79, batch 66/72, loss: 0.0102\n",
      "Epoch 79, batch 71/72, loss: 0.0092\n",
      "Epoch 79, train loss: 0.0152, train accuracy: 99.83%\n",
      "Epoch 79, val loss: 0.3588, val accuracy: 89.84%\n",
      "Epoch 80, batch 1/72, loss: 0.0172\n",
      "Epoch 80, batch 6/72, loss: 0.0053\n",
      "Epoch 80, batch 11/72, loss: 0.0097\n",
      "Epoch 80, batch 16/72, loss: 0.0132\n",
      "Epoch 80, batch 21/72, loss: 0.0093\n",
      "Epoch 80, batch 26/72, loss: 0.0045\n",
      "Epoch 80, batch 31/72, loss: 0.0071\n",
      "Epoch 80, batch 36/72, loss: 0.0057\n",
      "Epoch 80, batch 41/72, loss: 0.0055\n",
      "Epoch 80, batch 46/72, loss: 0.0043\n",
      "Epoch 80, batch 51/72, loss: 0.0102\n",
      "Epoch 80, batch 56/72, loss: 0.0215\n",
      "Epoch 80, batch 61/72, loss: 0.0071\n",
      "Epoch 80, batch 66/72, loss: 0.0187\n",
      "Epoch 80, batch 71/72, loss: 0.0081\n",
      "Epoch 80, train loss: 0.0134, train accuracy: 99.80%\n",
      "Epoch 80, val loss: 0.3532, val accuracy: 89.37%\n",
      "Epoch 81, batch 1/72, loss: 0.0063\n",
      "Epoch 81, batch 6/72, loss: 0.0054\n",
      "Epoch 81, batch 11/72, loss: 0.0088\n",
      "Epoch 81, batch 16/72, loss: 0.0088\n",
      "Epoch 81, batch 21/72, loss: 0.0192\n",
      "Epoch 81, batch 26/72, loss: 0.0065\n",
      "Epoch 81, batch 31/72, loss: 0.0073\n",
      "Epoch 81, batch 36/72, loss: 0.0365\n",
      "Epoch 81, batch 41/72, loss: 0.0089\n",
      "Epoch 81, batch 46/72, loss: 0.0139\n",
      "Epoch 81, batch 51/72, loss: 0.0133\n",
      "Epoch 81, batch 56/72, loss: 0.0147\n",
      "Epoch 81, batch 61/72, loss: 0.0142\n",
      "Epoch 81, batch 66/72, loss: 0.0108\n",
      "Epoch 81, batch 71/72, loss: 0.0167\n",
      "Epoch 81, train loss: 0.0178, train accuracy: 99.63%\n",
      "Epoch 81, val loss: 0.3749, val accuracy: 89.67%\n",
      "Epoch 82, batch 1/72, loss: 0.0078\n",
      "Epoch 82, batch 6/72, loss: 0.0197\n",
      "Epoch 82, batch 11/72, loss: 0.0118\n",
      "Epoch 82, batch 16/72, loss: 0.0152\n",
      "Epoch 82, batch 21/72, loss: 0.0077\n",
      "Epoch 82, batch 26/72, loss: 0.0147\n",
      "Epoch 82, batch 31/72, loss: 0.0039\n",
      "Epoch 82, batch 36/72, loss: 0.0291\n",
      "Epoch 82, batch 41/72, loss: 0.0082\n",
      "Epoch 82, batch 46/72, loss: 0.0050\n",
      "Epoch 82, batch 51/72, loss: 0.0140\n",
      "Epoch 82, batch 56/72, loss: 0.0038\n",
      "Epoch 82, batch 61/72, loss: 0.0054\n",
      "Epoch 82, batch 66/72, loss: 0.0136\n",
      "Epoch 82, batch 71/72, loss: 0.0039\n",
      "Epoch 82, train loss: 0.0120, train accuracy: 99.83%\n",
      "Epoch 82, val loss: 0.3546, val accuracy: 89.84%\n",
      "Epoch 83, batch 1/72, loss: 0.0094\n",
      "Epoch 83, batch 6/72, loss: 0.0132\n",
      "Epoch 83, batch 11/72, loss: 0.0052\n",
      "Epoch 83, batch 16/72, loss: 0.0074\n",
      "Epoch 83, batch 21/72, loss: 0.0247\n",
      "Epoch 83, batch 26/72, loss: 0.0064\n",
      "Epoch 83, batch 31/72, loss: 0.0078\n",
      "Epoch 83, batch 36/72, loss: 0.0055\n",
      "Epoch 83, batch 41/72, loss: 0.0118\n",
      "Epoch 83, batch 46/72, loss: 0.0173\n",
      "Epoch 83, batch 51/72, loss: 0.0128\n",
      "Epoch 83, batch 56/72, loss: 0.0335\n",
      "Epoch 83, batch 61/72, loss: 0.0064\n",
      "Epoch 83, batch 66/72, loss: 0.0137\n",
      "Epoch 83, batch 71/72, loss: 0.0313\n",
      "Epoch 83, train loss: 0.0138, train accuracy: 99.76%\n",
      "Epoch 83, val loss: 0.3663, val accuracy: 89.11%\n",
      "Epoch 84, batch 1/72, loss: 0.0098\n",
      "Epoch 84, batch 6/72, loss: 0.0109\n",
      "Epoch 84, batch 11/72, loss: 0.0079\n",
      "Epoch 84, batch 16/72, loss: 0.0101\n",
      "Epoch 84, batch 21/72, loss: 0.0072\n",
      "Epoch 84, batch 26/72, loss: 0.0069\n",
      "Epoch 84, batch 31/72, loss: 0.0187\n",
      "Epoch 84, batch 36/72, loss: 0.0056\n",
      "Epoch 84, batch 41/72, loss: 0.0025\n",
      "Epoch 84, batch 46/72, loss: 0.0086\n",
      "Epoch 84, batch 51/72, loss: 0.0597\n",
      "Epoch 84, batch 56/72, loss: 0.0069\n",
      "Epoch 84, batch 61/72, loss: 0.0143\n",
      "Epoch 84, batch 66/72, loss: 0.0067\n",
      "Epoch 84, batch 71/72, loss: 0.0077\n",
      "Epoch 84, train loss: 0.0135, train accuracy: 99.73%\n",
      "Epoch 84, val loss: 0.3929, val accuracy: 88.89%\n",
      "Epoch 85, batch 1/72, loss: 0.0128\n",
      "Epoch 85, batch 6/72, loss: 0.0181\n",
      "Epoch 85, batch 11/72, loss: 0.0207\n",
      "Epoch 85, batch 16/72, loss: 0.0076\n",
      "Epoch 85, batch 21/72, loss: 0.0113\n",
      "Epoch 85, batch 26/72, loss: 0.0184\n",
      "Epoch 85, batch 31/72, loss: 0.0122\n",
      "Epoch 85, batch 36/72, loss: 0.0140\n",
      "Epoch 85, batch 41/72, loss: 0.0129\n",
      "Epoch 85, batch 46/72, loss: 0.0047\n",
      "Epoch 85, batch 51/72, loss: 0.0155\n",
      "Epoch 85, batch 56/72, loss: 0.0055\n",
      "Epoch 85, batch 61/72, loss: 0.0038\n",
      "Epoch 85, batch 66/72, loss: 0.0263\n",
      "Epoch 85, batch 71/72, loss: 0.0119\n",
      "Epoch 85, train loss: 0.0126, train accuracy: 99.74%\n",
      "Epoch 85, val loss: 0.3471, val accuracy: 90.28%\n",
      "Epoch 86, batch 1/72, loss: 0.0046\n",
      "Epoch 86, batch 6/72, loss: 0.0126\n",
      "Epoch 86, batch 11/72, loss: 0.0079\n",
      "Epoch 86, batch 16/72, loss: 0.0082\n",
      "Epoch 86, batch 21/72, loss: 0.0239\n",
      "Epoch 86, batch 26/72, loss: 0.0089\n",
      "Epoch 86, batch 31/72, loss: 0.0099\n",
      "Epoch 86, batch 36/72, loss: 0.0311\n",
      "Epoch 86, batch 41/72, loss: 0.0212\n",
      "Epoch 86, batch 46/72, loss: 0.0258\n",
      "Epoch 86, batch 51/72, loss: 0.0060\n",
      "Epoch 86, batch 56/72, loss: 0.0262\n",
      "Epoch 86, batch 61/72, loss: 0.0199\n",
      "Epoch 86, batch 66/72, loss: 0.0175\n",
      "Epoch 86, batch 71/72, loss: 0.0132\n",
      "Epoch 86, train loss: 0.0152, train accuracy: 99.67%\n",
      "Epoch 86, val loss: 0.3720, val accuracy: 89.54%\n",
      "Epoch 87, batch 1/72, loss: 0.0364\n",
      "Epoch 87, batch 6/72, loss: 0.0110\n",
      "Epoch 87, batch 11/72, loss: 0.0070\n",
      "Epoch 87, batch 16/72, loss: 0.0053\n",
      "Epoch 87, batch 21/72, loss: 0.0224\n",
      "Epoch 87, batch 26/72, loss: 0.0118\n",
      "Epoch 87, batch 31/72, loss: 0.0049\n",
      "Epoch 87, batch 36/72, loss: 0.0129\n",
      "Epoch 87, batch 41/72, loss: 0.0095\n",
      "Epoch 87, batch 46/72, loss: 0.0054\n",
      "Epoch 87, batch 51/72, loss: 0.0059\n",
      "Epoch 87, batch 56/72, loss: 0.0224\n",
      "Epoch 87, batch 61/72, loss: 0.0067\n",
      "Epoch 87, batch 66/72, loss: 0.0109\n",
      "Epoch 87, batch 71/72, loss: 0.0090\n",
      "Epoch 87, train loss: 0.0124, train accuracy: 99.82%\n",
      "Epoch 87, val loss: 0.3589, val accuracy: 89.67%\n",
      "Epoch 88, batch 1/72, loss: 0.0044\n",
      "Epoch 88, batch 6/72, loss: 0.0121\n",
      "Epoch 88, batch 11/72, loss: 0.0212\n",
      "Epoch 88, batch 16/72, loss: 0.0067\n",
      "Epoch 88, batch 21/72, loss: 0.0150\n",
      "Epoch 88, batch 26/72, loss: 0.0088\n",
      "Epoch 88, batch 31/72, loss: 0.0091\n",
      "Epoch 88, batch 36/72, loss: 0.0140\n",
      "Epoch 88, batch 41/72, loss: 0.0031\n",
      "Epoch 88, batch 46/72, loss: 0.0118\n",
      "Epoch 88, batch 51/72, loss: 0.0119\n",
      "Epoch 88, batch 56/72, loss: 0.0108\n",
      "Epoch 88, batch 61/72, loss: 0.0130\n",
      "Epoch 88, batch 66/72, loss: 0.0134\n",
      "Epoch 88, batch 71/72, loss: 0.0220\n",
      "Epoch 88, train loss: 0.0140, train accuracy: 99.76%\n",
      "Epoch 88, val loss: 0.3863, val accuracy: 88.76%\n",
      "Epoch 89, batch 1/72, loss: 0.0051\n",
      "Epoch 89, batch 6/72, loss: 0.0095\n",
      "Epoch 89, batch 11/72, loss: 0.0098\n",
      "Epoch 89, batch 16/72, loss: 0.0059\n",
      "Epoch 89, batch 21/72, loss: 0.0186\n",
      "Epoch 89, batch 26/72, loss: 0.0122\n",
      "Epoch 89, batch 31/72, loss: 0.0158\n",
      "Epoch 89, batch 36/72, loss: 0.0078\n",
      "Epoch 89, batch 41/72, loss: 0.0060\n",
      "Epoch 89, batch 46/72, loss: 0.0101\n",
      "Epoch 89, batch 51/72, loss: 0.0059\n",
      "Epoch 89, batch 56/72, loss: 0.0122\n",
      "Epoch 89, batch 61/72, loss: 0.0060\n",
      "Epoch 89, batch 66/72, loss: 0.0097\n",
      "Epoch 89, batch 71/72, loss: 0.0056\n",
      "Epoch 89, train loss: 0.0140, train accuracy: 99.73%\n",
      "Epoch 89, val loss: 0.3724, val accuracy: 89.24%\n",
      "Epoch 90, batch 1/72, loss: 0.0574\n",
      "Epoch 90, batch 6/72, loss: 0.0078\n",
      "Epoch 90, batch 11/72, loss: 0.0259\n",
      "Epoch 90, batch 16/72, loss: 0.0126\n",
      "Epoch 90, batch 21/72, loss: 0.0155\n",
      "Epoch 90, batch 26/72, loss: 0.0085\n",
      "Epoch 90, batch 31/72, loss: 0.0080\n",
      "Epoch 90, batch 36/72, loss: 0.0044\n",
      "Epoch 90, batch 41/72, loss: 0.0298\n",
      "Epoch 90, batch 46/72, loss: 0.0094\n",
      "Epoch 90, batch 51/72, loss: 0.0044\n",
      "Epoch 90, batch 56/72, loss: 0.0054\n",
      "Epoch 90, batch 61/72, loss: 0.0054\n",
      "Epoch 90, batch 66/72, loss: 0.0076\n",
      "Epoch 90, batch 71/72, loss: 0.0084\n",
      "Epoch 90, train loss: 0.0121, train accuracy: 99.76%\n",
      "Epoch 90, val loss: 0.3718, val accuracy: 89.19%\n",
      "Epoch 91, batch 1/72, loss: 0.0291\n",
      "Epoch 91, batch 6/72, loss: 0.0125\n",
      "Epoch 91, batch 11/72, loss: 0.0100\n",
      "Epoch 91, batch 16/72, loss: 0.0087\n",
      "Epoch 91, batch 21/72, loss: 0.0140\n",
      "Epoch 91, batch 26/72, loss: 0.0078\n",
      "Epoch 91, batch 31/72, loss: 0.0272\n",
      "Epoch 91, batch 36/72, loss: 0.0158\n",
      "Epoch 91, batch 41/72, loss: 0.0172\n",
      "Epoch 91, batch 46/72, loss: 0.0085\n",
      "Epoch 91, batch 51/72, loss: 0.0043\n",
      "Epoch 91, batch 56/72, loss: 0.0043\n",
      "Epoch 91, batch 61/72, loss: 0.0081\n",
      "Epoch 91, batch 66/72, loss: 0.0037\n",
      "Epoch 91, batch 71/72, loss: 0.0115\n",
      "Epoch 91, train loss: 0.0107, train accuracy: 99.85%\n",
      "Epoch 91, val loss: 0.3691, val accuracy: 89.84%\n",
      "Epoch 92, batch 1/72, loss: 0.0174\n",
      "Epoch 92, batch 6/72, loss: 0.0104\n",
      "Epoch 92, batch 11/72, loss: 0.0056\n",
      "Epoch 92, batch 16/72, loss: 0.0151\n",
      "Epoch 92, batch 21/72, loss: 0.0060\n",
      "Epoch 92, batch 26/72, loss: 0.0072\n",
      "Epoch 92, batch 31/72, loss: 0.0065\n",
      "Epoch 92, batch 36/72, loss: 0.0037\n",
      "Epoch 92, batch 41/72, loss: 0.0352\n",
      "Epoch 92, batch 46/72, loss: 0.0125\n",
      "Epoch 92, batch 51/72, loss: 0.0058\n",
      "Epoch 92, batch 56/72, loss: 0.0210\n",
      "Epoch 92, batch 61/72, loss: 0.0431\n",
      "Epoch 92, batch 66/72, loss: 0.0123\n",
      "Epoch 92, batch 71/72, loss: 0.0158\n",
      "Epoch 92, train loss: 0.0165, train accuracy: 99.59%\n",
      "Epoch 92, val loss: 0.3870, val accuracy: 88.76%\n",
      "Epoch 93, batch 1/72, loss: 0.0153\n",
      "Epoch 93, batch 6/72, loss: 0.0394\n",
      "Epoch 93, batch 11/72, loss: 0.0107\n",
      "Epoch 93, batch 16/72, loss: 0.0133\n",
      "Epoch 93, batch 21/72, loss: 0.0329\n",
      "Epoch 93, batch 26/72, loss: 0.0192\n",
      "Epoch 93, batch 31/72, loss: 0.0093\n",
      "Epoch 93, batch 36/72, loss: 0.0247\n",
      "Epoch 93, batch 41/72, loss: 0.0087\n",
      "Epoch 93, batch 46/72, loss: 0.0123\n",
      "Epoch 93, batch 51/72, loss: 0.0173\n",
      "Epoch 93, batch 56/72, loss: 0.0496\n",
      "Epoch 93, batch 61/72, loss: 0.0193\n",
      "Epoch 93, batch 66/72, loss: 0.0168\n",
      "Epoch 93, batch 71/72, loss: 0.0298\n",
      "Epoch 93, train loss: 0.0213, train accuracy: 99.41%\n",
      "Epoch 93, val loss: 0.3784, val accuracy: 88.98%\n",
      "Epoch 94, batch 1/72, loss: 0.0117\n",
      "Epoch 94, batch 6/72, loss: 0.0134\n",
      "Epoch 94, batch 11/72, loss: 0.0236\n",
      "Epoch 94, batch 16/72, loss: 0.0127\n",
      "Epoch 94, batch 21/72, loss: 0.0071\n",
      "Epoch 94, batch 26/72, loss: 0.0114\n",
      "Epoch 94, batch 31/72, loss: 0.0074\n",
      "Epoch 94, batch 36/72, loss: 0.0260\n",
      "Epoch 94, batch 41/72, loss: 0.0075\n",
      "Epoch 94, batch 46/72, loss: 0.0115\n",
      "Epoch 94, batch 51/72, loss: 0.0049\n",
      "Epoch 94, batch 56/72, loss: 0.0037\n",
      "Epoch 94, batch 61/72, loss: 0.0174\n",
      "Epoch 94, batch 66/72, loss: 0.0068\n",
      "Epoch 94, batch 71/72, loss: 0.0127\n",
      "Epoch 94, train loss: 0.0148, train accuracy: 99.74%\n",
      "Epoch 94, val loss: 0.3604, val accuracy: 89.97%\n",
      "Epoch 95, batch 1/72, loss: 0.0198\n",
      "Epoch 95, batch 6/72, loss: 0.0182\n",
      "Epoch 95, batch 11/72, loss: 0.0070\n",
      "Epoch 95, batch 16/72, loss: 0.0155\n",
      "Epoch 95, batch 21/72, loss: 0.0076\n",
      "Epoch 95, batch 26/72, loss: 0.0103\n",
      "Epoch 95, batch 31/72, loss: 0.0214\n",
      "Epoch 95, batch 36/72, loss: 0.0134\n",
      "Epoch 95, batch 41/72, loss: 0.0078\n",
      "Epoch 95, batch 46/72, loss: 0.0065\n",
      "Epoch 95, batch 51/72, loss: 0.0043\n",
      "Epoch 95, batch 56/72, loss: 0.0108\n",
      "Epoch 95, batch 61/72, loss: 0.0063\n",
      "Epoch 95, batch 66/72, loss: 0.0082\n",
      "Epoch 95, batch 71/72, loss: 0.0104\n",
      "Epoch 95, train loss: 0.0122, train accuracy: 99.83%\n",
      "Epoch 95, val loss: 0.3681, val accuracy: 89.02%\n",
      "Epoch 96, batch 1/72, loss: 0.0097\n",
      "Epoch 96, batch 6/72, loss: 0.0036\n",
      "Epoch 96, batch 11/72, loss: 0.0057\n",
      "Epoch 96, batch 16/72, loss: 0.0072\n",
      "Epoch 96, batch 21/72, loss: 0.0111\n",
      "Epoch 96, batch 26/72, loss: 0.0056\n",
      "Epoch 96, batch 31/72, loss: 0.0091\n",
      "Epoch 96, batch 36/72, loss: 0.0046\n",
      "Epoch 96, batch 41/72, loss: 0.0144\n",
      "Epoch 96, batch 46/72, loss: 0.0046\n",
      "Epoch 96, batch 51/72, loss: 0.0052\n",
      "Epoch 96, batch 56/72, loss: 0.0203\n",
      "Epoch 96, batch 61/72, loss: 0.0093\n",
      "Epoch 96, batch 66/72, loss: 0.0223\n",
      "Epoch 96, batch 71/72, loss: 0.0637\n",
      "Epoch 96, train loss: 0.0106, train accuracy: 99.87%\n",
      "Epoch 96, val loss: 0.3892, val accuracy: 89.11%\n",
      "Epoch 97, batch 1/72, loss: 0.0235\n",
      "Epoch 97, batch 6/72, loss: 0.0048\n",
      "Epoch 97, batch 11/72, loss: 0.0131\n",
      "Epoch 97, batch 16/72, loss: 0.0087\n",
      "Epoch 97, batch 21/72, loss: 0.0088\n",
      "Epoch 97, batch 26/72, loss: 0.0048\n",
      "Epoch 97, batch 31/72, loss: 0.0176\n",
      "Epoch 97, batch 36/72, loss: 0.0308\n",
      "Epoch 97, batch 41/72, loss: 0.0274\n",
      "Epoch 97, batch 46/72, loss: 0.0097\n",
      "Epoch 97, batch 51/72, loss: 0.0125\n",
      "Epoch 97, batch 56/72, loss: 0.0067\n",
      "Epoch 97, batch 61/72, loss: 0.0133\n",
      "Epoch 97, batch 66/72, loss: 0.0343\n",
      "Epoch 97, batch 71/72, loss: 0.0100\n",
      "Epoch 97, train loss: 0.0139, train accuracy: 99.70%\n",
      "Epoch 97, val loss: 0.3846, val accuracy: 89.28%\n",
      "Epoch 98, batch 1/72, loss: 0.0186\n",
      "Epoch 98, batch 6/72, loss: 0.0039\n",
      "Epoch 98, batch 11/72, loss: 0.0152\n",
      "Epoch 98, batch 16/72, loss: 0.0047\n",
      "Epoch 98, batch 21/72, loss: 0.0178\n",
      "Epoch 98, batch 26/72, loss: 0.0073\n",
      "Epoch 98, batch 31/72, loss: 0.0253\n",
      "Epoch 98, batch 36/72, loss: 0.0136\n",
      "Epoch 98, batch 41/72, loss: 0.0089\n",
      "Epoch 98, batch 46/72, loss: 0.0077\n",
      "Epoch 98, batch 51/72, loss: 0.0099\n",
      "Epoch 98, batch 56/72, loss: 0.0051\n",
      "Epoch 98, batch 61/72, loss: 0.0043\n",
      "Epoch 98, batch 66/72, loss: 0.0076\n",
      "Epoch 98, batch 71/72, loss: 0.0047\n",
      "Epoch 98, train loss: 0.0120, train accuracy: 99.80%\n",
      "Epoch 98, val loss: 0.3683, val accuracy: 89.58%\n",
      "Epoch 99, batch 1/72, loss: 0.0040\n",
      "Epoch 99, batch 6/72, loss: 0.0053\n",
      "Epoch 99, batch 11/72, loss: 0.0076\n",
      "Epoch 99, batch 16/72, loss: 0.0072\n",
      "Epoch 99, batch 21/72, loss: 0.0065\n",
      "Epoch 99, batch 26/72, loss: 0.0024\n",
      "Epoch 99, batch 31/72, loss: 0.0465\n",
      "Epoch 99, batch 36/72, loss: 0.0415\n",
      "Epoch 99, batch 41/72, loss: 0.0039\n",
      "Epoch 99, batch 46/72, loss: 0.0097\n",
      "Epoch 99, batch 51/72, loss: 0.0104\n",
      "Epoch 99, batch 56/72, loss: 0.0193\n",
      "Epoch 99, batch 61/72, loss: 0.0030\n",
      "Epoch 99, batch 66/72, loss: 0.0079\n",
      "Epoch 99, batch 71/72, loss: 0.0058\n",
      "Epoch 99, train loss: 0.0118, train accuracy: 99.80%\n",
      "Epoch 99, val loss: 0.3959, val accuracy: 89.41%\n",
      "Epoch 100, batch 1/72, loss: 0.0054\n",
      "Epoch 100, batch 6/72, loss: 0.0258\n",
      "Epoch 100, batch 11/72, loss: 0.0120\n",
      "Epoch 100, batch 16/72, loss: 0.0071\n",
      "Epoch 100, batch 21/72, loss: 0.0044\n",
      "Epoch 100, batch 26/72, loss: 0.0118\n",
      "Epoch 100, batch 31/72, loss: 0.0191\n",
      "Epoch 100, batch 36/72, loss: 0.0098\n",
      "Epoch 100, batch 41/72, loss: 0.0145\n",
      "Epoch 100, batch 46/72, loss: 0.0077\n",
      "Epoch 100, batch 51/72, loss: 0.0171\n",
      "Epoch 100, batch 56/72, loss: 0.0047\n",
      "Epoch 100, batch 61/72, loss: 0.0047\n",
      "Epoch 100, batch 66/72, loss: 0.0034\n",
      "Epoch 100, batch 71/72, loss: 0.0055\n",
      "Epoch 100, train loss: 0.0107, train accuracy: 99.83%\n",
      "Epoch 100, val loss: 0.3637, val accuracy: 89.76%\n",
      "Epoch 101, batch 1/72, loss: 0.0105\n",
      "Epoch 101, batch 6/72, loss: 0.0052\n",
      "Epoch 101, batch 11/72, loss: 0.0046\n",
      "Epoch 101, batch 16/72, loss: 0.0017\n",
      "Epoch 101, batch 21/72, loss: 0.0018\n",
      "Epoch 101, batch 26/72, loss: 0.0019\n",
      "Epoch 101, batch 31/72, loss: 0.0021\n",
      "Epoch 101, batch 36/72, loss: 0.0027\n",
      "Epoch 101, batch 41/72, loss: 0.0026\n",
      "Epoch 101, batch 46/72, loss: 0.0094\n",
      "Epoch 101, batch 51/72, loss: 0.0028\n",
      "Epoch 101, batch 56/72, loss: 0.0154\n",
      "Epoch 101, batch 61/72, loss: 0.0044\n",
      "Epoch 101, batch 66/72, loss: 0.0071\n",
      "Epoch 101, batch 71/72, loss: 0.0029\n",
      "Epoch 101, train loss: 0.0056, train accuracy: 99.95%\n",
      "Epoch 101, val loss: 0.3708, val accuracy: 90.02%\n",
      "Epoch 102, batch 1/72, loss: 0.0035\n",
      "Epoch 102, batch 6/72, loss: 0.0033\n",
      "Epoch 102, batch 11/72, loss: 0.0125\n",
      "Epoch 102, batch 16/72, loss: 0.0030\n",
      "Epoch 102, batch 21/72, loss: 0.0052\n",
      "Epoch 102, batch 26/72, loss: 0.0211\n",
      "Epoch 102, batch 31/72, loss: 0.0052\n",
      "Epoch 102, batch 36/72, loss: 0.0140\n",
      "Epoch 102, batch 41/72, loss: 0.0070\n",
      "Epoch 102, batch 46/72, loss: 0.0045\n",
      "Epoch 102, batch 51/72, loss: 0.0178\n",
      "Epoch 102, batch 56/72, loss: 0.0101\n",
      "Epoch 102, batch 61/72, loss: 0.0024\n",
      "Epoch 102, batch 66/72, loss: 0.0087\n",
      "Epoch 102, batch 71/72, loss: 0.0094\n",
      "Epoch 102, train loss: 0.0082, train accuracy: 99.86%\n",
      "Epoch 102, val loss: 0.3956, val accuracy: 88.98%\n",
      "Epoch 103, batch 1/72, loss: 0.0048\n",
      "Epoch 103, batch 6/72, loss: 0.0296\n",
      "Epoch 103, batch 11/72, loss: 0.0093\n",
      "Epoch 103, batch 16/72, loss: 0.0074\n",
      "Epoch 103, batch 21/72, loss: 0.0035\n",
      "Epoch 103, batch 26/72, loss: 0.0035\n",
      "Epoch 103, batch 31/72, loss: 0.0044\n",
      "Epoch 103, batch 36/72, loss: 0.0028\n",
      "Epoch 103, batch 41/72, loss: 0.0036\n",
      "Epoch 103, batch 46/72, loss: 0.0021\n",
      "Epoch 103, batch 51/72, loss: 0.0157\n",
      "Epoch 103, batch 56/72, loss: 0.0122\n",
      "Epoch 103, batch 61/72, loss: 0.0037\n",
      "Epoch 103, batch 66/72, loss: 0.0034\n",
      "Epoch 103, batch 71/72, loss: 0.0062\n",
      "Epoch 103, train loss: 0.0085, train accuracy: 99.90%\n",
      "Epoch 103, val loss: 0.3716, val accuracy: 89.54%\n",
      "Epoch 104, batch 1/72, loss: 0.0028\n",
      "Epoch 104, batch 6/72, loss: 0.0274\n",
      "Epoch 104, batch 11/72, loss: 0.0098\n",
      "Epoch 104, batch 16/72, loss: 0.0137\n",
      "Epoch 104, batch 21/72, loss: 0.0113\n",
      "Epoch 104, batch 26/72, loss: 0.0155\n",
      "Epoch 104, batch 31/72, loss: 0.0109\n",
      "Epoch 104, batch 36/72, loss: 0.0071\n",
      "Epoch 104, batch 41/72, loss: 0.0050\n",
      "Epoch 104, batch 46/72, loss: 0.0054\n",
      "Epoch 104, batch 51/72, loss: 0.0355\n",
      "Epoch 104, batch 56/72, loss: 0.0045\n",
      "Epoch 104, batch 61/72, loss: 0.0064\n",
      "Epoch 104, batch 66/72, loss: 0.0043\n",
      "Epoch 104, batch 71/72, loss: 0.0062\n",
      "Epoch 104, train loss: 0.0100, train accuracy: 99.83%\n",
      "Epoch 104, val loss: 0.3641, val accuracy: 89.54%\n",
      "Epoch 105, batch 1/72, loss: 0.0053\n",
      "Epoch 105, batch 6/72, loss: 0.0051\n",
      "Epoch 105, batch 11/72, loss: 0.0091\n",
      "Epoch 105, batch 16/72, loss: 0.0043\n",
      "Epoch 105, batch 21/72, loss: 0.0085\n",
      "Epoch 105, batch 26/72, loss: 0.0054\n",
      "Epoch 105, batch 31/72, loss: 0.0030\n",
      "Epoch 105, batch 36/72, loss: 0.0044\n",
      "Epoch 105, batch 41/72, loss: 0.0328\n",
      "Epoch 105, batch 46/72, loss: 0.0053\n",
      "Epoch 105, batch 51/72, loss: 0.0077\n",
      "Epoch 105, batch 56/72, loss: 0.0030\n",
      "Epoch 105, batch 61/72, loss: 0.0025\n",
      "Epoch 105, batch 66/72, loss: 0.0151\n",
      "Epoch 105, batch 71/72, loss: 0.0301\n",
      "Epoch 105, train loss: 0.0070, train accuracy: 99.96%\n",
      "Epoch 105, val loss: 0.3568, val accuracy: 90.02%\n",
      "Epoch 106, batch 1/72, loss: 0.0139\n",
      "Epoch 106, batch 6/72, loss: 0.0025\n",
      "Epoch 106, batch 11/72, loss: 0.0110\n",
      "Epoch 106, batch 16/72, loss: 0.0027\n",
      "Epoch 106, batch 21/72, loss: 0.0044\n",
      "Epoch 106, batch 26/72, loss: 0.0050\n",
      "Epoch 106, batch 31/72, loss: 0.0057\n",
      "Epoch 106, batch 36/72, loss: 0.0182\n",
      "Epoch 106, batch 41/72, loss: 0.0029\n",
      "Epoch 106, batch 46/72, loss: 0.0040\n",
      "Epoch 106, batch 51/72, loss: 0.0083\n",
      "Epoch 106, batch 56/72, loss: 0.0043\n",
      "Epoch 106, batch 61/72, loss: 0.0087\n",
      "Epoch 106, batch 66/72, loss: 0.0039\n",
      "Epoch 106, batch 71/72, loss: 0.0030\n",
      "Epoch 106, train loss: 0.0070, train accuracy: 99.95%\n",
      "Epoch 106, val loss: 0.3868, val accuracy: 89.02%\n",
      "Epoch 107, batch 1/72, loss: 0.0124\n",
      "Epoch 107, batch 6/72, loss: 0.0059\n",
      "Epoch 107, batch 11/72, loss: 0.0079\n",
      "Epoch 107, batch 16/72, loss: 0.0047\n",
      "Epoch 107, batch 21/72, loss: 0.0101\n",
      "Epoch 107, batch 26/72, loss: 0.0046\n",
      "Epoch 107, batch 31/72, loss: 0.0019\n",
      "Epoch 107, batch 36/72, loss: 0.0076\n",
      "Epoch 107, batch 41/72, loss: 0.0037\n",
      "Epoch 107, batch 46/72, loss: 0.0086\n",
      "Epoch 107, batch 51/72, loss: 0.0019\n",
      "Epoch 107, batch 56/72, loss: 0.0030\n",
      "Epoch 107, batch 61/72, loss: 0.0094\n",
      "Epoch 107, batch 66/72, loss: 0.0030\n",
      "Epoch 107, batch 71/72, loss: 0.0029\n",
      "Epoch 107, train loss: 0.0062, train accuracy: 99.91%\n",
      "Epoch 107, val loss: 0.3476, val accuracy: 90.23%\n",
      "Epoch 108, batch 1/72, loss: 0.0020\n",
      "Epoch 108, batch 6/72, loss: 0.0073\n",
      "Epoch 108, batch 11/72, loss: 0.0041\n",
      "Epoch 108, batch 16/72, loss: 0.0032\n",
      "Epoch 108, batch 21/72, loss: 0.0124\n",
      "Epoch 108, batch 26/72, loss: 0.0030\n",
      "Epoch 108, batch 31/72, loss: 0.0165\n",
      "Epoch 108, batch 36/72, loss: 0.0026\n",
      "Epoch 108, batch 41/72, loss: 0.0156\n",
      "Epoch 108, batch 46/72, loss: 0.0044\n",
      "Epoch 108, batch 51/72, loss: 0.0026\n",
      "Epoch 108, batch 56/72, loss: 0.0048\n",
      "Epoch 108, batch 61/72, loss: 0.0043\n",
      "Epoch 108, batch 66/72, loss: 0.0044\n",
      "Epoch 108, batch 71/72, loss: 0.0086\n",
      "Epoch 108, train loss: 0.0059, train accuracy: 99.92%\n",
      "Epoch 108, val loss: 0.3546, val accuracy: 90.15%\n",
      "Epoch 109, batch 1/72, loss: 0.0049\n",
      "Epoch 109, batch 6/72, loss: 0.0023\n",
      "Epoch 109, batch 11/72, loss: 0.0014\n",
      "Epoch 109, batch 16/72, loss: 0.0065\n",
      "Epoch 109, batch 21/72, loss: 0.0030\n",
      "Epoch 109, batch 26/72, loss: 0.0067\n",
      "Epoch 109, batch 31/72, loss: 0.0030\n",
      "Epoch 109, batch 36/72, loss: 0.0011\n",
      "Epoch 109, batch 41/72, loss: 0.0152\n",
      "Epoch 109, batch 46/72, loss: 0.0048\n",
      "Epoch 109, batch 51/72, loss: 0.0123\n",
      "Epoch 109, batch 56/72, loss: 0.0074\n",
      "Epoch 109, batch 61/72, loss: 0.0043\n",
      "Epoch 109, batch 66/72, loss: 0.0150\n",
      "Epoch 109, batch 71/72, loss: 0.0059\n",
      "Epoch 109, train loss: 0.0050, train accuracy: 99.95%\n",
      "Epoch 109, val loss: 0.3591, val accuracy: 89.45%\n",
      "Epoch 110, batch 1/72, loss: 0.0030\n",
      "Epoch 110, batch 6/72, loss: 0.0070\n",
      "Epoch 110, batch 11/72, loss: 0.0089\n",
      "Epoch 110, batch 16/72, loss: 0.0025\n",
      "Epoch 110, batch 21/72, loss: 0.0047\n",
      "Epoch 110, batch 26/72, loss: 0.0121\n",
      "Epoch 110, batch 31/72, loss: 0.0023\n",
      "Epoch 110, batch 36/72, loss: 0.0096\n",
      "Epoch 110, batch 41/72, loss: 0.0017\n",
      "Epoch 110, batch 46/72, loss: 0.0303\n",
      "Epoch 110, batch 51/72, loss: 0.0061\n",
      "Epoch 110, batch 56/72, loss: 0.0063\n",
      "Epoch 110, batch 61/72, loss: 0.0021\n",
      "Epoch 110, batch 66/72, loss: 0.0020\n",
      "Epoch 110, batch 71/72, loss: 0.0039\n",
      "Epoch 110, train loss: 0.0082, train accuracy: 99.85%\n",
      "Epoch 110, val loss: 0.3797, val accuracy: 89.58%\n",
      "Epoch 111, batch 1/72, loss: 0.0200\n",
      "Epoch 111, batch 6/72, loss: 0.0044\n",
      "Epoch 111, batch 11/72, loss: 0.0083\n",
      "Epoch 111, batch 16/72, loss: 0.0145\n",
      "Epoch 111, batch 21/72, loss: 0.0067\n",
      "Epoch 111, batch 26/72, loss: 0.0343\n",
      "Epoch 111, batch 31/72, loss: 0.0096\n",
      "Epoch 111, batch 36/72, loss: 0.0152\n",
      "Epoch 111, batch 41/72, loss: 0.0026\n",
      "Epoch 111, batch 46/72, loss: 0.0085\n",
      "Epoch 111, batch 51/72, loss: 0.0174\n",
      "Epoch 111, batch 56/72, loss: 0.0190\n",
      "Epoch 111, batch 61/72, loss: 0.0544\n",
      "Epoch 111, batch 66/72, loss: 0.0130\n",
      "Epoch 111, batch 71/72, loss: 0.0085\n",
      "Epoch 111, train loss: 0.0160, train accuracy: 99.54%\n",
      "Epoch 111, val loss: 0.3765, val accuracy: 88.80%\n",
      "Epoch 112, batch 1/72, loss: 0.0127\n",
      "Epoch 112, batch 6/72, loss: 0.0126\n",
      "Epoch 112, batch 11/72, loss: 0.0052\n",
      "Epoch 112, batch 16/72, loss: 0.0128\n",
      "Epoch 112, batch 21/72, loss: 0.0069\n",
      "Epoch 112, batch 26/72, loss: 0.0030\n",
      "Epoch 112, batch 31/72, loss: 0.0096\n",
      "Epoch 112, batch 36/72, loss: 0.0063\n",
      "Epoch 112, batch 41/72, loss: 0.0057\n",
      "Epoch 112, batch 46/72, loss: 0.0044\n",
      "Epoch 112, batch 51/72, loss: 0.0098\n",
      "Epoch 112, batch 56/72, loss: 0.0093\n",
      "Epoch 112, batch 61/72, loss: 0.0035\n",
      "Epoch 112, batch 66/72, loss: 0.0054\n",
      "Epoch 112, batch 71/72, loss: 0.0147\n",
      "Epoch 112, train loss: 0.0120, train accuracy: 99.77%\n",
      "Epoch 112, val loss: 0.3609, val accuracy: 89.50%\n",
      "Epoch 113, batch 1/72, loss: 0.0045\n",
      "Epoch 113, batch 6/72, loss: 0.0024\n",
      "Epoch 113, batch 11/72, loss: 0.0033\n",
      "Epoch 113, batch 16/72, loss: 0.0099\n",
      "Epoch 113, batch 21/72, loss: 0.0031\n",
      "Epoch 113, batch 26/72, loss: 0.0028\n",
      "Epoch 113, batch 31/72, loss: 0.0108\n",
      "Epoch 113, batch 36/72, loss: 0.0135\n",
      "Epoch 113, batch 41/72, loss: 0.0023\n",
      "Epoch 113, batch 46/72, loss: 0.0015\n",
      "Epoch 113, batch 51/72, loss: 0.0056\n",
      "Epoch 113, batch 56/72, loss: 0.0032\n",
      "Epoch 113, batch 61/72, loss: 0.0031\n",
      "Epoch 113, batch 66/72, loss: 0.0034\n",
      "Epoch 113, batch 71/72, loss: 0.0083\n",
      "Epoch 113, train loss: 0.0051, train accuracy: 99.95%\n",
      "Epoch 113, val loss: 0.3708, val accuracy: 90.45%\n",
      "Epoch 114, batch 1/72, loss: 0.0050\n",
      "Epoch 114, batch 6/72, loss: 0.0071\n",
      "Epoch 114, batch 11/72, loss: 0.0059\n",
      "Epoch 114, batch 16/72, loss: 0.0119\n",
      "Epoch 114, batch 21/72, loss: 0.0056\n",
      "Epoch 114, batch 26/72, loss: 0.0058\n",
      "Epoch 114, batch 31/72, loss: 0.0059\n",
      "Epoch 114, batch 36/72, loss: 0.0042\n",
      "Epoch 114, batch 41/72, loss: 0.0033\n",
      "Epoch 114, batch 46/72, loss: 0.0042\n",
      "Epoch 114, batch 51/72, loss: 0.0029\n",
      "Epoch 114, batch 56/72, loss: 0.0028\n",
      "Epoch 114, batch 61/72, loss: 0.0045\n",
      "Epoch 114, batch 66/72, loss: 0.0045\n",
      "Epoch 114, batch 71/72, loss: 0.0031\n",
      "Epoch 114, train loss: 0.0070, train accuracy: 99.89%\n",
      "Epoch 114, val loss: 0.3675, val accuracy: 89.84%\n",
      "Epoch 115, batch 1/72, loss: 0.0020\n",
      "Epoch 115, batch 6/72, loss: 0.0064\n",
      "Epoch 115, batch 11/72, loss: 0.0061\n",
      "Epoch 115, batch 16/72, loss: 0.0265\n",
      "Epoch 115, batch 21/72, loss: 0.0063\n",
      "Epoch 115, batch 26/72, loss: 0.0050\n",
      "Epoch 115, batch 31/72, loss: 0.0057\n",
      "Epoch 115, batch 36/72, loss: 0.0062\n",
      "Epoch 115, batch 41/72, loss: 0.0038\n",
      "Epoch 115, batch 46/72, loss: 0.0086\n",
      "Epoch 115, batch 51/72, loss: 0.0044\n",
      "Epoch 115, batch 56/72, loss: 0.0032\n",
      "Epoch 115, batch 61/72, loss: 0.0033\n",
      "Epoch 115, batch 66/72, loss: 0.0041\n",
      "Epoch 115, batch 71/72, loss: 0.0027\n",
      "Epoch 115, train loss: 0.0064, train accuracy: 99.92%\n",
      "Epoch 115, val loss: 0.3685, val accuracy: 90.06%\n",
      "Epoch 116, batch 1/72, loss: 0.0051\n",
      "Epoch 116, batch 6/72, loss: 0.0065\n",
      "Epoch 116, batch 11/72, loss: 0.0060\n",
      "Epoch 116, batch 16/72, loss: 0.0043\n",
      "Epoch 116, batch 21/72, loss: 0.0033\n",
      "Epoch 116, batch 26/72, loss: 0.0196\n",
      "Epoch 116, batch 31/72, loss: 0.0015\n",
      "Epoch 116, batch 36/72, loss: 0.0011\n",
      "Epoch 116, batch 41/72, loss: 0.0116\n",
      "Epoch 116, batch 46/72, loss: 0.0024\n",
      "Epoch 116, batch 51/72, loss: 0.0023\n",
      "Epoch 116, batch 56/72, loss: 0.0030\n",
      "Epoch 116, batch 61/72, loss: 0.0032\n",
      "Epoch 116, batch 66/72, loss: 0.0031\n",
      "Epoch 116, batch 71/72, loss: 0.0083\n",
      "Epoch 116, train loss: 0.0064, train accuracy: 99.90%\n",
      "Epoch 116, val loss: 0.3731, val accuracy: 89.89%\n",
      "Epoch 117, batch 1/72, loss: 0.0016\n",
      "Epoch 117, batch 6/72, loss: 0.0012\n",
      "Epoch 117, batch 11/72, loss: 0.0040\n",
      "Epoch 117, batch 16/72, loss: 0.0067\n",
      "Epoch 117, batch 21/72, loss: 0.0043\n",
      "Epoch 117, batch 26/72, loss: 0.0014\n",
      "Epoch 117, batch 31/72, loss: 0.0074\n",
      "Epoch 117, batch 36/72, loss: 0.0038\n",
      "Epoch 117, batch 41/72, loss: 0.0017\n",
      "Epoch 117, batch 46/72, loss: 0.0033\n",
      "Epoch 117, batch 51/72, loss: 0.0140\n",
      "Epoch 117, batch 56/72, loss: 0.0102\n",
      "Epoch 117, batch 61/72, loss: 0.0030\n",
      "Epoch 117, batch 66/72, loss: 0.0088\n",
      "Epoch 117, batch 71/72, loss: 0.0019\n",
      "Epoch 117, train loss: 0.0057, train accuracy: 99.95%\n",
      "Epoch 117, val loss: 0.3709, val accuracy: 89.37%\n",
      "Epoch 118, batch 1/72, loss: 0.0025\n",
      "Epoch 118, batch 6/72, loss: 0.0066\n",
      "Epoch 118, batch 11/72, loss: 0.0055\n",
      "Epoch 118, batch 16/72, loss: 0.0060\n",
      "Epoch 118, batch 21/72, loss: 0.0028\n",
      "Epoch 118, batch 26/72, loss: 0.0045\n",
      "Epoch 118, batch 31/72, loss: 0.0041\n",
      "Epoch 118, batch 36/72, loss: 0.0061\n",
      "Epoch 118, batch 41/72, loss: 0.0100\n",
      "Epoch 118, batch 46/72, loss: 0.0045\n",
      "Epoch 118, batch 51/72, loss: 0.0146\n",
      "Epoch 118, batch 56/72, loss: 0.0048\n",
      "Epoch 118, batch 61/72, loss: 0.0041\n",
      "Epoch 118, batch 66/72, loss: 0.0020\n",
      "Epoch 118, batch 71/72, loss: 0.0013\n",
      "Epoch 118, train loss: 0.0056, train accuracy: 99.93%\n",
      "Epoch 118, val loss: 0.3711, val accuracy: 89.71%\n",
      "Epoch 119, batch 1/72, loss: 0.0066\n",
      "Epoch 119, batch 6/72, loss: 0.0027\n",
      "Epoch 119, batch 11/72, loss: 0.0032\n",
      "Epoch 119, batch 16/72, loss: 0.0046\n",
      "Epoch 119, batch 21/72, loss: 0.0026\n",
      "Epoch 119, batch 26/72, loss: 0.0030\n",
      "Epoch 119, batch 31/72, loss: 0.0038\n",
      "Epoch 119, batch 36/72, loss: 0.0075\n",
      "Epoch 119, batch 41/72, loss: 0.0053\n",
      "Epoch 119, batch 46/72, loss: 0.0023\n",
      "Epoch 119, batch 51/72, loss: 0.0175\n",
      "Epoch 119, batch 56/72, loss: 0.0096\n",
      "Epoch 119, batch 61/72, loss: 0.0034\n",
      "Epoch 119, batch 66/72, loss: 0.0018\n",
      "Epoch 119, batch 71/72, loss: 0.0033\n",
      "Epoch 119, train loss: 0.0049, train accuracy: 99.96%\n",
      "Epoch 119, val loss: 0.3700, val accuracy: 89.67%\n",
      "Epoch 120, batch 1/72, loss: 0.0030\n",
      "Epoch 120, batch 6/72, loss: 0.0017\n",
      "Epoch 120, batch 11/72, loss: 0.0045\n",
      "Epoch 120, batch 16/72, loss: 0.0031\n",
      "Epoch 120, batch 21/72, loss: 0.0037\n",
      "Epoch 120, batch 26/72, loss: 0.0020\n",
      "Epoch 120, batch 31/72, loss: 0.0057\n",
      "Epoch 120, batch 36/72, loss: 0.0031\n",
      "Epoch 120, batch 41/72, loss: 0.0057\n",
      "Epoch 120, batch 46/72, loss: 0.0051\n",
      "Epoch 120, batch 51/72, loss: 0.0050\n",
      "Epoch 120, batch 56/72, loss: 0.0025\n",
      "Epoch 120, batch 61/72, loss: 0.0134\n",
      "Epoch 120, batch 66/72, loss: 0.0032\n",
      "Epoch 120, batch 71/72, loss: 0.0031\n",
      "Epoch 120, train loss: 0.0049, train accuracy: 99.96%\n",
      "Epoch 120, val loss: 0.3626, val accuracy: 89.89%\n",
      "Epoch 121, batch 1/72, loss: 0.0053\n",
      "Epoch 121, batch 6/72, loss: 0.0041\n",
      "Epoch 121, batch 11/72, loss: 0.0043\n",
      "Epoch 121, batch 16/72, loss: 0.0021\n",
      "Epoch 121, batch 21/72, loss: 0.0029\n",
      "Epoch 121, batch 26/72, loss: 0.0412\n",
      "Epoch 121, batch 31/72, loss: 0.0134\n",
      "Epoch 121, batch 36/72, loss: 0.0051\n",
      "Epoch 121, batch 41/72, loss: 0.0126\n",
      "Epoch 121, batch 46/72, loss: 0.0040\n",
      "Epoch 121, batch 51/72, loss: 0.0055\n",
      "Epoch 121, batch 56/72, loss: 0.0027\n",
      "Epoch 121, batch 61/72, loss: 0.0021\n",
      "Epoch 121, batch 66/72, loss: 0.0081\n",
      "Epoch 121, batch 71/72, loss: 0.0055\n",
      "Epoch 121, train loss: 0.0068, train accuracy: 99.86%\n",
      "Epoch 121, val loss: 0.3792, val accuracy: 89.89%\n",
      "Epoch 122, batch 1/72, loss: 0.0041\n",
      "Epoch 122, batch 6/72, loss: 0.0040\n",
      "Epoch 122, batch 11/72, loss: 0.0034\n",
      "Epoch 122, batch 16/72, loss: 0.0043\n",
      "Epoch 122, batch 21/72, loss: 0.0020\n",
      "Epoch 122, batch 26/72, loss: 0.0044\n",
      "Epoch 122, batch 31/72, loss: 0.0117\n",
      "Epoch 122, batch 36/72, loss: 0.0057\n",
      "Epoch 122, batch 41/72, loss: 0.0059\n",
      "Epoch 122, batch 46/72, loss: 0.0075\n",
      "Epoch 122, batch 51/72, loss: 0.0026\n",
      "Epoch 122, batch 56/72, loss: 0.0058\n",
      "Epoch 122, batch 61/72, loss: 0.0157\n",
      "Epoch 122, batch 66/72, loss: 0.0025\n",
      "Epoch 122, batch 71/72, loss: 0.0138\n",
      "Epoch 122, train loss: 0.0074, train accuracy: 99.86%\n",
      "Epoch 122, val loss: 0.3608, val accuracy: 89.54%\n",
      "Epoch 123, batch 1/72, loss: 0.0153\n",
      "Epoch 123, batch 6/72, loss: 0.0142\n",
      "Epoch 123, batch 11/72, loss: 0.0031\n",
      "Epoch 123, batch 16/72, loss: 0.0072\n",
      "Epoch 123, batch 21/72, loss: 0.0067\n",
      "Epoch 123, batch 26/72, loss: 0.0059\n",
      "Epoch 123, batch 31/72, loss: 0.0073\n",
      "Epoch 123, batch 36/72, loss: 0.0039\n",
      "Epoch 123, batch 41/72, loss: 0.0042\n",
      "Epoch 123, batch 46/72, loss: 0.0085\n",
      "Epoch 123, batch 51/72, loss: 0.0159\n",
      "Epoch 123, batch 56/72, loss: 0.0278\n",
      "Epoch 123, batch 61/72, loss: 0.0176\n",
      "Epoch 123, batch 66/72, loss: 0.0064\n",
      "Epoch 123, batch 71/72, loss: 0.0040\n",
      "Epoch 123, train loss: 0.0093, train accuracy: 99.75%\n",
      "Epoch 123, val loss: 0.3844, val accuracy: 89.71%\n",
      "Epoch 124, batch 1/72, loss: 0.0034\n",
      "Epoch 124, batch 6/72, loss: 0.0031\n",
      "Epoch 124, batch 11/72, loss: 0.0249\n",
      "Epoch 124, batch 16/72, loss: 0.0092\n",
      "Epoch 124, batch 21/72, loss: 0.0257\n",
      "Epoch 124, batch 26/72, loss: 0.0020\n",
      "Epoch 124, batch 31/72, loss: 0.0147\n",
      "Epoch 124, batch 36/72, loss: 0.0211\n",
      "Epoch 124, batch 41/72, loss: 0.0057\n",
      "Epoch 124, batch 46/72, loss: 0.0124\n",
      "Epoch 124, batch 51/72, loss: 0.0115\n",
      "Epoch 124, batch 56/72, loss: 0.0105\n",
      "Epoch 124, batch 61/72, loss: 0.0072\n",
      "Epoch 124, batch 66/72, loss: 0.0081\n",
      "Epoch 124, batch 71/72, loss: 0.0099\n",
      "Epoch 124, train loss: 0.0103, train accuracy: 99.86%\n",
      "Epoch 124, val loss: 0.3791, val accuracy: 89.24%\n",
      "Epoch 125, batch 1/72, loss: 0.0020\n",
      "Epoch 125, batch 6/72, loss: 0.0080\n",
      "Epoch 125, batch 11/72, loss: 0.0028\n",
      "Epoch 125, batch 16/72, loss: 0.0037\n",
      "Epoch 125, batch 21/72, loss: 0.0051\n",
      "Epoch 125, batch 26/72, loss: 0.0060\n",
      "Epoch 125, batch 31/72, loss: 0.0024\n",
      "Epoch 125, batch 36/72, loss: 0.0025\n",
      "Epoch 125, batch 41/72, loss: 0.0037\n",
      "Epoch 125, batch 46/72, loss: 0.0154\n",
      "Epoch 125, batch 51/72, loss: 0.0131\n",
      "Epoch 125, batch 56/72, loss: 0.0028\n",
      "Epoch 125, batch 61/72, loss: 0.0049\n",
      "Epoch 125, batch 66/72, loss: 0.0051\n",
      "Epoch 125, batch 71/72, loss: 0.0031\n",
      "Epoch 125, train loss: 0.0056, train accuracy: 99.96%\n",
      "Epoch 125, val loss: 0.3696, val accuracy: 89.28%\n",
      "Epoch 126, batch 1/72, loss: 0.0025\n",
      "Epoch 126, batch 6/72, loss: 0.0082\n",
      "Epoch 126, batch 11/72, loss: 0.0025\n",
      "Epoch 126, batch 16/72, loss: 0.0059\n",
      "Epoch 126, batch 21/72, loss: 0.0055\n",
      "Epoch 126, batch 26/72, loss: 0.0026\n",
      "Epoch 126, batch 31/72, loss: 0.0052\n",
      "Epoch 126, batch 36/72, loss: 0.0050\n",
      "Epoch 126, batch 41/72, loss: 0.0235\n",
      "Epoch 126, batch 46/72, loss: 0.0039\n",
      "Epoch 126, batch 51/72, loss: 0.0055\n",
      "Epoch 126, batch 56/72, loss: 0.0040\n",
      "Epoch 126, batch 61/72, loss: 0.0049\n",
      "Epoch 126, batch 66/72, loss: 0.0028\n",
      "Epoch 126, batch 71/72, loss: 0.0037\n",
      "Epoch 126, train loss: 0.0053, train accuracy: 99.97%\n",
      "Epoch 126, val loss: 0.3673, val accuracy: 89.41%\n",
      "Epoch 127, batch 1/72, loss: 0.0080\n",
      "Epoch 127, batch 6/72, loss: 0.0014\n",
      "Epoch 127, batch 11/72, loss: 0.0034\n",
      "Epoch 127, batch 16/72, loss: 0.0120\n",
      "Epoch 127, batch 21/72, loss: 0.0071\n",
      "Epoch 127, batch 26/72, loss: 0.0070\n",
      "Epoch 127, batch 31/72, loss: 0.0110\n",
      "Epoch 127, batch 36/72, loss: 0.0020\n",
      "Epoch 127, batch 41/72, loss: 0.0037\n",
      "Epoch 127, batch 46/72, loss: 0.0031\n",
      "Epoch 127, batch 51/72, loss: 0.0014\n",
      "Epoch 127, batch 56/72, loss: 0.0102\n",
      "Epoch 127, batch 61/72, loss: 0.0014\n",
      "Epoch 127, batch 66/72, loss: 0.0064\n",
      "Epoch 127, batch 71/72, loss: 0.0031\n",
      "Epoch 127, train loss: 0.0045, train accuracy: 99.98%\n",
      "Epoch 127, val loss: 0.3791, val accuracy: 89.37%\n",
      "Epoch 128, batch 1/72, loss: 0.0114\n",
      "Epoch 128, batch 6/72, loss: 0.0016\n",
      "Epoch 128, batch 11/72, loss: 0.0110\n",
      "Epoch 128, batch 16/72, loss: 0.0021\n",
      "Epoch 128, batch 21/72, loss: 0.0197\n",
      "Epoch 128, batch 26/72, loss: 0.0011\n",
      "Epoch 128, batch 31/72, loss: 0.0026\n",
      "Epoch 128, batch 36/72, loss: 0.0038\n",
      "Epoch 128, batch 41/72, loss: 0.0024\n",
      "Epoch 128, batch 46/72, loss: 0.0053\n",
      "Epoch 128, batch 51/72, loss: 0.0036\n",
      "Epoch 128, batch 56/72, loss: 0.0016\n",
      "Epoch 128, batch 61/72, loss: 0.0078\n",
      "Epoch 128, batch 66/72, loss: 0.0036\n",
      "Epoch 128, batch 71/72, loss: 0.0110\n",
      "Epoch 128, train loss: 0.0046, train accuracy: 99.93%\n",
      "Epoch 128, val loss: 0.3802, val accuracy: 89.37%\n",
      "Epoch 129, batch 1/72, loss: 0.0056\n",
      "Epoch 129, batch 6/72, loss: 0.0019\n",
      "Epoch 129, batch 11/72, loss: 0.0033\n",
      "Epoch 129, batch 16/72, loss: 0.0061\n",
      "Epoch 129, batch 21/72, loss: 0.0020\n",
      "Epoch 129, batch 26/72, loss: 0.0161\n",
      "Epoch 129, batch 31/72, loss: 0.0040\n",
      "Epoch 129, batch 36/72, loss: 0.0052\n",
      "Epoch 129, batch 41/72, loss: 0.0065\n",
      "Epoch 129, batch 46/72, loss: 0.0027\n",
      "Epoch 129, batch 51/72, loss: 0.0036\n",
      "Epoch 129, batch 56/72, loss: 0.0046\n",
      "Epoch 129, batch 61/72, loss: 0.0041\n",
      "Epoch 129, batch 66/72, loss: 0.0059\n",
      "Epoch 129, batch 71/72, loss: 0.0037\n",
      "Epoch 129, train loss: 0.0068, train accuracy: 99.86%\n",
      "Epoch 129, val loss: 0.3633, val accuracy: 89.97%\n",
      "Epoch 130, batch 1/72, loss: 0.0042\n",
      "Epoch 130, batch 6/72, loss: 0.0064\n",
      "Epoch 130, batch 11/72, loss: 0.0030\n",
      "Epoch 130, batch 16/72, loss: 0.0019\n",
      "Epoch 130, batch 21/72, loss: 0.0027\n",
      "Epoch 130, batch 26/72, loss: 0.0048\n",
      "Epoch 130, batch 31/72, loss: 0.0024\n",
      "Epoch 130, batch 36/72, loss: 0.0087\n",
      "Epoch 130, batch 41/72, loss: 0.0179\n",
      "Epoch 130, batch 46/72, loss: 0.0112\n",
      "Epoch 130, batch 51/72, loss: 0.0035\n",
      "Epoch 130, batch 56/72, loss: 0.0024\n",
      "Epoch 130, batch 61/72, loss: 0.0123\n",
      "Epoch 130, batch 66/72, loss: 0.0120\n",
      "Epoch 130, batch 71/72, loss: 0.0056\n",
      "Epoch 130, train loss: 0.0055, train accuracy: 99.90%\n",
      "Epoch 130, val loss: 0.3741, val accuracy: 89.41%\n",
      "Epoch 131, batch 1/72, loss: 0.0040\n",
      "Epoch 131, batch 6/72, loss: 0.0030\n",
      "Epoch 131, batch 11/72, loss: 0.0022\n",
      "Epoch 131, batch 16/72, loss: 0.0027\n",
      "Epoch 131, batch 21/72, loss: 0.0019\n",
      "Epoch 131, batch 26/72, loss: 0.0038\n",
      "Epoch 131, batch 31/72, loss: 0.0022\n",
      "Epoch 131, batch 36/72, loss: 0.0017\n",
      "Epoch 131, batch 41/72, loss: 0.0016\n",
      "Epoch 131, batch 46/72, loss: 0.0013\n",
      "Epoch 131, batch 51/72, loss: 0.0081\n",
      "Epoch 131, batch 56/72, loss: 0.0040\n",
      "Epoch 131, batch 61/72, loss: 0.0028\n",
      "Epoch 131, batch 66/72, loss: 0.0053\n",
      "Epoch 131, batch 71/72, loss: 0.0029\n",
      "Epoch 131, train loss: 0.0032, train accuracy: 99.98%\n",
      "Epoch 131, val loss: 0.3742, val accuracy: 90.15%\n",
      "Epoch 132, batch 1/72, loss: 0.0031\n",
      "Epoch 132, batch 6/72, loss: 0.0014\n",
      "Epoch 132, batch 11/72, loss: 0.0011\n",
      "Epoch 132, batch 16/72, loss: 0.0031\n",
      "Epoch 132, batch 21/72, loss: 0.0020\n",
      "Epoch 132, batch 26/72, loss: 0.0027\n",
      "Epoch 132, batch 31/72, loss: 0.0018\n",
      "Epoch 132, batch 36/72, loss: 0.0133\n",
      "Epoch 132, batch 41/72, loss: 0.0041\n",
      "Epoch 132, batch 46/72, loss: 0.0026\n",
      "Epoch 132, batch 51/72, loss: 0.0024\n",
      "Epoch 132, batch 56/72, loss: 0.0091\n",
      "Epoch 132, batch 61/72, loss: 0.0020\n",
      "Epoch 132, batch 66/72, loss: 0.0070\n",
      "Epoch 132, batch 71/72, loss: 0.0032\n",
      "Epoch 132, train loss: 0.0039, train accuracy: 99.97%\n",
      "Epoch 132, val loss: 0.3833, val accuracy: 89.54%\n",
      "Epoch 133, batch 1/72, loss: 0.0013\n",
      "Epoch 133, batch 6/72, loss: 0.0033\n",
      "Epoch 133, batch 11/72, loss: 0.0027\n",
      "Epoch 133, batch 16/72, loss: 0.0166\n",
      "Epoch 133, batch 21/72, loss: 0.0044\n",
      "Epoch 133, batch 26/72, loss: 0.0009\n",
      "Epoch 133, batch 31/72, loss: 0.0015\n",
      "Epoch 133, batch 36/72, loss: 0.0042\n",
      "Epoch 133, batch 41/72, loss: 0.0019\n",
      "Epoch 133, batch 46/72, loss: 0.0019\n",
      "Epoch 133, batch 51/72, loss: 0.0030\n",
      "Epoch 133, batch 56/72, loss: 0.0042\n",
      "Epoch 133, batch 61/72, loss: 0.0059\n",
      "Epoch 133, batch 66/72, loss: 0.0048\n",
      "Epoch 133, batch 71/72, loss: 0.0027\n",
      "Epoch 133, train loss: 0.0032, train accuracy: 100.00%\n",
      "Epoch 133, val loss: 0.3587, val accuracy: 90.32%\n",
      "Epoch 134, batch 1/72, loss: 0.0015\n",
      "Epoch 134, batch 6/72, loss: 0.0010\n",
      "Epoch 134, batch 11/72, loss: 0.0022\n",
      "Epoch 134, batch 16/72, loss: 0.0012\n",
      "Epoch 134, batch 21/72, loss: 0.0047\n",
      "Epoch 134, batch 26/72, loss: 0.0018\n",
      "Epoch 134, batch 31/72, loss: 0.0018\n",
      "Epoch 134, batch 36/72, loss: 0.0075\n",
      "Epoch 134, batch 41/72, loss: 0.0013\n",
      "Epoch 134, batch 46/72, loss: 0.0018\n",
      "Epoch 134, batch 51/72, loss: 0.0024\n",
      "Epoch 134, batch 56/72, loss: 0.0028\n",
      "Epoch 134, batch 61/72, loss: 0.0017\n",
      "Epoch 134, batch 66/72, loss: 0.0012\n",
      "Epoch 134, batch 71/72, loss: 0.0010\n",
      "Epoch 134, train loss: 0.0032, train accuracy: 99.97%\n",
      "Epoch 134, val loss: 0.3682, val accuracy: 90.23%\n",
      "Epoch 135, batch 1/72, loss: 0.0033\n",
      "Epoch 135, batch 6/72, loss: 0.0021\n",
      "Epoch 135, batch 11/72, loss: 0.0014\n",
      "Epoch 135, batch 16/72, loss: 0.0016\n",
      "Epoch 135, batch 21/72, loss: 0.0023\n",
      "Epoch 135, batch 26/72, loss: 0.0026\n",
      "Epoch 135, batch 31/72, loss: 0.0014\n",
      "Epoch 135, batch 36/72, loss: 0.0012\n",
      "Epoch 135, batch 41/72, loss: 0.0019\n",
      "Epoch 135, batch 46/72, loss: 0.0177\n",
      "Epoch 135, batch 51/72, loss: 0.0023\n",
      "Epoch 135, batch 56/72, loss: 0.0016\n",
      "Epoch 135, batch 61/72, loss: 0.0096\n",
      "Epoch 135, batch 66/72, loss: 0.0039\n",
      "Epoch 135, batch 71/72, loss: 0.0058\n",
      "Epoch 135, train loss: 0.0028, train accuracy: 99.99%\n",
      "Epoch 135, val loss: 0.3774, val accuracy: 89.63%\n",
      "Epoch 136, batch 1/72, loss: 0.0025\n",
      "Epoch 136, batch 6/72, loss: 0.0035\n",
      "Epoch 136, batch 11/72, loss: 0.0013\n",
      "Epoch 136, batch 16/72, loss: 0.0008\n",
      "Epoch 136, batch 21/72, loss: 0.0012\n",
      "Epoch 136, batch 26/72, loss: 0.0155\n",
      "Epoch 136, batch 31/72, loss: 0.0071\n",
      "Epoch 136, batch 36/72, loss: 0.0039\n",
      "Epoch 136, batch 41/72, loss: 0.0016\n",
      "Epoch 136, batch 46/72, loss: 0.0032\n",
      "Epoch 136, batch 51/72, loss: 0.0018\n",
      "Epoch 136, batch 56/72, loss: 0.0015\n",
      "Epoch 136, batch 61/72, loss: 0.0015\n",
      "Epoch 136, batch 66/72, loss: 0.0057\n",
      "Epoch 136, batch 71/72, loss: 0.0015\n",
      "Epoch 136, train loss: 0.0033, train accuracy: 99.99%\n",
      "Epoch 136, val loss: 0.3759, val accuracy: 89.84%\n",
      "Epoch 137, batch 1/72, loss: 0.0016\n",
      "Epoch 137, batch 6/72, loss: 0.0045\n",
      "Epoch 137, batch 11/72, loss: 0.0027\n",
      "Epoch 137, batch 16/72, loss: 0.0014\n",
      "Epoch 137, batch 21/72, loss: 0.0015\n",
      "Epoch 137, batch 26/72, loss: 0.0031\n",
      "Epoch 137, batch 31/72, loss: 0.0016\n",
      "Epoch 137, batch 36/72, loss: 0.0012\n",
      "Epoch 137, batch 41/72, loss: 0.0032\n",
      "Epoch 137, batch 46/72, loss: 0.0029\n",
      "Epoch 137, batch 51/72, loss: 0.0046\n",
      "Epoch 137, batch 56/72, loss: 0.0035\n",
      "Epoch 137, batch 61/72, loss: 0.0026\n",
      "Epoch 137, batch 66/72, loss: 0.0061\n",
      "Epoch 137, batch 71/72, loss: 0.0055\n",
      "Epoch 137, train loss: 0.0046, train accuracy: 99.92%\n",
      "Epoch 137, val loss: 0.3673, val accuracy: 89.89%\n",
      "Epoch 138, batch 1/72, loss: 0.0016\n",
      "Epoch 138, batch 6/72, loss: 0.0012\n",
      "Epoch 138, batch 11/72, loss: 0.0035\n",
      "Epoch 138, batch 16/72, loss: 0.0027\n",
      "Epoch 138, batch 21/72, loss: 0.0032\n",
      "Epoch 138, batch 26/72, loss: 0.0015\n",
      "Epoch 138, batch 31/72, loss: 0.0029\n",
      "Epoch 138, batch 36/72, loss: 0.0016\n",
      "Epoch 138, batch 41/72, loss: 0.0042\n",
      "Epoch 138, batch 46/72, loss: 0.0043\n",
      "Epoch 138, batch 51/72, loss: 0.0017\n",
      "Epoch 138, batch 56/72, loss: 0.0015\n",
      "Epoch 138, batch 61/72, loss: 0.0025\n",
      "Epoch 138, batch 66/72, loss: 0.0055\n",
      "Epoch 138, batch 71/72, loss: 0.0043\n",
      "Epoch 138, train loss: 0.0046, train accuracy: 99.91%\n",
      "Epoch 138, val loss: 0.3659, val accuracy: 89.67%\n",
      "Epoch 139, batch 1/72, loss: 0.0013\n",
      "Epoch 139, batch 6/72, loss: 0.0023\n",
      "Epoch 139, batch 11/72, loss: 0.0023\n",
      "Epoch 139, batch 16/72, loss: 0.0020\n",
      "Epoch 139, batch 21/72, loss: 0.0028\n",
      "Epoch 139, batch 26/72, loss: 0.0022\n",
      "Epoch 139, batch 31/72, loss: 0.0214\n",
      "Epoch 139, batch 36/72, loss: 0.0106\n",
      "Epoch 139, batch 41/72, loss: 0.0309\n",
      "Epoch 139, batch 46/72, loss: 0.0039\n",
      "Epoch 139, batch 51/72, loss: 0.0142\n",
      "Epoch 139, batch 56/72, loss: 0.0060\n",
      "Epoch 139, batch 61/72, loss: 0.0234\n",
      "Epoch 139, batch 66/72, loss: 0.0023\n",
      "Epoch 139, batch 71/72, loss: 0.0038\n",
      "Epoch 139, train loss: 0.0097, train accuracy: 99.75%\n",
      "Epoch 139, val loss: 0.3956, val accuracy: 88.67%\n",
      "Epoch 140, batch 1/72, loss: 0.0071\n",
      "Epoch 140, batch 6/72, loss: 0.0104\n",
      "Epoch 140, batch 11/72, loss: 0.0704\n",
      "Epoch 140, batch 16/72, loss: 0.0100\n",
      "Epoch 140, batch 21/72, loss: 0.0406\n",
      "Epoch 140, batch 26/72, loss: 0.0375\n",
      "Epoch 140, batch 31/72, loss: 0.0263\n",
      "Epoch 140, batch 36/72, loss: 0.0291\n",
      "Epoch 140, batch 41/72, loss: 0.0736\n",
      "Epoch 140, batch 46/72, loss: 0.0138\n",
      "Epoch 140, batch 51/72, loss: 0.0135\n",
      "Epoch 140, batch 56/72, loss: 0.0284\n",
      "Epoch 140, batch 61/72, loss: 0.0281\n",
      "Epoch 140, batch 66/72, loss: 0.0268\n",
      "Epoch 140, batch 71/72, loss: 0.0082\n",
      "Epoch 140, train loss: 0.0263, train accuracy: 99.18%\n",
      "Epoch 140, val loss: 0.4297, val accuracy: 88.41%\n",
      "Epoch 141, batch 1/72, loss: 0.0059\n",
      "Epoch 141, batch 6/72, loss: 0.0101\n",
      "Epoch 141, batch 11/72, loss: 0.0102\n",
      "Epoch 141, batch 16/72, loss: 0.0057\n",
      "Epoch 141, batch 21/72, loss: 0.0022\n",
      "Epoch 141, batch 26/72, loss: 0.0099\n",
      "Epoch 141, batch 31/72, loss: 0.0073\n",
      "Epoch 141, batch 36/72, loss: 0.0055\n",
      "Epoch 141, batch 41/72, loss: 0.0109\n",
      "Epoch 141, batch 46/72, loss: 0.0065\n",
      "Epoch 141, batch 51/72, loss: 0.0089\n",
      "Epoch 141, batch 56/72, loss: 0.0192\n",
      "Epoch 141, batch 61/72, loss: 0.0029\n",
      "Epoch 141, batch 66/72, loss: 0.0156\n",
      "Epoch 141, batch 71/72, loss: 0.0024\n",
      "Epoch 141, train loss: 0.0080, train accuracy: 99.90%\n",
      "Epoch 141, val loss: 0.4126, val accuracy: 88.85%\n",
      "Epoch 142, batch 1/72, loss: 0.0153\n",
      "Epoch 142, batch 6/72, loss: 0.0021\n",
      "Epoch 142, batch 11/72, loss: 0.0049\n",
      "Epoch 142, batch 16/72, loss: 0.0025\n",
      "Epoch 142, batch 21/72, loss: 0.0060\n",
      "Epoch 142, batch 26/72, loss: 0.0015\n",
      "Epoch 142, batch 31/72, loss: 0.0080\n",
      "Epoch 142, batch 36/72, loss: 0.0039\n",
      "Epoch 142, batch 41/72, loss: 0.0053\n",
      "Epoch 142, batch 46/72, loss: 0.0036\n",
      "Epoch 142, batch 51/72, loss: 0.0048\n",
      "Epoch 142, batch 56/72, loss: 0.0155\n",
      "Epoch 142, batch 61/72, loss: 0.0035\n",
      "Epoch 142, batch 66/72, loss: 0.0185\n",
      "Epoch 142, batch 71/72, loss: 0.0114\n",
      "Epoch 142, train loss: 0.0066, train accuracy: 99.91%\n",
      "Epoch 142, val loss: 0.4095, val accuracy: 88.76%\n",
      "Epoch 143, batch 1/72, loss: 0.0067\n",
      "Epoch 143, batch 6/72, loss: 0.0047\n",
      "Epoch 143, batch 11/72, loss: 0.0017\n",
      "Epoch 143, batch 16/72, loss: 0.0134\n",
      "Epoch 143, batch 21/72, loss: 0.0104\n",
      "Epoch 143, batch 26/72, loss: 0.0059\n",
      "Epoch 143, batch 31/72, loss: 0.0129\n",
      "Epoch 143, batch 36/72, loss: 0.0051\n",
      "Epoch 143, batch 41/72, loss: 0.0093\n",
      "Epoch 143, batch 46/72, loss: 0.0175\n",
      "Epoch 143, batch 51/72, loss: 0.0118\n",
      "Epoch 143, batch 56/72, loss: 0.0082\n",
      "Epoch 143, batch 61/72, loss: 0.0114\n",
      "Epoch 143, batch 66/72, loss: 0.0058\n",
      "Epoch 143, batch 71/72, loss: 0.0016\n",
      "Epoch 143, train loss: 0.0083, train accuracy: 99.85%\n",
      "Epoch 143, val loss: 0.3926, val accuracy: 88.85%\n",
      "Epoch 144, batch 1/72, loss: 0.0020\n",
      "Epoch 144, batch 6/72, loss: 0.0053\n",
      "Epoch 144, batch 11/72, loss: 0.0038\n",
      "Epoch 144, batch 16/72, loss: 0.0030\n",
      "Epoch 144, batch 21/72, loss: 0.0028\n",
      "Epoch 144, batch 26/72, loss: 0.0044\n",
      "Epoch 144, batch 31/72, loss: 0.0064\n",
      "Epoch 144, batch 36/72, loss: 0.0013\n",
      "Epoch 144, batch 41/72, loss: 0.0026\n",
      "Epoch 144, batch 46/72, loss: 0.0079\n",
      "Epoch 144, batch 51/72, loss: 0.0020\n",
      "Epoch 144, batch 56/72, loss: 0.0034\n",
      "Epoch 144, batch 61/72, loss: 0.0034\n",
      "Epoch 144, batch 66/72, loss: 0.0139\n",
      "Epoch 144, batch 71/72, loss: 0.0024\n",
      "Epoch 144, train loss: 0.0036, train accuracy: 100.00%\n",
      "Epoch 144, val loss: 0.3789, val accuracy: 90.06%\n",
      "Epoch 145, batch 1/72, loss: 0.0018\n",
      "Epoch 145, batch 6/72, loss: 0.0070\n",
      "Epoch 145, batch 11/72, loss: 0.0014\n",
      "Epoch 145, batch 16/72, loss: 0.0078\n",
      "Epoch 145, batch 21/72, loss: 0.0013\n",
      "Epoch 145, batch 26/72, loss: 0.0022\n",
      "Epoch 145, batch 31/72, loss: 0.0015\n",
      "Epoch 145, batch 36/72, loss: 0.0035\n",
      "Epoch 145, batch 41/72, loss: 0.0027\n",
      "Epoch 145, batch 46/72, loss: 0.0073\n",
      "Epoch 145, batch 51/72, loss: 0.0066\n",
      "Epoch 145, batch 56/72, loss: 0.0038\n",
      "Epoch 145, batch 61/72, loss: 0.0067\n",
      "Epoch 145, batch 66/72, loss: 0.0026\n",
      "Epoch 145, batch 71/72, loss: 0.0038\n",
      "Epoch 145, train loss: 0.0039, train accuracy: 99.97%\n",
      "Epoch 145, val loss: 0.4069, val accuracy: 89.50%\n",
      "Epoch 146, batch 1/72, loss: 0.0025\n",
      "Epoch 146, batch 6/72, loss: 0.0029\n",
      "Epoch 146, batch 11/72, loss: 0.0024\n",
      "Epoch 146, batch 16/72, loss: 0.0027\n",
      "Epoch 146, batch 21/72, loss: 0.0026\n",
      "Epoch 146, batch 26/72, loss: 0.0050\n",
      "Epoch 146, batch 31/72, loss: 0.0023\n",
      "Epoch 146, batch 36/72, loss: 0.0016\n",
      "Epoch 146, batch 41/72, loss: 0.0019\n",
      "Epoch 146, batch 46/72, loss: 0.0026\n",
      "Epoch 146, batch 51/72, loss: 0.0112\n",
      "Epoch 146, batch 56/72, loss: 0.0032\n",
      "Epoch 146, batch 61/72, loss: 0.0022\n",
      "Epoch 146, batch 66/72, loss: 0.0014\n",
      "Epoch 146, batch 71/72, loss: 0.0010\n",
      "Epoch 146, train loss: 0.0032, train accuracy: 99.96%\n",
      "Epoch 146, val loss: 0.3775, val accuracy: 89.67%\n",
      "Epoch 147, batch 1/72, loss: 0.0009\n",
      "Epoch 147, batch 6/72, loss: 0.0024\n",
      "Epoch 147, batch 11/72, loss: 0.0016\n",
      "Epoch 147, batch 16/72, loss: 0.0027\n",
      "Epoch 147, batch 21/72, loss: 0.0022\n",
      "Epoch 147, batch 26/72, loss: 0.0025\n",
      "Epoch 147, batch 31/72, loss: 0.0015\n",
      "Epoch 147, batch 36/72, loss: 0.0028\n",
      "Epoch 147, batch 41/72, loss: 0.0028\n",
      "Epoch 147, batch 46/72, loss: 0.0057\n",
      "Epoch 147, batch 51/72, loss: 0.0015\n",
      "Epoch 147, batch 56/72, loss: 0.0015\n",
      "Epoch 147, batch 61/72, loss: 0.0057\n",
      "Epoch 147, batch 66/72, loss: 0.0020\n",
      "Epoch 147, batch 71/72, loss: 0.0025\n",
      "Epoch 147, train loss: 0.0032, train accuracy: 99.97%\n",
      "Epoch 147, val loss: 0.3917, val accuracy: 89.58%\n",
      "Epoch 148, batch 1/72, loss: 0.0012\n",
      "Epoch 148, batch 6/72, loss: 0.0012\n",
      "Epoch 148, batch 11/72, loss: 0.0023\n",
      "Epoch 148, batch 16/72, loss: 0.0012\n",
      "Epoch 148, batch 21/72, loss: 0.0005\n",
      "Epoch 148, batch 26/72, loss: 0.0041\n",
      "Epoch 148, batch 31/72, loss: 0.0015\n",
      "Epoch 148, batch 36/72, loss: 0.0032\n",
      "Epoch 148, batch 41/72, loss: 0.0045\n",
      "Epoch 148, batch 46/72, loss: 0.0028\n",
      "Epoch 148, batch 51/72, loss: 0.0037\n",
      "Epoch 148, batch 56/72, loss: 0.0027\n",
      "Epoch 148, batch 61/72, loss: 0.0036\n",
      "Epoch 148, batch 66/72, loss: 0.0072\n",
      "Epoch 148, batch 71/72, loss: 0.0042\n",
      "Epoch 148, train loss: 0.0032, train accuracy: 99.96%\n",
      "Epoch 148, val loss: 0.3677, val accuracy: 89.89%\n",
      "Epoch 149, batch 1/72, loss: 0.0024\n",
      "Epoch 149, batch 6/72, loss: 0.0007\n",
      "Epoch 149, batch 11/72, loss: 0.0161\n",
      "Epoch 149, batch 16/72, loss: 0.0061\n",
      "Epoch 149, batch 21/72, loss: 0.0042\n",
      "Epoch 149, batch 26/72, loss: 0.0040\n",
      "Epoch 149, batch 31/72, loss: 0.0017\n",
      "Epoch 149, batch 36/72, loss: 0.0022\n",
      "Epoch 149, batch 41/72, loss: 0.0155\n",
      "Epoch 149, batch 46/72, loss: 0.0033\n",
      "Epoch 149, batch 51/72, loss: 0.0076\n",
      "Epoch 149, batch 56/72, loss: 0.0076\n",
      "Epoch 149, batch 61/72, loss: 0.0030\n",
      "Epoch 149, batch 66/72, loss: 0.0033\n",
      "Epoch 149, batch 71/72, loss: 0.0029\n",
      "Epoch 149, train loss: 0.0039, train accuracy: 99.98%\n",
      "Epoch 149, val loss: 0.3895, val accuracy: 89.93%\n",
      "Epoch 150, batch 1/72, loss: 0.0029\n",
      "Epoch 150, batch 6/72, loss: 0.0032\n",
      "Epoch 150, batch 11/72, loss: 0.0020\n",
      "Epoch 150, batch 16/72, loss: 0.0033\n",
      "Epoch 150, batch 21/72, loss: 0.0017\n",
      "Epoch 150, batch 26/72, loss: 0.0032\n",
      "Epoch 150, batch 31/72, loss: 0.0022\n",
      "Epoch 150, batch 36/72, loss: 0.0015\n",
      "Epoch 150, batch 41/72, loss: 0.0052\n",
      "Epoch 150, batch 46/72, loss: 0.0024\n",
      "Epoch 150, batch 51/72, loss: 0.0027\n",
      "Epoch 150, batch 56/72, loss: 0.0018\n",
      "Epoch 150, batch 61/72, loss: 0.0035\n",
      "Epoch 150, batch 66/72, loss: 0.0031\n",
      "Epoch 150, batch 71/72, loss: 0.0038\n",
      "Epoch 150, train loss: 0.0030, train accuracy: 99.97%\n",
      "Epoch 150, val loss: 0.4025, val accuracy: 89.54%\n"
     ]
    }
   ],
   "source": [
    "train_losses, train_accs, val_losses, val_accs = train(model, train_loader, val_loader, criterion, optimizer, device, num_epochs=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "61b66e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"best_model_v5.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e8a4296c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_loss_accuracy2(train_losses, val_losses, train_accs, val_accs):\n",
    "    # Create figure and axes\n",
    "    fig, ax1 = plt.subplots(figsize=(8, 4))\n",
    "    ax2 = ax1.twinx()\n",
    "\n",
    "    # Plot loss curves\n",
    "    ax1.plot(train_losses, label=\"Training Loss\", color=\"blue\")\n",
    "    ax1.plot(val_losses, label=\"Validation Loss\", color=\"orange\")\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    # Plot accuracy curves\n",
    "    ax2.plot(train_accs, label=\"Training Accuracy\", color=\"green\")\n",
    "    ax2.plot(val_accs, label=\"Validation Accuracy\", color=\"red\")\n",
    "    \n",
    "    \n",
    "    ax1.grid()\n",
    "    ax2.grid()\n",
    "\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper left\")\n",
    "    ax1.set_xlabel(\"Epoch\")\n",
    "    ax2.set_ylabel(\"Accuracy (%)\")\n",
    "    ax2.legend(loc=\"upper right\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b983a7d0-4506-4a87-90b5-3713e6c088b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss_accuracy2(train_losses, val_losses, train_accs, val_accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55958b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
