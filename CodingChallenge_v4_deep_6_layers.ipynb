{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2617147a-5efd-4572-9ac0-6b48d1269fa3",
   "metadata": {},
   "source": [
    "## Data loading\n",
    "\n",
    "To work with any kind of data we first have to load it. For this we use a dataloader that reads the images as well as their labels and transforms them into pytorch readable tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d80171d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.io import read_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0090711b-668a-4bab-a4d7-686081d239cb",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "class RSiMCCDataset(Dataset):\n",
    "    def __init__(self, transform=None):\n",
    "        super().__init__()\n",
    "        self.transform = transform\n",
    "        # get images\n",
    "        image_files = [x.resolve() for x in pathlib.Path(\".\").glob('data/*/*')]\n",
    "        # Image.open() has a bug, this is a workaround\n",
    "        self.images=[read_image(str(p)) for p in image_files] \n",
    "        # get labels from image path\n",
    "        labels = [x.parts[-2] for x in image_files]\n",
    "        self.classes = sorted(list(set(labels)))\n",
    "        self.labels = [self.label_to_tensor(lbl) for lbl in labels]\n",
    "\n",
    "        assert len(self.labels) == len(self.images), f\"Found {len(self.labels)} labels and {len(self.images)} images\"\n",
    "\n",
    "    def label_to_tensor(self, lbl):\n",
    "        \"\"\"\n",
    "        Converts the string label to a one-hot tensor where every entry is zero except the label which is one.\n",
    "        \n",
    "        \"\"\"\n",
    "        assert lbl in self.classes, f\"Class {lbl} not a valid class (valid classes: {self.classes})\"\n",
    "        t = torch.zeros(len(self.classes))\n",
    "        t[self.classes.index(lbl)] = 1\n",
    "        return t\n",
    "\n",
    "    def tensor_to_label(self, t):\n",
    "        \"\"\"\n",
    "        Returns the classname in string format\n",
    "        \"\"\"\n",
    "        assert len(t.shape) == 1, f\"Can only convert 1-dimensional tensors (shape of tensor: {t.shape})\"\n",
    "        assert len(t) == len(self.classes), f\"Lenght of tensor ({len(t)}) does not match number of classes ({len(classes)})\"\n",
    "        return self.classes[t.argmax()]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.images[idx].float()/255\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, self.labels[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685aa618-6d38-4ff5-bac6-2e16f8c7a25e",
   "metadata": {},
   "source": [
    "## Examples\n",
    "Now lets load the dataset and look at some examples by just randomly loading the images and their labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "673f108f-b1c0-4ab8-a331-3a6d038d2cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = RSiMCCDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e8257cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset consists of 11519 RGB images of shape torch.Size([3, 64, 64]) labeled in 10 classes which are AnnualCrop, Forest, HerbaceousVegetation, Highway, Industrial, Pasture, PermanentCrop, Residential, River, SeaLake\n"
     ]
    }
   ],
   "source": [
    "print(f\"The dataset consists of {len(dataset)} RGB images of shape {dataset.images[0].shape} labeled in {dataset.labels[0].shape[0]} classes which are \" + ', '.join(dataset.classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88aefed8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.labels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a81752",
   "metadata": {},
   "source": [
    "# Create my custom convolution model for the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "609bc17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepCNN(nn.Module):\n",
    "    def __init__(self, nb_classes):\n",
    "        super(DeepCNN, self).__init__()\n",
    "        # inputs (N,3,64,64)\n",
    "\n",
    "        # Classical processing of images with pattern recognition\n",
    "        # Follows ResNET architecture: Conv2d -> BN -> ReLU (-> MaxPool)\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=8)\n",
    "        self.bn1   = nn.BatchNorm2d(32)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        # no pooling in 1st step\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=5, padding=2)\n",
    "        self.bn2   = nn.BatchNorm2d(32)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=16, kernel_size=5)\n",
    "        self.bn3   = nn.BatchNorm2d(16)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        # no pooling in 3rd step\n",
    "\n",
    "        self.conv4 = nn.Conv2d(in_channels=16, out_channels=16, kernel_size=5, padding=2)\n",
    "        self.bn4   = nn.BatchNorm2d(16)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv5 = nn.Conv2d(in_channels=16, out_channels=8, kernel_size=5)\n",
    "        self.bn5   = nn.BatchNorm2d(8)\n",
    "        self.relu5 = nn.ReLU()\n",
    "        # no pooling in 5th step\n",
    "\n",
    "        self.conv6 = nn.Conv2d(in_channels=8, out_channels=8, kernel_size=5, padding=2)\n",
    "        self.bn6   = nn.BatchNorm2d(8)\n",
    "        self.relu6 = nn.ReLU()\n",
    "        self.pool6 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "\n",
    "        self.drop7 = nn.Dropout(p=0.05)\n",
    "        self.lin7  = nn.Linear(8*4*4, nb_classes) # adapt here!\n",
    "        \n",
    "    def forward(self, x):        \n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "\n",
    "        x = x + self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu3(x)\n",
    "\n",
    "        x = x + self.conv4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = self.relu4(x)\n",
    "        x = self.pool4(x)\n",
    "\n",
    "        x = self.conv5(x)\n",
    "        x = self.bn5(x)\n",
    "        x = self.relu5(x)\n",
    "\n",
    "        x = x + self.conv6(x)\n",
    "        x = self.bn6(x)\n",
    "        x = self.relu6(x)\n",
    "        x = self.pool6(x)\n",
    "\n",
    "        # outputs (None, 8, 4, 4)\n",
    "        x = self.drop7(x)\n",
    "        x = self.lin7(x.view(-1, 8*4*4))  # adapt here!\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b01bebee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(nb_classes):\n",
    "    return DeepCNN(nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1dea5346",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([42, 10])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_model(len(dataset.classes))\n",
    "dummy_img = torch.rand(42, 3, 64, 64)\n",
    "model(dummy_img).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5ebb7962",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.43.0 (0)\n",
       " -->\n",
       "<!-- Title: DeepCNN Pages: 1 -->\n",
       "<svg width=\"300pt\" height=\"2743pt\"\n",
       " viewBox=\"0.00 0.00 300.14 2743.07\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 2739.07)\">\n",
       "<title>DeepCNN</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-2739.07 296.14,-2739.07 296.14,4 -4,4\"/>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-2720.87\" font-family=\"Times,serif\" font-weight=\"bold\" font-size=\"14.00\">DeepCNN</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-2706.87\" font-family=\"Times,serif\" font-size=\"14.00\">41 tensors total (133.9 MB)</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-2692.87\" font-family=\"Times,serif\" font-size=\"14.00\">57370 params total (227.2 KB)</text>\n",
       "<!-- input_1 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>input_1</title>\n",
       "<ellipse fill=\"#98fb98\" stroke=\"black\" cx=\"158.5\" cy=\"-35.36\" rx=\"120.83\" ry=\"35.21\"/>\n",
       "<text text-anchor=\"start\" x=\"128.5\" y=\"-46.16\" font-family=\"Times,serif\" font-weight=\"bold\" font-size=\"14.00\">input_1</text>\n",
       "<text text-anchor=\"start\" x=\"81\" y=\"-32.16\" font-family=\"Times,serif\" font-size=\"14.00\">42x3x64x64 (2.0 MB)</text>\n",
       "<text text-anchor=\"start\" x=\"126\" y=\"-18.16\" font-family=\"Times,serif\" font-size=\"14.00\">@input.x</text>\n",
       "</g>\n",
       "<!-- conv2d_1_1 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>conv2d_1_1</title>\n",
       "<polygon fill=\"#e6e6e6\" stroke=\"black\" points=\"253,-170.71 64,-170.71 64,-106.71 253,-106.71 253,-170.71\"/>\n",
       "<text text-anchor=\"start\" x=\"114\" y=\"-156.51\" font-family=\"Times,serif\" font-weight=\"bold\" font-size=\"14.00\">conv2d_1_1</text>\n",
       "<text text-anchor=\"start\" x=\"72\" y=\"-142.51\" font-family=\"Times,serif\" font-size=\"14.00\">42x32x57x57 (16.7 MB)</text>\n",
       "<text text-anchor=\"start\" x=\"74.5\" y=\"-128.51\" font-family=\"Times,serif\" font-size=\"14.00\">params: 32x3x8x8, x32</text>\n",
       "<text text-anchor=\"start\" x=\"130\" y=\"-114.51\" font-family=\"Times,serif\" font-size=\"14.00\">@conv1</text>\n",
       "</g>\n",
       "<!-- input_1&#45;&gt;conv2d_1_1 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>input_1&#45;&gt;conv2d_1_1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M158.5,-70.95C158.5,-80.18 158.5,-90.21 158.5,-99.62\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"156.05,-99.67 158.5,-106.67 160.95,-99.67 156.05,-99.67\"/>\n",
       "</g>\n",
       "<!-- batchnorm_1_2 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>batchnorm_1_2</title>\n",
       "<polygon fill=\"#e6e6e6\" stroke=\"black\" points=\"253,-270.71 64,-270.71 64,-206.71 253,-206.71 253,-270.71\"/>\n",
       "<text text-anchor=\"start\" x=\"98.5\" y=\"-256.51\" font-family=\"Times,serif\" font-weight=\"bold\" font-size=\"14.00\">batchnorm_1_2</text>\n",
       "<text text-anchor=\"start\" x=\"72\" y=\"-242.51\" font-family=\"Times,serif\" font-size=\"14.00\">42x32x57x57 (16.7 MB)</text>\n",
       "<text text-anchor=\"start\" x=\"96.5\" y=\"-228.51\" font-family=\"Times,serif\" font-size=\"14.00\">params: x32, x32</text>\n",
       "<text text-anchor=\"start\" x=\"137.5\" y=\"-214.51\" font-family=\"Times,serif\" font-size=\"14.00\">@bn1</text>\n",
       "</g>\n",
       "<!-- conv2d_1_1&#45;&gt;batchnorm_1_2 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>conv2d_1_1&#45;&gt;batchnorm_1_2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M158.5,-170.72C158.5,-179.83 158.5,-189.91 158.5,-199.43\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"156.05,-199.57 158.5,-206.57 160.95,-199.57 156.05,-199.57\"/>\n",
       "</g>\n",
       "<!-- relu_1_3 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>relu_1_3</title>\n",
       "<polygon fill=\"white\" stroke=\"black\" points=\"253,-356.71 64,-356.71 64,-306.71 253,-306.71 253,-356.71\"/>\n",
       "<text text-anchor=\"start\" x=\"125.5\" y=\"-342.51\" font-family=\"Times,serif\" font-weight=\"bold\" font-size=\"14.00\">relu_1_3</text>\n",
       "<text text-anchor=\"start\" x=\"72\" y=\"-328.51\" font-family=\"Times,serif\" font-size=\"14.00\">42x32x57x57 (16.7 MB)</text>\n",
       "<text text-anchor=\"start\" x=\"132.5\" y=\"-314.51\" font-family=\"Times,serif\" font-size=\"14.00\">@relu1</text>\n",
       "</g>\n",
       "<!-- batchnorm_1_2&#45;&gt;relu_1_3 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>batchnorm_1_2&#45;&gt;relu_1_3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M158.5,-270.77C158.5,-280.04 158.5,-290.19 158.5,-299.46\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"156.05,-299.68 158.5,-306.68 160.95,-299.68 156.05,-299.68\"/>\n",
       "</g>\n",
       "<!-- conv2d_2_4 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>conv2d_2_4</title>\n",
       "<polygon fill=\"#e6e6e6\" stroke=\"black\" points=\"193,-456.71 0,-456.71 0,-392.71 193,-392.71 193,-456.71\"/>\n",
       "<text text-anchor=\"start\" x=\"52\" y=\"-442.51\" font-family=\"Times,serif\" font-weight=\"bold\" font-size=\"14.00\">conv2d_2_4</text>\n",
       "<text text-anchor=\"start\" x=\"10\" y=\"-428.51\" font-family=\"Times,serif\" font-size=\"14.00\">42x32x57x57 (16.7 MB)</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-414.51\" font-family=\"Times,serif\" font-size=\"14.00\">params: 32x32x5x5, x32</text>\n",
       "<text text-anchor=\"start\" x=\"68\" y=\"-400.51\" font-family=\"Times,serif\" font-size=\"14.00\">@conv2</text>\n",
       "</g>\n",
       "<!-- relu_1_3&#45;&gt;conv2d_2_4 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>relu_1_3&#45;&gt;conv2d_2_4</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M141.89,-357.1C135.77,-366.08 128.69,-376.47 121.96,-386.33\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"119.75,-385.23 117.84,-392.39 123.8,-387.99 119.75,-385.23\"/>\n",
       "</g>\n",
       "<!-- add_1_5 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>add_1_5</title>\n",
       "<ellipse fill=\"white\" stroke=\"black\" cx=\"158.5\" cy=\"-518.17\" rx=\"133.79\" ry=\"25.41\"/>\n",
       "<text text-anchor=\"start\" x=\"127.5\" y=\"-521.97\" font-family=\"Times,serif\" font-weight=\"bold\" font-size=\"14.00\">add_1_5</text>\n",
       "<text text-anchor=\"start\" x=\"72\" y=\"-507.97\" font-family=\"Times,serif\" font-size=\"14.00\">42x32x57x57 (16.7 MB)</text>\n",
       "</g>\n",
       "<!-- relu_1_3&#45;&gt;add_1_5 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>relu_1_3&#45;&gt;add_1_5</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M181.73,-356.97C189.96,-367.17 198.24,-379.7 202.5,-392.71 211.35,-419.74 211.3,-429.66 202.5,-456.71 198.97,-467.55 192.69,-478.06 185.96,-487.19\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"183.83,-485.95 181.5,-492.99 187.71,-488.94 183.83,-485.95\"/>\n",
       "</g>\n",
       "<!-- conv2d_2_4&#45;&gt;add_1_5 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>conv2d_2_4&#45;&gt;add_1_5</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M117.66,-456.93C124.23,-466.61 131.45,-477.27 137.97,-486.89\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"135.97,-488.31 141.93,-492.72 140.03,-485.56 135.97,-488.31\"/>\n",
       "</g>\n",
       "<!-- batchnorm_2_6 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>batchnorm_2_6</title>\n",
       "<polygon fill=\"#e6e6e6\" stroke=\"black\" points=\"253,-643.62 64,-643.62 64,-579.62 253,-579.62 253,-643.62\"/>\n",
       "<text text-anchor=\"start\" x=\"98.5\" y=\"-629.42\" font-family=\"Times,serif\" font-weight=\"bold\" font-size=\"14.00\">batchnorm_2_6</text>\n",
       "<text text-anchor=\"start\" x=\"72\" y=\"-615.42\" font-family=\"Times,serif\" font-size=\"14.00\">42x32x57x57 (16.7 MB)</text>\n",
       "<text text-anchor=\"start\" x=\"96.5\" y=\"-601.42\" font-family=\"Times,serif\" font-size=\"14.00\">params: x32, x32</text>\n",
       "<text text-anchor=\"start\" x=\"137.5\" y=\"-587.42\" font-family=\"Times,serif\" font-size=\"14.00\">@bn2</text>\n",
       "</g>\n",
       "<!-- add_1_5&#45;&gt;batchnorm_2_6 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>add_1_5&#45;&gt;batchnorm_2_6</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M158.5,-543.68C158.5,-552.44 158.5,-562.55 158.5,-572.21\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"156.05,-572.48 158.5,-579.48 160.95,-572.48 156.05,-572.48\"/>\n",
       "</g>\n",
       "<!-- relu_2_7 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>relu_2_7</title>\n",
       "<polygon fill=\"white\" stroke=\"black\" points=\"253,-729.62 64,-729.62 64,-679.62 253,-679.62 253,-729.62\"/>\n",
       "<text text-anchor=\"start\" x=\"125.5\" y=\"-715.42\" font-family=\"Times,serif\" font-weight=\"bold\" font-size=\"14.00\">relu_2_7</text>\n",
       "<text text-anchor=\"start\" x=\"72\" y=\"-701.42\" font-family=\"Times,serif\" font-size=\"14.00\">42x32x57x57 (16.7 MB)</text>\n",
       "<text text-anchor=\"start\" x=\"132.5\" y=\"-687.42\" font-family=\"Times,serif\" font-size=\"14.00\">@relu2</text>\n",
       "</g>\n",
       "<!-- batchnorm_2_6&#45;&gt;relu_2_7 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>batchnorm_2_6&#45;&gt;relu_2_7</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M158.5,-643.68C158.5,-652.95 158.5,-663.1 158.5,-672.38\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"156.05,-672.59 158.5,-679.59 160.95,-672.59 156.05,-672.59\"/>\n",
       "</g>\n",
       "<!-- maxpool2d_1_8 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>maxpool2d_1_8</title>\n",
       "<polygon fill=\"white\" stroke=\"black\" points=\"248.5,-815.62 68.5,-815.62 68.5,-765.62 248.5,-765.62 248.5,-815.62\"/>\n",
       "<text text-anchor=\"start\" x=\"99.5\" y=\"-801.42\" font-family=\"Times,serif\" font-weight=\"bold\" font-size=\"14.00\">maxpool2d_1_8</text>\n",
       "<text text-anchor=\"start\" x=\"76.5\" y=\"-787.42\" font-family=\"Times,serif\" font-size=\"14.00\">42x32x28x28 (4.0 MB)</text>\n",
       "<text text-anchor=\"start\" x=\"131.5\" y=\"-773.42\" font-family=\"Times,serif\" font-size=\"14.00\">@pool2</text>\n",
       "</g>\n",
       "<!-- relu_2_7&#45;&gt;maxpool2d_1_8 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>relu_2_7&#45;&gt;maxpool2d_1_8</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M158.5,-729.92C158.5,-738.8 158.5,-748.97 158.5,-758.38\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"156.05,-758.38 158.5,-765.38 160.95,-758.38 156.05,-758.38\"/>\n",
       "</g>\n",
       "<!-- conv2d_3_9 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>conv2d_3_9</title>\n",
       "<polygon fill=\"#e6e6e6\" stroke=\"black\" points=\"255,-915.62 62,-915.62 62,-851.62 255,-851.62 255,-915.62\"/>\n",
       "<text text-anchor=\"start\" x=\"114\" y=\"-901.42\" font-family=\"Times,serif\" font-weight=\"bold\" font-size=\"14.00\">conv2d_3_9</text>\n",
       "<text text-anchor=\"start\" x=\"76.5\" y=\"-887.42\" font-family=\"Times,serif\" font-size=\"14.00\">42x16x24x24 (1.5 MB)</text>\n",
       "<text text-anchor=\"start\" x=\"70\" y=\"-873.42\" font-family=\"Times,serif\" font-size=\"14.00\">params: 16x32x5x5, x16</text>\n",
       "<text text-anchor=\"start\" x=\"130\" y=\"-859.42\" font-family=\"Times,serif\" font-size=\"14.00\">@conv3</text>\n",
       "</g>\n",
       "<!-- maxpool2d_1_8&#45;&gt;conv2d_3_9 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>maxpool2d_1_8&#45;&gt;conv2d_3_9</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M158.5,-816.01C158.5,-824.65 158.5,-834.6 158.5,-844.13\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"156.05,-844.3 158.5,-851.3 160.95,-844.3 156.05,-844.3\"/>\n",
       "</g>\n",
       "<!-- batchnorm_3_10 -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>batchnorm_3_10</title>\n",
       "<polygon fill=\"#e6e6e6\" stroke=\"black\" points=\"248.5,-1015.62 68.5,-1015.62 68.5,-951.62 248.5,-951.62 248.5,-1015.62\"/>\n",
       "<text text-anchor=\"start\" x=\"93.5\" y=\"-1001.42\" font-family=\"Times,serif\" font-weight=\"bold\" font-size=\"14.00\">batchnorm_3_10</text>\n",
       "<text text-anchor=\"start\" x=\"76.5\" y=\"-987.42\" font-family=\"Times,serif\" font-size=\"14.00\">42x16x24x24 (1.5 MB)</text>\n",
       "<text text-anchor=\"start\" x=\"96.5\" y=\"-973.42\" font-family=\"Times,serif\" font-size=\"14.00\">params: x16, x16</text>\n",
       "<text text-anchor=\"start\" x=\"137.5\" y=\"-959.42\" font-family=\"Times,serif\" font-size=\"14.00\">@bn3</text>\n",
       "</g>\n",
       "<!-- conv2d_3_9&#45;&gt;batchnorm_3_10 -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>conv2d_3_9&#45;&gt;batchnorm_3_10</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M158.5,-915.63C158.5,-924.75 158.5,-934.83 158.5,-944.34\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"156.05,-944.48 158.5,-951.48 160.95,-944.48 156.05,-944.48\"/>\n",
       "</g>\n",
       "<!-- relu_3_11 -->\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>relu_3_11</title>\n",
       "<polygon fill=\"white\" stroke=\"black\" points=\"248.5,-1101.62 68.5,-1101.62 68.5,-1051.62 248.5,-1051.62 248.5,-1101.62\"/>\n",
       "<text text-anchor=\"start\" x=\"120.5\" y=\"-1087.42\" font-family=\"Times,serif\" font-weight=\"bold\" font-size=\"14.00\">relu_3_11</text>\n",
       "<text text-anchor=\"start\" x=\"76.5\" y=\"-1073.42\" font-family=\"Times,serif\" font-size=\"14.00\">42x16x24x24 (1.5 MB)</text>\n",
       "<text text-anchor=\"start\" x=\"132.5\" y=\"-1059.42\" font-family=\"Times,serif\" font-size=\"14.00\">@relu3</text>\n",
       "</g>\n",
       "<!-- batchnorm_3_10&#45;&gt;relu_3_11 -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>batchnorm_3_10&#45;&gt;relu_3_11</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M158.5,-1015.68C158.5,-1024.95 158.5,-1035.1 158.5,-1044.38\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"156.05,-1044.59 158.5,-1051.59 160.95,-1044.59 156.05,-1044.59\"/>\n",
       "</g>\n",
       "<!-- conv2d_4_12 -->\n",
       "<g id=\"node13\" class=\"node\">\n",
       "<title>conv2d_4_12</title>\n",
       "<polygon fill=\"#e6e6e6\" stroke=\"black\" points=\"193,-1201.62 0,-1201.62 0,-1137.62 193,-1137.62 193,-1201.62\"/>\n",
       "<text text-anchor=\"start\" x=\"47\" y=\"-1187.42\" font-family=\"Times,serif\" font-weight=\"bold\" font-size=\"14.00\">conv2d_4_12</text>\n",
       "<text text-anchor=\"start\" x=\"14.5\" y=\"-1173.42\" font-family=\"Times,serif\" font-size=\"14.00\">42x16x24x24 (1.5 MB)</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-1159.42\" font-family=\"Times,serif\" font-size=\"14.00\">params: 16x16x5x5, x16</text>\n",
       "<text text-anchor=\"start\" x=\"68\" y=\"-1145.42\" font-family=\"Times,serif\" font-size=\"14.00\">@conv4</text>\n",
       "</g>\n",
       "<!-- relu_3_11&#45;&gt;conv2d_4_12 -->\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>relu_3_11&#45;&gt;conv2d_4_12</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M141.89,-1102.01C135.77,-1110.99 128.69,-1121.38 121.96,-1131.25\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"119.75,-1130.14 117.84,-1137.3 123.8,-1132.9 119.75,-1130.14\"/>\n",
       "</g>\n",
       "<!-- add_2_13 -->\n",
       "<g id=\"node14\" class=\"node\">\n",
       "<title>add_2_13</title>\n",
       "<ellipse fill=\"white\" stroke=\"black\" cx=\"158.5\" cy=\"-1263.08\" rx=\"127.06\" ry=\"25.41\"/>\n",
       "<text text-anchor=\"start\" x=\"122.5\" y=\"-1266.88\" font-family=\"Times,serif\" font-weight=\"bold\" font-size=\"14.00\">add_2_13</text>\n",
       "<text text-anchor=\"start\" x=\"76.5\" y=\"-1252.88\" font-family=\"Times,serif\" font-size=\"14.00\">42x16x24x24 (1.5 MB)</text>\n",
       "</g>\n",
       "<!-- relu_3_11&#45;&gt;add_2_13 -->\n",
       "<g id=\"edge14\" class=\"edge\">\n",
       "<title>relu_3_11&#45;&gt;add_2_13</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M181.73,-1101.88C189.96,-1112.09 198.24,-1124.61 202.5,-1137.62 211.35,-1164.66 211.3,-1174.57 202.5,-1201.62 198.97,-1212.46 192.69,-1222.97 185.96,-1232.11\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"183.83,-1230.86 181.5,-1237.9 187.71,-1233.85 183.83,-1230.86\"/>\n",
       "</g>\n",
       "<!-- conv2d_4_12&#45;&gt;add_2_13 -->\n",
       "<g id=\"edge15\" class=\"edge\">\n",
       "<title>conv2d_4_12&#45;&gt;add_2_13</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M117.66,-1201.84C124.23,-1211.53 131.45,-1222.18 137.97,-1231.8\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"135.97,-1233.22 141.93,-1237.64 140.03,-1230.47 135.97,-1233.22\"/>\n",
       "</g>\n",
       "<!-- batchnorm_4_14 -->\n",
       "<g id=\"node15\" class=\"node\">\n",
       "<title>batchnorm_4_14</title>\n",
       "<polygon fill=\"#e6e6e6\" stroke=\"black\" points=\"248.5,-1388.53 68.5,-1388.53 68.5,-1324.53 248.5,-1324.53 248.5,-1388.53\"/>\n",
       "<text text-anchor=\"start\" x=\"93.5\" y=\"-1374.33\" font-family=\"Times,serif\" font-weight=\"bold\" font-size=\"14.00\">batchnorm_4_14</text>\n",
       "<text text-anchor=\"start\" x=\"76.5\" y=\"-1360.33\" font-family=\"Times,serif\" font-size=\"14.00\">42x16x24x24 (1.5 MB)</text>\n",
       "<text text-anchor=\"start\" x=\"96.5\" y=\"-1346.33\" font-family=\"Times,serif\" font-size=\"14.00\">params: x16, x16</text>\n",
       "<text text-anchor=\"start\" x=\"137.5\" y=\"-1332.33\" font-family=\"Times,serif\" font-size=\"14.00\">@bn4</text>\n",
       "</g>\n",
       "<!-- add_2_13&#45;&gt;batchnorm_4_14 -->\n",
       "<g id=\"edge16\" class=\"edge\">\n",
       "<title>add_2_13&#45;&gt;batchnorm_4_14</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M158.5,-1288.59C158.5,-1297.36 158.5,-1307.46 158.5,-1317.13\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"156.05,-1317.4 158.5,-1324.4 160.95,-1317.4 156.05,-1317.4\"/>\n",
       "</g>\n",
       "<!-- relu_4_15 -->\n",
       "<g id=\"node16\" class=\"node\">\n",
       "<title>relu_4_15</title>\n",
       "<polygon fill=\"white\" stroke=\"black\" points=\"248.5,-1474.53 68.5,-1474.53 68.5,-1424.53 248.5,-1424.53 248.5,-1474.53\"/>\n",
       "<text text-anchor=\"start\" x=\"120.5\" y=\"-1460.33\" font-family=\"Times,serif\" font-weight=\"bold\" font-size=\"14.00\">relu_4_15</text>\n",
       "<text text-anchor=\"start\" x=\"76.5\" y=\"-1446.33\" font-family=\"Times,serif\" font-size=\"14.00\">42x16x24x24 (1.5 MB)</text>\n",
       "<text text-anchor=\"start\" x=\"132.5\" y=\"-1432.33\" font-family=\"Times,serif\" font-size=\"14.00\">@relu4</text>\n",
       "</g>\n",
       "<!-- batchnorm_4_14&#45;&gt;relu_4_15 -->\n",
       "<g id=\"edge17\" class=\"edge\">\n",
       "<title>batchnorm_4_14&#45;&gt;relu_4_15</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M158.5,-1388.6C158.5,-1397.86 158.5,-1408.01 158.5,-1417.29\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"156.05,-1417.5 158.5,-1424.5 160.95,-1417.5 156.05,-1417.5\"/>\n",
       "</g>\n",
       "<!-- maxpool2d_2_16 -->\n",
       "<g id=\"node17\" class=\"node\">\n",
       "<title>maxpool2d_2_16</title>\n",
       "<polygon fill=\"white\" stroke=\"black\" points=\"255.5,-1560.53 61.5,-1560.53 61.5,-1510.53 255.5,-1510.53 255.5,-1560.53\"/>\n",
       "<text text-anchor=\"start\" x=\"94.5\" y=\"-1546.33\" font-family=\"Times,serif\" font-weight=\"bold\" font-size=\"14.00\">maxpool2d_2_16</text>\n",
       "<text text-anchor=\"start\" x=\"69.5\" y=\"-1532.33\" font-family=\"Times,serif\" font-size=\"14.00\">42x16x12x12 (378.2 KB)</text>\n",
       "<text text-anchor=\"start\" x=\"131.5\" y=\"-1518.33\" font-family=\"Times,serif\" font-size=\"14.00\">@pool4</text>\n",
       "</g>\n",
       "<!-- relu_4_15&#45;&gt;maxpool2d_2_16 -->\n",
       "<g id=\"edge18\" class=\"edge\">\n",
       "<title>relu_4_15&#45;&gt;maxpool2d_2_16</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M158.5,-1474.83C158.5,-1483.71 158.5,-1493.88 158.5,-1503.29\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"156.05,-1503.29 158.5,-1510.29 160.95,-1503.29 156.05,-1503.29\"/>\n",
       "</g>\n",
       "<!-- conv2d_5_17 -->\n",
       "<g id=\"node18\" class=\"node\">\n",
       "<title>conv2d_5_17</title>\n",
       "<polygon fill=\"#e6e6e6\" stroke=\"black\" points=\"246,-1660.53 71,-1660.53 71,-1596.53 246,-1596.53 246,-1660.53\"/>\n",
       "<text text-anchor=\"start\" x=\"109\" y=\"-1646.33\" font-family=\"Times,serif\" font-weight=\"bold\" font-size=\"14.00\">conv2d_5_17</text>\n",
       "<text text-anchor=\"start\" x=\"87.5\" y=\"-1632.33\" font-family=\"Times,serif\" font-size=\"14.00\">42x8x8x8 (84.2 KB)</text>\n",
       "<text text-anchor=\"start\" x=\"79\" y=\"-1618.33\" font-family=\"Times,serif\" font-size=\"14.00\">params: 8x16x5x5, x8</text>\n",
       "<text text-anchor=\"start\" x=\"130\" y=\"-1604.33\" font-family=\"Times,serif\" font-size=\"14.00\">@conv5</text>\n",
       "</g>\n",
       "<!-- maxpool2d_2_16&#45;&gt;conv2d_5_17 -->\n",
       "<g id=\"edge19\" class=\"edge\">\n",
       "<title>maxpool2d_2_16&#45;&gt;conv2d_5_17</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M158.5,-1560.92C158.5,-1569.56 158.5,-1579.51 158.5,-1589.04\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"156.05,-1589.21 158.5,-1596.22 160.95,-1589.22 156.05,-1589.21\"/>\n",
       "</g>\n",
       "<!-- batchnorm_5_18 -->\n",
       "<g id=\"node19\" class=\"node\">\n",
       "<title>batchnorm_5_18</title>\n",
       "<polygon fill=\"#e6e6e6\" stroke=\"black\" points=\"237.5,-1760.53 79.5,-1760.53 79.5,-1696.53 237.5,-1696.53 237.5,-1760.53\"/>\n",
       "<text text-anchor=\"start\" x=\"93.5\" y=\"-1746.33\" font-family=\"Times,serif\" font-weight=\"bold\" font-size=\"14.00\">batchnorm_5_18</text>\n",
       "<text text-anchor=\"start\" x=\"87.5\" y=\"-1732.33\" font-family=\"Times,serif\" font-size=\"14.00\">42x8x8x8 (84.2 KB)</text>\n",
       "<text text-anchor=\"start\" x=\"105.5\" y=\"-1718.33\" font-family=\"Times,serif\" font-size=\"14.00\">params: x8, x8</text>\n",
       "<text text-anchor=\"start\" x=\"137.5\" y=\"-1704.33\" font-family=\"Times,serif\" font-size=\"14.00\">@bn5</text>\n",
       "</g>\n",
       "<!-- conv2d_5_17&#45;&gt;batchnorm_5_18 -->\n",
       "<g id=\"edge20\" class=\"edge\">\n",
       "<title>conv2d_5_17&#45;&gt;batchnorm_5_18</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M158.5,-1660.54C158.5,-1669.66 158.5,-1679.74 158.5,-1689.25\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"156.05,-1689.39 158.5,-1696.39 160.95,-1689.39 156.05,-1689.39\"/>\n",
       "</g>\n",
       "<!-- relu_5_19 -->\n",
       "<g id=\"node20\" class=\"node\">\n",
       "<title>relu_5_19</title>\n",
       "<polygon fill=\"white\" stroke=\"black\" points=\"237.5,-1846.53 79.5,-1846.53 79.5,-1796.53 237.5,-1796.53 237.5,-1846.53\"/>\n",
       "<text text-anchor=\"start\" x=\"120.5\" y=\"-1832.33\" font-family=\"Times,serif\" font-weight=\"bold\" font-size=\"14.00\">relu_5_19</text>\n",
       "<text text-anchor=\"start\" x=\"87.5\" y=\"-1818.33\" font-family=\"Times,serif\" font-size=\"14.00\">42x8x8x8 (84.2 KB)</text>\n",
       "<text text-anchor=\"start\" x=\"132.5\" y=\"-1804.33\" font-family=\"Times,serif\" font-size=\"14.00\">@relu5</text>\n",
       "</g>\n",
       "<!-- batchnorm_5_18&#45;&gt;relu_5_19 -->\n",
       "<g id=\"edge21\" class=\"edge\">\n",
       "<title>batchnorm_5_18&#45;&gt;relu_5_19</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M158.5,-1760.6C158.5,-1769.86 158.5,-1780.01 158.5,-1789.29\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"156.05,-1789.5 158.5,-1796.5 160.95,-1789.5 156.05,-1789.5\"/>\n",
       "</g>\n",
       "<!-- conv2d_6_20 -->\n",
       "<g id=\"node21\" class=\"node\">\n",
       "<title>conv2d_6_20</title>\n",
       "<polygon fill=\"#e6e6e6\" stroke=\"black\" points=\"186.5,-1946.53 20.5,-1946.53 20.5,-1882.53 186.5,-1882.53 186.5,-1946.53\"/>\n",
       "<text text-anchor=\"start\" x=\"54\" y=\"-1932.33\" font-family=\"Times,serif\" font-weight=\"bold\" font-size=\"14.00\">conv2d_6_20</text>\n",
       "<text text-anchor=\"start\" x=\"32.5\" y=\"-1918.33\" font-family=\"Times,serif\" font-size=\"14.00\">42x8x8x8 (84.2 KB)</text>\n",
       "<text text-anchor=\"start\" x=\"28.5\" y=\"-1904.33\" font-family=\"Times,serif\" font-size=\"14.00\">params: 8x8x5x5, x8</text>\n",
       "<text text-anchor=\"start\" x=\"75\" y=\"-1890.33\" font-family=\"Times,serif\" font-size=\"14.00\">@conv6</text>\n",
       "</g>\n",
       "<!-- relu_5_19&#45;&gt;conv2d_6_20 -->\n",
       "<g id=\"edge22\" class=\"edge\">\n",
       "<title>relu_5_19&#45;&gt;conv2d_6_20</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M143.76,-1846.92C138.33,-1855.9 132.05,-1866.29 126.09,-1876.16\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"123.95,-1874.96 122.43,-1882.22 128.15,-1877.49 123.95,-1874.96\"/>\n",
       "</g>\n",
       "<!-- add_3_21 -->\n",
       "<g id=\"node22\" class=\"node\">\n",
       "<title>add_3_21</title>\n",
       "<ellipse fill=\"white\" stroke=\"black\" cx=\"158.5\" cy=\"-2007.99\" rx=\"111.95\" ry=\"25.41\"/>\n",
       "<text text-anchor=\"start\" x=\"122.5\" y=\"-2011.79\" font-family=\"Times,serif\" font-weight=\"bold\" font-size=\"14.00\">add_3_21</text>\n",
       "<text text-anchor=\"start\" x=\"87.5\" y=\"-1997.79\" font-family=\"Times,serif\" font-size=\"14.00\">42x8x8x8 (84.2 KB)</text>\n",
       "</g>\n",
       "<!-- relu_5_19&#45;&gt;add_3_21 -->\n",
       "<g id=\"edge23\" class=\"edge\">\n",
       "<title>relu_5_19&#45;&gt;add_3_21</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M177.69,-1846.76C184.75,-1857.15 191.89,-1869.82 195.5,-1882.53 203.26,-1909.9 203.21,-1919.16 195.5,-1946.53 192.52,-1957.12 187.1,-1967.69 181.32,-1976.96\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"179.25,-1975.65 177.49,-1982.85 183.36,-1978.32 179.25,-1975.65\"/>\n",
       "</g>\n",
       "<!-- conv2d_6_20&#45;&gt;add_3_21 -->\n",
       "<g id=\"edge24\" class=\"edge\">\n",
       "<title>conv2d_6_20&#45;&gt;add_3_21</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M122.27,-1946.75C128.04,-1956.35 134.38,-1966.89 140.13,-1976.44\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"138.09,-1977.81 143.8,-1982.55 142.29,-1975.29 138.09,-1977.81\"/>\n",
       "</g>\n",
       "<!-- batchnorm_6_22 -->\n",
       "<g id=\"node23\" class=\"node\">\n",
       "<title>batchnorm_6_22</title>\n",
       "<polygon fill=\"#e6e6e6\" stroke=\"black\" points=\"237.5,-2133.45 79.5,-2133.45 79.5,-2069.45 237.5,-2069.45 237.5,-2133.45\"/>\n",
       "<text text-anchor=\"start\" x=\"93.5\" y=\"-2119.25\" font-family=\"Times,serif\" font-weight=\"bold\" font-size=\"14.00\">batchnorm_6_22</text>\n",
       "<text text-anchor=\"start\" x=\"87.5\" y=\"-2105.25\" font-family=\"Times,serif\" font-size=\"14.00\">42x8x8x8 (84.2 KB)</text>\n",
       "<text text-anchor=\"start\" x=\"105.5\" y=\"-2091.25\" font-family=\"Times,serif\" font-size=\"14.00\">params: x8, x8</text>\n",
       "<text text-anchor=\"start\" x=\"137.5\" y=\"-2077.25\" font-family=\"Times,serif\" font-size=\"14.00\">@bn6</text>\n",
       "</g>\n",
       "<!-- add_3_21&#45;&gt;batchnorm_6_22 -->\n",
       "<g id=\"edge25\" class=\"edge\">\n",
       "<title>add_3_21&#45;&gt;batchnorm_6_22</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M158.5,-2033.5C158.5,-2042.27 158.5,-2052.38 158.5,-2062.04\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"156.05,-2062.31 158.5,-2069.31 160.95,-2062.31 156.05,-2062.31\"/>\n",
       "</g>\n",
       "<!-- relu_6_23 -->\n",
       "<g id=\"node24\" class=\"node\">\n",
       "<title>relu_6_23</title>\n",
       "<polygon fill=\"white\" stroke=\"black\" points=\"237.5,-2219.45 79.5,-2219.45 79.5,-2169.45 237.5,-2169.45 237.5,-2219.45\"/>\n",
       "<text text-anchor=\"start\" x=\"120.5\" y=\"-2205.25\" font-family=\"Times,serif\" font-weight=\"bold\" font-size=\"14.00\">relu_6_23</text>\n",
       "<text text-anchor=\"start\" x=\"87.5\" y=\"-2191.25\" font-family=\"Times,serif\" font-size=\"14.00\">42x8x8x8 (84.2 KB)</text>\n",
       "<text text-anchor=\"start\" x=\"132.5\" y=\"-2177.25\" font-family=\"Times,serif\" font-size=\"14.00\">@relu6</text>\n",
       "</g>\n",
       "<!-- batchnorm_6_22&#45;&gt;relu_6_23 -->\n",
       "<g id=\"edge26\" class=\"edge\">\n",
       "<title>batchnorm_6_22&#45;&gt;relu_6_23</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M158.5,-2133.51C158.5,-2142.77 158.5,-2152.93 158.5,-2162.2\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"156.05,-2162.42 158.5,-2169.42 160.95,-2162.42 156.05,-2162.42\"/>\n",
       "</g>\n",
       "<!-- maxpool2d_3_24 -->\n",
       "<g id=\"node25\" class=\"node\">\n",
       "<title>maxpool2d_3_24</title>\n",
       "<polygon fill=\"white\" stroke=\"black\" points=\"237.5,-2305.45 79.5,-2305.45 79.5,-2255.45 237.5,-2255.45 237.5,-2305.45\"/>\n",
       "<text text-anchor=\"start\" x=\"94.5\" y=\"-2291.25\" font-family=\"Times,serif\" font-weight=\"bold\" font-size=\"14.00\">maxpool2d_3_24</text>\n",
       "<text text-anchor=\"start\" x=\"87.5\" y=\"-2277.25\" font-family=\"Times,serif\" font-size=\"14.00\">42x8x4x4 (21.2 KB)</text>\n",
       "<text text-anchor=\"start\" x=\"131.5\" y=\"-2263.25\" font-family=\"Times,serif\" font-size=\"14.00\">@pool6</text>\n",
       "</g>\n",
       "<!-- relu_6_23&#45;&gt;maxpool2d_3_24 -->\n",
       "<g id=\"edge27\" class=\"edge\">\n",
       "<title>relu_6_23&#45;&gt;maxpool2d_3_24</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M158.5,-2219.74C158.5,-2228.62 158.5,-2238.8 158.5,-2248.2\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"156.05,-2248.2 158.5,-2255.2 160.95,-2248.2 156.05,-2248.2\"/>\n",
       "</g>\n",
       "<!-- dropout_1_25 -->\n",
       "<g id=\"node26\" class=\"node\">\n",
       "<title>dropout_1_25</title>\n",
       "<polygon fill=\"white\" stroke=\"black\" points=\"237.5,-2391.45 79.5,-2391.45 79.5,-2341.45 237.5,-2341.45 237.5,-2391.45\"/>\n",
       "<text text-anchor=\"start\" x=\"105.5\" y=\"-2377.25\" font-family=\"Times,serif\" font-weight=\"bold\" font-size=\"14.00\">dropout_1_25</text>\n",
       "<text text-anchor=\"start\" x=\"87.5\" y=\"-2363.25\" font-family=\"Times,serif\" font-size=\"14.00\">42x8x4x4 (21.2 KB)</text>\n",
       "<text text-anchor=\"start\" x=\"130\" y=\"-2349.25\" font-family=\"Times,serif\" font-size=\"14.00\">@drop7</text>\n",
       "</g>\n",
       "<!-- maxpool2d_3_24&#45;&gt;dropout_1_25 -->\n",
       "<g id=\"edge28\" class=\"edge\">\n",
       "<title>maxpool2d_3_24&#45;&gt;dropout_1_25</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M158.5,-2305.74C158.5,-2314.62 158.5,-2324.8 158.5,-2334.2\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"156.05,-2334.2 158.5,-2341.2 160.95,-2334.2 156.05,-2334.2\"/>\n",
       "</g>\n",
       "<!-- view_1_26 -->\n",
       "<g id=\"node27\" class=\"node\">\n",
       "<title>view_1_26</title>\n",
       "<ellipse fill=\"white\" stroke=\"black\" cx=\"158.5\" cy=\"-2452.9\" rx=\"100.32\" ry=\"25.41\"/>\n",
       "<text text-anchor=\"start\" x=\"119.5\" y=\"-2456.7\" font-family=\"Times,serif\" font-weight=\"bold\" font-size=\"14.00\">view_1_26</text>\n",
       "<text text-anchor=\"start\" x=\"95.5\" y=\"-2442.7\" font-family=\"Times,serif\" font-size=\"14.00\">42x128 (21.1 KB)</text>\n",
       "</g>\n",
       "<!-- dropout_1_25&#45;&gt;view_1_26 -->\n",
       "<g id=\"edge29\" class=\"edge\">\n",
       "<title>dropout_1_25&#45;&gt;view_1_26</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M158.5,-2391.87C158.5,-2400.64 158.5,-2410.65 158.5,-2419.96\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"156.05,-2420.24 158.5,-2427.24 160.95,-2420.24 156.05,-2420.24\"/>\n",
       "</g>\n",
       "<!-- linear_1_27 -->\n",
       "<g id=\"node28\" class=\"node\">\n",
       "<title>linear_1_27</title>\n",
       "<polygon fill=\"#e6e6e6\" stroke=\"black\" points=\"242,-2578.36 75,-2578.36 75,-2514.36 242,-2514.36 242,-2578.36\"/>\n",
       "<text text-anchor=\"start\" x=\"113.5\" y=\"-2564.16\" font-family=\"Times,serif\" font-weight=\"bold\" font-size=\"14.00\">linear_1_27</text>\n",
       "<text text-anchor=\"start\" x=\"104.5\" y=\"-2550.16\" font-family=\"Times,serif\" font-size=\"14.00\">42x10 (1.8 KB)</text>\n",
       "<text text-anchor=\"start\" x=\"83\" y=\"-2536.16\" font-family=\"Times,serif\" font-size=\"14.00\">params: 10x128, x10</text>\n",
       "<text text-anchor=\"start\" x=\"137.5\" y=\"-2522.16\" font-family=\"Times,serif\" font-size=\"14.00\">@lin7</text>\n",
       "</g>\n",
       "<!-- view_1_26&#45;&gt;linear_1_27 -->\n",
       "<g id=\"edge30\" class=\"edge\">\n",
       "<title>view_1_26&#45;&gt;linear_1_27</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M158.5,-2478.41C158.5,-2487.18 158.5,-2497.29 158.5,-2506.95\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"156.05,-2507.22 158.5,-2514.22 160.95,-2507.22 156.05,-2507.22\"/>\n",
       "</g>\n",
       "<!-- output_1 -->\n",
       "<g id=\"node29\" class=\"node\">\n",
       "<title>output_1</title>\n",
       "<ellipse fill=\"#ff9999\" stroke=\"black\" cx=\"158.5\" cy=\"-2649.71\" rx=\"87.86\" ry=\"35.21\"/>\n",
       "<text text-anchor=\"start\" x=\"123.5\" y=\"-2660.51\" font-family=\"Times,serif\" font-weight=\"bold\" font-size=\"14.00\">output_1</text>\n",
       "<text text-anchor=\"start\" x=\"104.5\" y=\"-2646.51\" font-family=\"Times,serif\" font-size=\"14.00\">42x10 (1.8 KB)</text>\n",
       "<text text-anchor=\"start\" x=\"127.5\" y=\"-2632.51\" font-family=\"Times,serif\" font-size=\"14.00\">@output</text>\n",
       "</g>\n",
       "<!-- linear_1_27&#45;&gt;output_1 -->\n",
       "<g id=\"edge31\" class=\"edge\">\n",
       "<title>linear_1_27&#45;&gt;output_1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M158.5,-2578.59C158.5,-2587.59 158.5,-2597.54 158.5,-2607.06\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"156.05,-2607.22 158.5,-2614.22 160.95,-2607.23 156.05,-2607.22\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7f1b1648b790>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log of DeepCNN forward pass:\n",
      "\tRandom seed: 2881197070\n",
      "\tTime elapsed: 6.63s (6.55s spent logging)\n",
      "\tStructure:\n",
      "\t\t- purely feedforward, no recurrence\n",
      "\t\t- with branching\n",
      "\t\t- no conditional (if-then) branching\n",
      "\t\t- contains 12 buffer layers\n",
      "\t\t- 23 total modules\n",
      "\tTensor info:\n",
      "\t\t- 41 total tensors (133.9 MB) computed in forward pass.\n",
      "\t\t- 41 tensors (133.9 MB) with saved activations.\n",
      "\tParameters: 13 parameter operations (57370 params total; 227.2 KB)\n",
      "\tModule Hierarchy:\n",
      "\t\tconv1\n",
      "\t\tbn1\n",
      "\t\trelu1\n",
      "\t\tconv2\n",
      "\t\tbn2\n",
      "\t\trelu2\n",
      "\t\tpool2\n",
      "\t\tconv3\n",
      "\t\tbn3\n",
      "\t\trelu3\n",
      "\t\tconv4\n",
      "\t\tbn4\n",
      "\t\trelu4\n",
      "\t\tpool4\n",
      "\t\tconv5\n",
      "\t\tbn5\n",
      "\t\trelu5\n",
      "\t\tconv6\n",
      "\t\tbn6\n",
      "\t\trelu6\n",
      "\t\tpool6\n",
      "\t\tdrop7\n",
      "\t\tlin7\n",
      "\tLayers (all have saved activations):\n",
      "\t\t  (0) input_1 \n",
      "\t\t  (1) conv2d_1_1 \n",
      "\t\t  (2) buffer_1 \n",
      "\t\t  (3) buffer_2 \n",
      "\t\t  (4) batchnorm_1_2 \n",
      "\t\t  (5) relu_1_3 \n",
      "\t\t  (6) conv2d_2_4 \n",
      "\t\t  (7) add_1_5 \n",
      "\t\t  (8) buffer_3 \n",
      "\t\t  (9) buffer_4 \n",
      "\t\t  (10) batchnorm_2_6 \n",
      "\t\t  (11) relu_2_7 \n",
      "\t\t  (12) maxpool2d_1_8 \n",
      "\t\t  (13) conv2d_3_9 \n",
      "\t\t  (14) buffer_5 \n",
      "\t\t  (15) buffer_6 \n",
      "\t\t  (16) batchnorm_3_10 \n",
      "\t\t  (17) relu_3_11 \n",
      "\t\t  (18) conv2d_4_12 \n",
      "\t\t  (19) add_2_13 \n",
      "\t\t  (20) buffer_7 \n",
      "\t\t  (21) buffer_8 \n",
      "\t\t  (22) batchnorm_4_14 \n",
      "\t\t  (23) relu_4_15 \n",
      "\t\t  (24) maxpool2d_2_16 \n",
      "\t\t  (25) conv2d_5_17 \n",
      "\t\t  (26) buffer_9 \n",
      "\t\t  (27) buffer_10 \n",
      "\t\t  (28) batchnorm_5_18 \n",
      "\t\t  (29) relu_5_19 \n",
      "\t\t  (30) conv2d_6_20 \n",
      "\t\t  (31) add_3_21 \n",
      "\t\t  (32) buffer_11 \n",
      "\t\t  (33) buffer_12 \n",
      "\t\t  (34) batchnorm_6_22 \n",
      "\t\t  (35) relu_6_23 \n",
      "\t\t  (36) maxpool2d_3_24 \n",
      "\t\t  (37) dropout_1_25 \n",
      "\t\t  (38) view_1_26 \n",
      "\t\t  (39) linear_1_27 \n",
      "\t\t  (40) output_1 \n"
     ]
    }
   ],
   "source": [
    "import torchlens as tl\n",
    "model_history = tl.log_forward_pass(model, dummy_img, layers_to_save='all', vis_opt='rolled')\n",
    "print(model_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d82760",
   "metadata": {},
   "source": [
    "### Initialize the model and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b095176",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' from sklearn.model_selection import train_test_split\\n\\n# Split dataset into training and validation sets\\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\\n\\n# Create data loaders for training and validation sets\\ntrain_dataset = MyDataset(X_train, y_train)\\nval_dataset = MyDataset(X_val, y_val)\\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False) '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split dataset into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create data loaders for training and validation sets\n",
    "train_dataset = MyDataset(X_train, y_train)\n",
    "val_dataset = MyDataset(X_val, y_val)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "37dce276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([50, 3, 64, 64])\n",
      "Labels batch shape: torch.Size([50, 10])\n",
      "torch.Size([20, 10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "DeepCNN                                  --\n",
       "├─Conv2d: 1-1                            6,176\n",
       "├─BatchNorm2d: 1-2                       64\n",
       "├─ReLU: 1-3                              --\n",
       "├─Conv2d: 1-4                            25,632\n",
       "├─BatchNorm2d: 1-5                       64\n",
       "├─ReLU: 1-6                              --\n",
       "├─MaxPool2d: 1-7                         --\n",
       "├─Conv2d: 1-8                            12,816\n",
       "├─BatchNorm2d: 1-9                       32\n",
       "├─ReLU: 1-10                             --\n",
       "├─Conv2d: 1-11                           6,416\n",
       "├─BatchNorm2d: 1-12                      32\n",
       "├─ReLU: 1-13                             --\n",
       "├─MaxPool2d: 1-14                        --\n",
       "├─Conv2d: 1-15                           3,208\n",
       "├─BatchNorm2d: 1-16                      16\n",
       "├─ReLU: 1-17                             --\n",
       "├─Conv2d: 1-18                           1,608\n",
       "├─BatchNorm2d: 1-19                      16\n",
       "├─ReLU: 1-20                             --\n",
       "├─MaxPool2d: 1-21                        --\n",
       "├─Dropout: 1-22                          --\n",
       "├─Linear: 1-23                           1,290\n",
       "=================================================================\n",
       "Total params: 57,370\n",
       "Trainable params: 57,370\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create training and validation datasets\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "batch_size = 100\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Create data loaders for training and validation sets\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "data_loader = DataLoader(dataset, batch_size=50, shuffle=True)\n",
    "\n",
    "train_features, train_labels = next(iter(data_loader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "\n",
    "\n",
    "num_classes = len(dataset.classes)\n",
    "model = get_model(num_classes) # define the model\n",
    "# model.load_state_dict(torch.load(\"best_model.pt\"))\n",
    "\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss() # CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "\n",
    "image = torch.randn(20, 3, 64, 64) # create dummy image\n",
    "output = model(image)\n",
    "\n",
    "print(model(image).shape) # show the output of one batch of images\n",
    "\n",
    "from torchinfo import summary\n",
    "summary(model) # display summary of the model and number of trainable parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520830e6",
   "metadata": {},
   "source": [
    "### Send the model to the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "76203502",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeepCNN(\n",
       "  (conv1): Conv2d(3, 32, kernel_size=(8, 8), stride=(1, 1))\n",
       "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu1): ReLU()\n",
       "  (conv2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu2): ReLU()\n",
       "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv3): Conv2d(32, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (bn3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu3): ReLU()\n",
       "  (conv4): Conv2d(16, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (bn4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu4): ReLU()\n",
       "  (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv5): Conv2d(16, 8, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (bn5): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu5): ReLU()\n",
       "  (conv6): Conv2d(8, 8, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (bn6): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu6): ReLU()\n",
       "  (pool6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (drop7): Dropout(p=0.05, inplace=False)\n",
       "  (lin7): Linear(in_features=128, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a8b5dc",
   "metadata": {},
   "source": [
    "### Define the training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bbb08e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, loss_fn, optimizer, device, num_epochs):\n",
    "    train_losses = []\n",
    "    train_accs = []\n",
    "    val_losses = []\n",
    "    val_accs = []\n",
    "    best_val_acc = 0.0\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for batch, (X, y) in enumerate(train_loader):\n",
    "            # Move tensors to the configured device\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Compute prediction and loss\n",
    "            pred = model(X)\n",
    "\n",
    "            # Compute loss and its gradients\n",
    "            loss = loss_fn(pred, y)\n",
    "            loss.backward()\n",
    "            \n",
    "\n",
    "            # Backpropagation step\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(pred.data, 1)\n",
    "            total += y.size(0)\n",
    "            correct += (predicted == y.argmax(1)).sum().item()\n",
    "\n",
    "            # Display progress\n",
    "            if batch % 5 == 0:\n",
    "                loss, current = loss.item(), batch * len(X)\n",
    "                print(f\"Epoch {epoch+1}, batch {batch+1}/{len(train_loader)}, loss: {loss:.4f}\")\n",
    "        \n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_acc = 100 * correct / total\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        print(f\"Epoch {epoch+1}, train loss: {train_loss:.4f}, train accuracy: {train_acc:.2f}%\")\n",
    "\n",
    "        # Validation\n",
    "\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for batch, (X, y) in enumerate(val_loader):\n",
    "                X, y = X.to(device), y.to(device)\n",
    "                pred = model(X)\n",
    "                loss = loss_fn(pred, y)\n",
    "                running_loss += loss.item()\n",
    "                _, predicted = torch.max(pred.data, 1)\n",
    "                total += y.size(0)\n",
    "                correct += (predicted == y.argmax(1)).sum().item()\n",
    "            val_loss = running_loss / len(val_loader)\n",
    "            val_acc = 100 * correct / total\n",
    "            val_losses.append(val_loss)\n",
    "            val_accs.append(val_acc)\n",
    "            print(f\"Epoch {epoch+1}, val loss: {val_loss:.4f}, val accuracy: {val_acc:.2f}%\")\n",
    "        \n",
    "            # # Save best model\n",
    "            # if val_acc > best_val_acc:\n",
    "            #     best_val_acc = val_acc\n",
    "            #     torch.save(model.state_dict(), \"best_model.pt\")\n",
    "\n",
    "    return train_losses, train_accs, val_losses, val_accs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f224c264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, batch 1/93, loss: 2.6524\n",
      "Epoch 1, batch 6/93, loss: 2.2379\n",
      "Epoch 1, batch 11/93, loss: 1.9756\n",
      "Epoch 1, batch 16/93, loss: 1.8687\n",
      "Epoch 1, batch 21/93, loss: 1.7609\n",
      "Epoch 1, batch 26/93, loss: 1.7980\n",
      "Epoch 1, batch 31/93, loss: 1.6581\n",
      "Epoch 1, batch 36/93, loss: 1.6107\n",
      "Epoch 1, batch 41/93, loss: 1.5668\n",
      "Epoch 1, batch 46/93, loss: 1.5470\n",
      "Epoch 1, batch 51/93, loss: 1.5341\n",
      "Epoch 1, batch 56/93, loss: 1.5425\n",
      "Epoch 1, batch 61/93, loss: 1.5396\n",
      "Epoch 1, batch 66/93, loss: 1.5971\n",
      "Epoch 1, batch 71/93, loss: 1.3395\n",
      "Epoch 1, batch 76/93, loss: 1.3418\n",
      "Epoch 1, batch 81/93, loss: 1.3723\n",
      "Epoch 1, batch 86/93, loss: 1.4420\n",
      "Epoch 1, batch 91/93, loss: 1.3374\n",
      "Epoch 1, train loss: 1.6491, train accuracy: 42.31%\n",
      "Epoch 1, val loss: 1.3483, val accuracy: 55.25%\n",
      "Epoch 2, batch 1/93, loss: 1.1841\n",
      "Epoch 2, batch 6/93, loss: 1.3050\n",
      "Epoch 2, batch 11/93, loss: 1.2581\n",
      "Epoch 2, batch 16/93, loss: 1.1998\n",
      "Epoch 2, batch 21/93, loss: 1.2234\n",
      "Epoch 2, batch 26/93, loss: 1.2120\n",
      "Epoch 2, batch 31/93, loss: 1.1327\n",
      "Epoch 2, batch 36/93, loss: 1.1570\n",
      "Epoch 2, batch 41/93, loss: 1.0989\n",
      "Epoch 2, batch 46/93, loss: 1.0419\n",
      "Epoch 2, batch 51/93, loss: 1.1356\n",
      "Epoch 2, batch 56/93, loss: 1.2142\n",
      "Epoch 2, batch 61/93, loss: 1.0424\n",
      "Epoch 2, batch 66/93, loss: 1.0062\n",
      "Epoch 2, batch 71/93, loss: 1.1249\n",
      "Epoch 2, batch 76/93, loss: 0.9618\n",
      "Epoch 2, batch 81/93, loss: 1.0993\n",
      "Epoch 2, batch 86/93, loss: 1.0837\n",
      "Epoch 2, batch 91/93, loss: 1.0290\n",
      "Epoch 2, train loss: 1.1257, train accuracy: 61.76%\n",
      "Epoch 2, val loss: 1.0868, val accuracy: 63.72%\n",
      "Epoch 3, batch 1/93, loss: 1.0718\n",
      "Epoch 3, batch 6/93, loss: 1.0203\n",
      "Epoch 3, batch 11/93, loss: 0.9740\n",
      "Epoch 3, batch 16/93, loss: 0.9611\n",
      "Epoch 3, batch 21/93, loss: 1.0482\n",
      "Epoch 3, batch 26/93, loss: 0.8973\n",
      "Epoch 3, batch 31/93, loss: 0.9982\n",
      "Epoch 3, batch 36/93, loss: 0.9157\n",
      "Epoch 3, batch 41/93, loss: 0.7571\n",
      "Epoch 3, batch 46/93, loss: 0.9414\n",
      "Epoch 3, batch 51/93, loss: 0.9099\n",
      "Epoch 3, batch 56/93, loss: 0.8538\n",
      "Epoch 3, batch 61/93, loss: 0.8323\n",
      "Epoch 3, batch 66/93, loss: 0.8841\n",
      "Epoch 3, batch 71/93, loss: 0.8597\n",
      "Epoch 3, batch 76/93, loss: 0.8287\n",
      "Epoch 3, batch 81/93, loss: 0.9000\n",
      "Epoch 3, batch 86/93, loss: 0.9317\n",
      "Epoch 3, batch 91/93, loss: 0.9163\n",
      "Epoch 3, train loss: 0.9344, train accuracy: 68.95%\n",
      "Epoch 3, val loss: 0.9796, val accuracy: 68.40%\n",
      "Epoch 4, batch 1/93, loss: 0.8735\n",
      "Epoch 4, batch 6/93, loss: 0.8523\n",
      "Epoch 4, batch 11/93, loss: 0.8603\n",
      "Epoch 4, batch 16/93, loss: 0.7785\n",
      "Epoch 4, batch 21/93, loss: 0.7251\n",
      "Epoch 4, batch 26/93, loss: 0.9695\n",
      "Epoch 4, batch 31/93, loss: 0.8095\n",
      "Epoch 4, batch 36/93, loss: 0.7469\n",
      "Epoch 4, batch 41/93, loss: 0.7372\n",
      "Epoch 4, batch 46/93, loss: 0.8930\n",
      "Epoch 4, batch 51/93, loss: 0.9475\n",
      "Epoch 4, batch 56/93, loss: 0.7602\n",
      "Epoch 4, batch 61/93, loss: 0.9021\n",
      "Epoch 4, batch 66/93, loss: 0.8124\n",
      "Epoch 4, batch 71/93, loss: 0.8229\n",
      "Epoch 4, batch 76/93, loss: 0.8386\n",
      "Epoch 4, batch 81/93, loss: 0.7656\n",
      "Epoch 4, batch 86/93, loss: 0.8377\n",
      "Epoch 4, batch 91/93, loss: 0.9157\n",
      "Epoch 4, train loss: 0.8137, train accuracy: 71.96%\n",
      "Epoch 4, val loss: 0.8437, val accuracy: 72.83%\n",
      "Epoch 5, batch 1/93, loss: 0.8274\n",
      "Epoch 5, batch 6/93, loss: 0.7928\n",
      "Epoch 5, batch 11/93, loss: 0.8178\n",
      "Epoch 5, batch 16/93, loss: 0.9799\n",
      "Epoch 5, batch 21/93, loss: 0.6840\n",
      "Epoch 5, batch 26/93, loss: 0.6680\n",
      "Epoch 5, batch 31/93, loss: 0.6525\n",
      "Epoch 5, batch 36/93, loss: 0.6581\n",
      "Epoch 5, batch 41/93, loss: 0.7153\n",
      "Epoch 5, batch 46/93, loss: 0.7992\n",
      "Epoch 5, batch 51/93, loss: 0.7592\n",
      "Epoch 5, batch 56/93, loss: 0.5706\n",
      "Epoch 5, batch 61/93, loss: 0.7919\n",
      "Epoch 5, batch 66/93, loss: 0.7897\n",
      "Epoch 5, batch 71/93, loss: 0.5998\n",
      "Epoch 5, batch 76/93, loss: 0.7236\n",
      "Epoch 5, batch 81/93, loss: 0.6919\n",
      "Epoch 5, batch 86/93, loss: 0.6688\n",
      "Epoch 5, batch 91/93, loss: 0.7506\n",
      "Epoch 5, train loss: 0.7415, train accuracy: 75.06%\n",
      "Epoch 5, val loss: 0.8269, val accuracy: 73.00%\n",
      "Epoch 6, batch 1/93, loss: 0.6816\n",
      "Epoch 6, batch 6/93, loss: 0.7885\n",
      "Epoch 6, batch 11/93, loss: 0.9276\n",
      "Epoch 6, batch 16/93, loss: 0.7361\n",
      "Epoch 6, batch 21/93, loss: 0.6858\n",
      "Epoch 6, batch 26/93, loss: 0.5686\n",
      "Epoch 6, batch 31/93, loss: 0.6269\n",
      "Epoch 6, batch 36/93, loss: 0.6599\n",
      "Epoch 6, batch 41/93, loss: 0.6282\n",
      "Epoch 6, batch 46/93, loss: 0.5896\n",
      "Epoch 6, batch 51/93, loss: 0.8676\n",
      "Epoch 6, batch 56/93, loss: 0.7378\n",
      "Epoch 6, batch 61/93, loss: 0.5794\n",
      "Epoch 6, batch 66/93, loss: 0.6633\n",
      "Epoch 6, batch 71/93, loss: 0.6686\n",
      "Epoch 6, batch 76/93, loss: 0.7091\n",
      "Epoch 6, batch 81/93, loss: 0.6752\n",
      "Epoch 6, batch 86/93, loss: 0.6553\n",
      "Epoch 6, batch 91/93, loss: 0.8298\n",
      "Epoch 6, train loss: 0.7083, train accuracy: 75.17%\n",
      "Epoch 6, val loss: 0.8352, val accuracy: 73.00%\n",
      "Epoch 7, batch 1/93, loss: 0.6081\n",
      "Epoch 7, batch 6/93, loss: 0.6349\n",
      "Epoch 7, batch 11/93, loss: 0.6425\n",
      "Epoch 7, batch 16/93, loss: 0.6751\n",
      "Epoch 7, batch 21/93, loss: 0.7645\n",
      "Epoch 7, batch 26/93, loss: 0.6383\n",
      "Epoch 7, batch 31/93, loss: 0.5320\n",
      "Epoch 7, batch 36/93, loss: 0.5790\n",
      "Epoch 7, batch 41/93, loss: 0.6991\n",
      "Epoch 7, batch 46/93, loss: 0.5412\n",
      "Epoch 7, batch 51/93, loss: 0.6480\n",
      "Epoch 7, batch 56/93, loss: 0.6585\n",
      "Epoch 7, batch 61/93, loss: 0.5572\n",
      "Epoch 7, batch 66/93, loss: 0.6364\n",
      "Epoch 7, batch 71/93, loss: 0.6639\n",
      "Epoch 7, batch 76/93, loss: 0.6865\n",
      "Epoch 7, batch 81/93, loss: 0.4633\n",
      "Epoch 7, batch 86/93, loss: 0.6975\n",
      "Epoch 7, batch 91/93, loss: 0.7163\n",
      "Epoch 7, train loss: 0.6639, train accuracy: 77.17%\n",
      "Epoch 7, val loss: 0.7670, val accuracy: 75.69%\n",
      "Epoch 8, batch 1/93, loss: 0.6190\n",
      "Epoch 8, batch 6/93, loss: 0.5853\n",
      "Epoch 8, batch 11/93, loss: 0.5450\n",
      "Epoch 8, batch 16/93, loss: 0.6448\n",
      "Epoch 8, batch 21/93, loss: 0.7069\n",
      "Epoch 8, batch 26/93, loss: 0.7346\n",
      "Epoch 8, batch 31/93, loss: 0.4650\n",
      "Epoch 8, batch 36/93, loss: 0.6032\n",
      "Epoch 8, batch 41/93, loss: 0.4908\n",
      "Epoch 8, batch 46/93, loss: 0.5382\n",
      "Epoch 8, batch 51/93, loss: 0.5629\n",
      "Epoch 8, batch 56/93, loss: 0.6067\n",
      "Epoch 8, batch 61/93, loss: 0.7461\n",
      "Epoch 8, batch 66/93, loss: 0.7224\n",
      "Epoch 8, batch 71/93, loss: 0.6241\n",
      "Epoch 8, batch 76/93, loss: 0.6373\n",
      "Epoch 8, batch 81/93, loss: 0.6504\n",
      "Epoch 8, batch 86/93, loss: 0.7381\n",
      "Epoch 8, batch 91/93, loss: 0.7303\n",
      "Epoch 8, train loss: 0.6295, train accuracy: 78.24%\n",
      "Epoch 8, val loss: 0.7777, val accuracy: 74.22%\n",
      "Epoch 9, batch 1/93, loss: 0.5297\n",
      "Epoch 9, batch 6/93, loss: 0.7412\n",
      "Epoch 9, batch 11/93, loss: 0.4556\n",
      "Epoch 9, batch 16/93, loss: 0.5941\n",
      "Epoch 9, batch 21/93, loss: 0.6340\n",
      "Epoch 9, batch 26/93, loss: 0.5691\n",
      "Epoch 9, batch 31/93, loss: 0.4548\n",
      "Epoch 9, batch 36/93, loss: 0.5254\n",
      "Epoch 9, batch 41/93, loss: 0.5714\n",
      "Epoch 9, batch 46/93, loss: 0.5778\n",
      "Epoch 9, batch 51/93, loss: 0.6500\n",
      "Epoch 9, batch 56/93, loss: 0.6665\n",
      "Epoch 9, batch 61/93, loss: 0.5142\n",
      "Epoch 9, batch 66/93, loss: 0.6770\n",
      "Epoch 9, batch 71/93, loss: 0.6667\n",
      "Epoch 9, batch 76/93, loss: 0.5736\n",
      "Epoch 9, batch 81/93, loss: 0.5015\n",
      "Epoch 9, batch 86/93, loss: 0.6248\n",
      "Epoch 9, batch 91/93, loss: 0.5296\n",
      "Epoch 9, train loss: 0.5965, train accuracy: 79.32%\n",
      "Epoch 9, val loss: 0.6862, val accuracy: 78.17%\n",
      "Epoch 10, batch 1/93, loss: 0.5542\n",
      "Epoch 10, batch 6/93, loss: 0.4402\n",
      "Epoch 10, batch 11/93, loss: 0.4808\n",
      "Epoch 10, batch 16/93, loss: 0.5174\n",
      "Epoch 10, batch 21/93, loss: 0.5379\n",
      "Epoch 10, batch 26/93, loss: 0.5350\n",
      "Epoch 10, batch 31/93, loss: 0.6767\n",
      "Epoch 10, batch 36/93, loss: 0.5259\n",
      "Epoch 10, batch 41/93, loss: 0.5288\n",
      "Epoch 10, batch 46/93, loss: 0.5407\n",
      "Epoch 10, batch 51/93, loss: 0.6475\n",
      "Epoch 10, batch 56/93, loss: 0.5672\n",
      "Epoch 10, batch 61/93, loss: 0.6145\n",
      "Epoch 10, batch 66/93, loss: 0.5195\n",
      "Epoch 10, batch 71/93, loss: 0.4765\n",
      "Epoch 10, batch 76/93, loss: 0.5580\n",
      "Epoch 10, batch 81/93, loss: 0.6090\n",
      "Epoch 10, batch 86/93, loss: 0.5022\n",
      "Epoch 10, batch 91/93, loss: 0.4670\n",
      "Epoch 10, train loss: 0.5518, train accuracy: 80.90%\n",
      "Epoch 10, val loss: 0.7196, val accuracy: 77.30%\n",
      "Epoch 11, batch 1/93, loss: 0.5854\n",
      "Epoch 11, batch 6/93, loss: 0.7206\n",
      "Epoch 11, batch 11/93, loss: 0.4992\n",
      "Epoch 11, batch 16/93, loss: 0.5345\n",
      "Epoch 11, batch 21/93, loss: 0.4380\n",
      "Epoch 11, batch 26/93, loss: 0.5539\n",
      "Epoch 11, batch 31/93, loss: 0.5038\n",
      "Epoch 11, batch 36/93, loss: 0.5668\n",
      "Epoch 11, batch 41/93, loss: 0.4052\n",
      "Epoch 11, batch 46/93, loss: 0.5752\n",
      "Epoch 11, batch 51/93, loss: 0.4002\n",
      "Epoch 11, batch 56/93, loss: 0.4466\n",
      "Epoch 11, batch 61/93, loss: 0.5438\n",
      "Epoch 11, batch 66/93, loss: 0.4910\n",
      "Epoch 11, batch 71/93, loss: 0.4089\n",
      "Epoch 11, batch 76/93, loss: 0.4664\n",
      "Epoch 11, batch 81/93, loss: 0.4898\n",
      "Epoch 11, batch 86/93, loss: 0.4535\n",
      "Epoch 11, batch 91/93, loss: 0.3529\n",
      "Epoch 11, train loss: 0.5332, train accuracy: 81.48%\n",
      "Epoch 11, val loss: 0.6872, val accuracy: 79.51%\n",
      "Epoch 12, batch 1/93, loss: 0.5052\n",
      "Epoch 12, batch 6/93, loss: 0.5855\n",
      "Epoch 12, batch 11/93, loss: 0.5807\n",
      "Epoch 12, batch 16/93, loss: 0.4683\n",
      "Epoch 12, batch 21/93, loss: 0.5761\n",
      "Epoch 12, batch 26/93, loss: 0.5482\n",
      "Epoch 12, batch 31/93, loss: 0.5091\n",
      "Epoch 12, batch 36/93, loss: 0.3674\n",
      "Epoch 12, batch 41/93, loss: 0.4257\n",
      "Epoch 12, batch 46/93, loss: 0.5119\n",
      "Epoch 12, batch 51/93, loss: 0.4968\n",
      "Epoch 12, batch 56/93, loss: 0.4685\n",
      "Epoch 12, batch 61/93, loss: 0.3532\n",
      "Epoch 12, batch 66/93, loss: 0.4568\n",
      "Epoch 12, batch 71/93, loss: 0.3507\n",
      "Epoch 12, batch 76/93, loss: 0.6131\n",
      "Epoch 12, batch 81/93, loss: 0.6036\n",
      "Epoch 12, batch 86/93, loss: 0.5272\n",
      "Epoch 12, batch 91/93, loss: 0.5316\n",
      "Epoch 12, train loss: 0.5119, train accuracy: 82.88%\n",
      "Epoch 12, val loss: 0.7167, val accuracy: 76.13%\n",
      "Epoch 13, batch 1/93, loss: 0.4837\n",
      "Epoch 13, batch 6/93, loss: 0.6457\n",
      "Epoch 13, batch 11/93, loss: 0.6140\n",
      "Epoch 13, batch 16/93, loss: 0.7089\n",
      "Epoch 13, batch 21/93, loss: 0.8380\n",
      "Epoch 13, batch 26/93, loss: 0.6385\n",
      "Epoch 13, batch 31/93, loss: 0.8328\n",
      "Epoch 13, batch 36/93, loss: 0.4784\n",
      "Epoch 13, batch 41/93, loss: 0.4598\n",
      "Epoch 13, batch 46/93, loss: 0.3743\n",
      "Epoch 13, batch 51/93, loss: 0.5443\n",
      "Epoch 13, batch 56/93, loss: 0.5392\n",
      "Epoch 13, batch 61/93, loss: 0.5644\n",
      "Epoch 13, batch 66/93, loss: 0.4252\n",
      "Epoch 13, batch 71/93, loss: 0.4495\n",
      "Epoch 13, batch 76/93, loss: 0.6226\n",
      "Epoch 13, batch 81/93, loss: 0.6242\n",
      "Epoch 13, batch 86/93, loss: 0.4899\n",
      "Epoch 13, batch 91/93, loss: 0.4595\n",
      "Epoch 13, train loss: 0.5555, train accuracy: 81.24%\n",
      "Epoch 13, val loss: 0.6449, val accuracy: 79.64%\n",
      "Epoch 14, batch 1/93, loss: 0.4246\n",
      "Epoch 14, batch 6/93, loss: 0.4858\n",
      "Epoch 14, batch 11/93, loss: 0.6782\n",
      "Epoch 14, batch 16/93, loss: 0.3264\n",
      "Epoch 14, batch 21/93, loss: 0.5021\n",
      "Epoch 14, batch 26/93, loss: 0.3991\n",
      "Epoch 14, batch 31/93, loss: 0.5741\n",
      "Epoch 14, batch 36/93, loss: 0.4631\n",
      "Epoch 14, batch 41/93, loss: 0.5730\n",
      "Epoch 14, batch 46/93, loss: 0.4978\n",
      "Epoch 14, batch 51/93, loss: 0.3420\n",
      "Epoch 14, batch 56/93, loss: 0.4970\n",
      "Epoch 14, batch 61/93, loss: 0.5168\n",
      "Epoch 14, batch 66/93, loss: 0.4266\n",
      "Epoch 14, batch 71/93, loss: 0.4998\n",
      "Epoch 14, batch 76/93, loss: 0.5827\n",
      "Epoch 14, batch 81/93, loss: 0.5037\n",
      "Epoch 14, batch 86/93, loss: 0.5311\n",
      "Epoch 14, batch 91/93, loss: 0.3866\n",
      "Epoch 14, train loss: 0.4937, train accuracy: 82.81%\n",
      "Epoch 14, val loss: 0.6472, val accuracy: 80.47%\n",
      "Epoch 15, batch 1/93, loss: 0.4861\n",
      "Epoch 15, batch 6/93, loss: 0.5701\n",
      "Epoch 15, batch 11/93, loss: 0.5277\n",
      "Epoch 15, batch 16/93, loss: 0.4706\n",
      "Epoch 15, batch 21/93, loss: 0.6087\n",
      "Epoch 15, batch 26/93, loss: 0.3265\n",
      "Epoch 15, batch 31/93, loss: 0.5187\n",
      "Epoch 15, batch 36/93, loss: 0.3558\n",
      "Epoch 15, batch 41/93, loss: 0.4637\n",
      "Epoch 15, batch 46/93, loss: 0.3987\n",
      "Epoch 15, batch 51/93, loss: 0.6184\n",
      "Epoch 15, batch 56/93, loss: 0.4865\n",
      "Epoch 15, batch 61/93, loss: 0.4431\n",
      "Epoch 15, batch 66/93, loss: 0.5487\n",
      "Epoch 15, batch 71/93, loss: 0.3687\n",
      "Epoch 15, batch 76/93, loss: 0.4916\n",
      "Epoch 15, batch 81/93, loss: 0.5335\n",
      "Epoch 15, batch 86/93, loss: 0.4518\n",
      "Epoch 15, batch 91/93, loss: 0.4058\n",
      "Epoch 15, train loss: 0.4583, train accuracy: 84.10%\n",
      "Epoch 15, val loss: 0.5661, val accuracy: 80.95%\n",
      "Epoch 16, batch 1/93, loss: 0.3387\n",
      "Epoch 16, batch 6/93, loss: 0.5286\n",
      "Epoch 16, batch 11/93, loss: 0.4816\n",
      "Epoch 16, batch 16/93, loss: 0.5180\n",
      "Epoch 16, batch 21/93, loss: 0.4256\n",
      "Epoch 16, batch 26/93, loss: 0.4587\n",
      "Epoch 16, batch 31/93, loss: 0.4578\n",
      "Epoch 16, batch 36/93, loss: 0.5037\n",
      "Epoch 16, batch 41/93, loss: 0.4468\n",
      "Epoch 16, batch 46/93, loss: 0.5092\n",
      "Epoch 16, batch 51/93, loss: 0.3365\n",
      "Epoch 16, batch 56/93, loss: 0.3913\n",
      "Epoch 16, batch 61/93, loss: 0.3575\n",
      "Epoch 16, batch 66/93, loss: 0.3608\n",
      "Epoch 16, batch 71/93, loss: 0.4556\n",
      "Epoch 16, batch 76/93, loss: 0.4348\n",
      "Epoch 16, batch 81/93, loss: 0.4536\n",
      "Epoch 16, batch 86/93, loss: 0.3658\n",
      "Epoch 16, batch 91/93, loss: 0.4621\n",
      "Epoch 16, train loss: 0.4333, train accuracy: 85.01%\n",
      "Epoch 16, val loss: 0.6293, val accuracy: 80.03%\n",
      "Epoch 17, batch 1/93, loss: 0.5406\n",
      "Epoch 17, batch 6/93, loss: 0.4072\n",
      "Epoch 17, batch 11/93, loss: 0.3961\n",
      "Epoch 17, batch 16/93, loss: 0.4831\n",
      "Epoch 17, batch 21/93, loss: 0.4746\n",
      "Epoch 17, batch 26/93, loss: 0.4412\n",
      "Epoch 17, batch 31/93, loss: 0.3421\n",
      "Epoch 17, batch 36/93, loss: 0.2955\n",
      "Epoch 17, batch 41/93, loss: 0.4434\n",
      "Epoch 17, batch 46/93, loss: 0.3352\n",
      "Epoch 17, batch 51/93, loss: 0.4045\n",
      "Epoch 17, batch 56/93, loss: 0.2833\n",
      "Epoch 17, batch 61/93, loss: 0.3204\n",
      "Epoch 17, batch 66/93, loss: 0.4153\n",
      "Epoch 17, batch 71/93, loss: 0.3285\n",
      "Epoch 17, batch 76/93, loss: 0.3440\n",
      "Epoch 17, batch 81/93, loss: 0.3701\n",
      "Epoch 17, batch 86/93, loss: 0.3690\n",
      "Epoch 17, batch 91/93, loss: 0.4773\n",
      "Epoch 17, train loss: 0.4294, train accuracy: 85.21%\n",
      "Epoch 17, val loss: 0.6275, val accuracy: 80.99%\n",
      "Epoch 18, batch 1/93, loss: 0.3731\n",
      "Epoch 18, batch 6/93, loss: 0.4645\n",
      "Epoch 18, batch 11/93, loss: 0.3991\n",
      "Epoch 18, batch 16/93, loss: 0.4703\n",
      "Epoch 18, batch 21/93, loss: 0.6122\n",
      "Epoch 18, batch 26/93, loss: 0.4402\n",
      "Epoch 18, batch 31/93, loss: 0.4689\n",
      "Epoch 18, batch 36/93, loss: 0.3749\n",
      "Epoch 18, batch 41/93, loss: 0.4415\n",
      "Epoch 18, batch 46/93, loss: 0.3520\n",
      "Epoch 18, batch 51/93, loss: 0.5596\n",
      "Epoch 18, batch 56/93, loss: 0.3893\n",
      "Epoch 18, batch 61/93, loss: 0.4241\n",
      "Epoch 18, batch 66/93, loss: 0.3318\n",
      "Epoch 18, batch 71/93, loss: 0.3641\n",
      "Epoch 18, batch 76/93, loss: 0.4658\n",
      "Epoch 18, batch 81/93, loss: 0.3765\n",
      "Epoch 18, batch 86/93, loss: 0.2972\n",
      "Epoch 18, batch 91/93, loss: 0.3538\n",
      "Epoch 18, train loss: 0.4073, train accuracy: 86.35%\n",
      "Epoch 18, val loss: 0.5903, val accuracy: 82.90%\n",
      "Epoch 19, batch 1/93, loss: 0.2631\n",
      "Epoch 19, batch 6/93, loss: 0.4759\n",
      "Epoch 19, batch 11/93, loss: 0.3443\n",
      "Epoch 19, batch 16/93, loss: 0.3916\n",
      "Epoch 19, batch 21/93, loss: 0.2363\n",
      "Epoch 19, batch 26/93, loss: 0.4217\n",
      "Epoch 19, batch 31/93, loss: 0.4111\n",
      "Epoch 19, batch 36/93, loss: 0.3061\n",
      "Epoch 19, batch 41/93, loss: 0.2881\n",
      "Epoch 19, batch 46/93, loss: 0.2947\n",
      "Epoch 19, batch 51/93, loss: 0.3927\n",
      "Epoch 19, batch 56/93, loss: 0.3321\n",
      "Epoch 19, batch 61/93, loss: 0.3118\n",
      "Epoch 19, batch 66/93, loss: 0.4587\n",
      "Epoch 19, batch 71/93, loss: 0.4473\n",
      "Epoch 19, batch 76/93, loss: 0.3393\n",
      "Epoch 19, batch 81/93, loss: 0.3735\n",
      "Epoch 19, batch 86/93, loss: 0.3958\n",
      "Epoch 19, batch 91/93, loss: 0.6210\n",
      "Epoch 19, train loss: 0.3955, train accuracy: 86.60%\n",
      "Epoch 19, val loss: 0.6711, val accuracy: 78.43%\n",
      "Epoch 20, batch 1/93, loss: 0.4573\n",
      "Epoch 20, batch 6/93, loss: 0.3869\n",
      "Epoch 20, batch 11/93, loss: 0.3466\n",
      "Epoch 20, batch 16/93, loss: 0.3897\n",
      "Epoch 20, batch 21/93, loss: 0.3466\n",
      "Epoch 20, batch 26/93, loss: 0.5780\n",
      "Epoch 20, batch 31/93, loss: 0.2878\n",
      "Epoch 20, batch 36/93, loss: 0.3531\n",
      "Epoch 20, batch 41/93, loss: 0.3415\n",
      "Epoch 20, batch 46/93, loss: 0.4678\n",
      "Epoch 20, batch 51/93, loss: 0.2470\n",
      "Epoch 20, batch 56/93, loss: 0.2802\n",
      "Epoch 20, batch 61/93, loss: 0.4885\n",
      "Epoch 20, batch 66/93, loss: 0.3403\n",
      "Epoch 20, batch 71/93, loss: 0.2724\n",
      "Epoch 20, batch 76/93, loss: 0.2831\n",
      "Epoch 20, batch 81/93, loss: 0.3209\n",
      "Epoch 20, batch 86/93, loss: 0.3745\n",
      "Epoch 20, batch 91/93, loss: 0.3419\n",
      "Epoch 20, train loss: 0.3861, train accuracy: 86.88%\n",
      "Epoch 20, val loss: 0.5851, val accuracy: 82.29%\n",
      "Epoch 21, batch 1/93, loss: 0.3297\n",
      "Epoch 21, batch 6/93, loss: 0.2199\n",
      "Epoch 21, batch 11/93, loss: 0.3438\n",
      "Epoch 21, batch 16/93, loss: 0.2880\n",
      "Epoch 21, batch 21/93, loss: 0.3479\n",
      "Epoch 21, batch 26/93, loss: 0.3275\n",
      "Epoch 21, batch 31/93, loss: 0.3845\n",
      "Epoch 21, batch 36/93, loss: 0.3857\n",
      "Epoch 21, batch 41/93, loss: 0.5028\n",
      "Epoch 21, batch 46/93, loss: 0.3046\n",
      "Epoch 21, batch 51/93, loss: 0.3683\n",
      "Epoch 21, batch 56/93, loss: 0.4544\n",
      "Epoch 21, batch 61/93, loss: 0.2976\n",
      "Epoch 21, batch 66/93, loss: 0.3230\n",
      "Epoch 21, batch 71/93, loss: 0.4036\n",
      "Epoch 21, batch 76/93, loss: 0.3635\n",
      "Epoch 21, batch 81/93, loss: 0.3410\n",
      "Epoch 21, batch 86/93, loss: 0.3167\n",
      "Epoch 21, batch 91/93, loss: 0.3401\n",
      "Epoch 21, train loss: 0.3609, train accuracy: 87.40%\n",
      "Epoch 21, val loss: 0.5309, val accuracy: 82.55%\n",
      "Epoch 22, batch 1/93, loss: 0.3082\n",
      "Epoch 22, batch 6/93, loss: 0.2942\n",
      "Epoch 22, batch 11/93, loss: 0.2922\n",
      "Epoch 22, batch 16/93, loss: 0.3669\n",
      "Epoch 22, batch 21/93, loss: 0.3302\n",
      "Epoch 22, batch 26/93, loss: 0.2977\n",
      "Epoch 22, batch 31/93, loss: 0.4109\n",
      "Epoch 22, batch 36/93, loss: 0.4211\n",
      "Epoch 22, batch 41/93, loss: 0.3993\n",
      "Epoch 22, batch 46/93, loss: 0.3679\n",
      "Epoch 22, batch 51/93, loss: 0.3794\n",
      "Epoch 22, batch 56/93, loss: 0.3097\n",
      "Epoch 22, batch 61/93, loss: 0.3062\n",
      "Epoch 22, batch 66/93, loss: 0.4344\n",
      "Epoch 22, batch 71/93, loss: 0.3278\n",
      "Epoch 22, batch 76/93, loss: 0.2524\n",
      "Epoch 22, batch 81/93, loss: 0.3248\n",
      "Epoch 22, batch 86/93, loss: 0.3803\n",
      "Epoch 22, batch 91/93, loss: 0.3028\n",
      "Epoch 22, train loss: 0.3245, train accuracy: 88.68%\n",
      "Epoch 22, val loss: 0.5003, val accuracy: 82.99%\n",
      "Epoch 23, batch 1/93, loss: 0.1543\n",
      "Epoch 23, batch 6/93, loss: 0.2400\n",
      "Epoch 23, batch 11/93, loss: 0.2830\n",
      "Epoch 23, batch 16/93, loss: 0.2821\n",
      "Epoch 23, batch 21/93, loss: 0.5756\n",
      "Epoch 23, batch 26/93, loss: 0.3232\n",
      "Epoch 23, batch 31/93, loss: 0.4023\n",
      "Epoch 23, batch 36/93, loss: 0.3060\n",
      "Epoch 23, batch 41/93, loss: 0.3729\n",
      "Epoch 23, batch 46/93, loss: 0.3821\n",
      "Epoch 23, batch 51/93, loss: 0.4407\n",
      "Epoch 23, batch 56/93, loss: 0.3287\n",
      "Epoch 23, batch 61/93, loss: 0.4029\n",
      "Epoch 23, batch 66/93, loss: 0.2846\n",
      "Epoch 23, batch 71/93, loss: 0.2725\n",
      "Epoch 23, batch 76/93, loss: 0.2653\n",
      "Epoch 23, batch 81/93, loss: 0.3266\n",
      "Epoch 23, batch 86/93, loss: 0.2479\n",
      "Epoch 23, batch 91/93, loss: 0.4117\n",
      "Epoch 23, train loss: 0.3257, train accuracy: 88.68%\n",
      "Epoch 23, val loss: 0.5660, val accuracy: 82.73%\n",
      "Epoch 24, batch 1/93, loss: 0.4304\n",
      "Epoch 24, batch 6/93, loss: 0.2954\n",
      "Epoch 24, batch 11/93, loss: 0.3270\n",
      "Epoch 24, batch 16/93, loss: 0.3210\n",
      "Epoch 24, batch 21/93, loss: 0.2721\n",
      "Epoch 24, batch 26/93, loss: 0.2377\n",
      "Epoch 24, batch 31/93, loss: 0.2964\n",
      "Epoch 24, batch 36/93, loss: 0.2847\n",
      "Epoch 24, batch 41/93, loss: 0.2899\n",
      "Epoch 24, batch 46/93, loss: 0.2947\n",
      "Epoch 24, batch 51/93, loss: 0.2632\n",
      "Epoch 24, batch 56/93, loss: 0.2277\n",
      "Epoch 24, batch 61/93, loss: 0.1234\n",
      "Epoch 24, batch 66/93, loss: 0.4016\n",
      "Epoch 24, batch 71/93, loss: 0.2410\n",
      "Epoch 24, batch 76/93, loss: 0.3527\n",
      "Epoch 24, batch 81/93, loss: 0.3289\n",
      "Epoch 24, batch 86/93, loss: 0.2285\n",
      "Epoch 24, batch 91/93, loss: 0.2312\n",
      "Epoch 24, train loss: 0.3251, train accuracy: 88.75%\n",
      "Epoch 24, val loss: 0.6089, val accuracy: 81.25%\n",
      "Epoch 25, batch 1/93, loss: 0.3433\n",
      "Epoch 25, batch 6/93, loss: 0.2920\n",
      "Epoch 25, batch 11/93, loss: 0.3865\n",
      "Epoch 25, batch 16/93, loss: 0.2962\n",
      "Epoch 25, batch 21/93, loss: 0.2528\n",
      "Epoch 25, batch 26/93, loss: 0.4592\n",
      "Epoch 25, batch 31/93, loss: 0.3248\n",
      "Epoch 25, batch 36/93, loss: 0.3790\n",
      "Epoch 25, batch 41/93, loss: 0.2703\n",
      "Epoch 25, batch 46/93, loss: 0.3407\n",
      "Epoch 25, batch 51/93, loss: 0.1931\n",
      "Epoch 25, batch 56/93, loss: 0.2722\n",
      "Epoch 25, batch 61/93, loss: 0.2867\n",
      "Epoch 25, batch 66/93, loss: 0.2436\n",
      "Epoch 25, batch 71/93, loss: 0.2618\n",
      "Epoch 25, batch 76/93, loss: 0.2335\n",
      "Epoch 25, batch 81/93, loss: 0.2150\n",
      "Epoch 25, batch 86/93, loss: 0.3607\n",
      "Epoch 25, batch 91/93, loss: 0.2509\n",
      "Epoch 25, train loss: 0.3150, train accuracy: 89.35%\n",
      "Epoch 25, val loss: 0.6119, val accuracy: 82.29%\n",
      "Epoch 26, batch 1/93, loss: 0.3815\n",
      "Epoch 26, batch 6/93, loss: 0.3675\n",
      "Epoch 26, batch 11/93, loss: 0.4192\n",
      "Epoch 26, batch 16/93, loss: 0.4472\n",
      "Epoch 26, batch 21/93, loss: 0.3496\n",
      "Epoch 26, batch 26/93, loss: 0.3216\n",
      "Epoch 26, batch 31/93, loss: 0.2092\n",
      "Epoch 26, batch 36/93, loss: 0.3398\n",
      "Epoch 26, batch 41/93, loss: 0.2647\n",
      "Epoch 26, batch 46/93, loss: 0.3121\n",
      "Epoch 26, batch 51/93, loss: 0.2165\n",
      "Epoch 26, batch 56/93, loss: 0.2749\n",
      "Epoch 26, batch 61/93, loss: 0.2999\n",
      "Epoch 26, batch 66/93, loss: 0.3484\n",
      "Epoch 26, batch 71/93, loss: 0.4857\n",
      "Epoch 26, batch 76/93, loss: 0.2490\n",
      "Epoch 26, batch 81/93, loss: 0.3580\n",
      "Epoch 26, batch 86/93, loss: 0.4226\n",
      "Epoch 26, batch 91/93, loss: 0.3292\n",
      "Epoch 26, train loss: 0.3052, train accuracy: 89.76%\n",
      "Epoch 26, val loss: 0.5538, val accuracy: 83.46%\n",
      "Epoch 27, batch 1/93, loss: 0.1604\n",
      "Epoch 27, batch 6/93, loss: 0.3556\n",
      "Epoch 27, batch 11/93, loss: 0.3686\n",
      "Epoch 27, batch 16/93, loss: 0.1802\n",
      "Epoch 27, batch 21/93, loss: 0.1668\n",
      "Epoch 27, batch 26/93, loss: 0.3070\n",
      "Epoch 27, batch 31/93, loss: 0.3365\n",
      "Epoch 27, batch 36/93, loss: 0.2388\n",
      "Epoch 27, batch 41/93, loss: 0.4302\n",
      "Epoch 27, batch 46/93, loss: 0.2998\n",
      "Epoch 27, batch 51/93, loss: 0.2321\n",
      "Epoch 27, batch 56/93, loss: 0.3614\n",
      "Epoch 27, batch 61/93, loss: 0.2402\n",
      "Epoch 27, batch 66/93, loss: 0.2434\n",
      "Epoch 27, batch 71/93, loss: 0.2732\n",
      "Epoch 27, batch 76/93, loss: 0.3078\n",
      "Epoch 27, batch 81/93, loss: 0.3170\n",
      "Epoch 27, batch 86/93, loss: 0.3889\n",
      "Epoch 27, batch 91/93, loss: 0.2975\n",
      "Epoch 27, train loss: 0.2823, train accuracy: 90.74%\n",
      "Epoch 27, val loss: 0.5416, val accuracy: 84.07%\n",
      "Epoch 28, batch 1/93, loss: 0.1706\n",
      "Epoch 28, batch 6/93, loss: 0.3473\n",
      "Epoch 28, batch 11/93, loss: 0.3425\n",
      "Epoch 28, batch 16/93, loss: 0.2196\n",
      "Epoch 28, batch 21/93, loss: 0.3876\n",
      "Epoch 28, batch 26/93, loss: 0.2153\n",
      "Epoch 28, batch 31/93, loss: 0.2151\n",
      "Epoch 28, batch 36/93, loss: 0.2907\n",
      "Epoch 28, batch 41/93, loss: 0.2227\n",
      "Epoch 28, batch 46/93, loss: 0.1674\n",
      "Epoch 28, batch 51/93, loss: 0.1793\n",
      "Epoch 28, batch 56/93, loss: 0.1412\n",
      "Epoch 28, batch 61/93, loss: 0.2405\n",
      "Epoch 28, batch 66/93, loss: 0.1543\n",
      "Epoch 28, batch 71/93, loss: 0.4542\n",
      "Epoch 28, batch 76/93, loss: 0.2228\n",
      "Epoch 28, batch 81/93, loss: 0.3539\n",
      "Epoch 28, batch 86/93, loss: 0.1799\n",
      "Epoch 28, batch 91/93, loss: 0.1960\n",
      "Epoch 28, train loss: 0.2661, train accuracy: 91.09%\n",
      "Epoch 28, val loss: 0.5524, val accuracy: 84.07%\n",
      "Epoch 29, batch 1/93, loss: 0.2847\n",
      "Epoch 29, batch 6/93, loss: 0.2734\n",
      "Epoch 29, batch 11/93, loss: 0.2046\n",
      "Epoch 29, batch 16/93, loss: 0.2189\n",
      "Epoch 29, batch 21/93, loss: 0.2802\n",
      "Epoch 29, batch 26/93, loss: 0.1314\n",
      "Epoch 29, batch 31/93, loss: 0.2509\n",
      "Epoch 29, batch 36/93, loss: 0.2681\n",
      "Epoch 29, batch 41/93, loss: 0.1966\n",
      "Epoch 29, batch 46/93, loss: 0.2607\n",
      "Epoch 29, batch 51/93, loss: 0.2002\n",
      "Epoch 29, batch 56/93, loss: 0.2699\n",
      "Epoch 29, batch 61/93, loss: 0.2300\n",
      "Epoch 29, batch 66/93, loss: 0.1744\n",
      "Epoch 29, batch 71/93, loss: 0.2907\n",
      "Epoch 29, batch 76/93, loss: 0.2698\n",
      "Epoch 29, batch 81/93, loss: 0.1963\n",
      "Epoch 29, batch 86/93, loss: 0.1317\n",
      "Epoch 29, batch 91/93, loss: 0.2792\n",
      "Epoch 29, train loss: 0.2428, train accuracy: 91.86%\n",
      "Epoch 29, val loss: 0.6385, val accuracy: 81.60%\n",
      "Epoch 30, batch 1/93, loss: 0.3093\n",
      "Epoch 30, batch 6/93, loss: 0.2649\n",
      "Epoch 30, batch 11/93, loss: 0.2406\n",
      "Epoch 30, batch 16/93, loss: 0.2020\n",
      "Epoch 30, batch 21/93, loss: 0.3254\n",
      "Epoch 30, batch 26/93, loss: 0.3276\n",
      "Epoch 30, batch 31/93, loss: 0.2611\n",
      "Epoch 30, batch 36/93, loss: 0.2966\n",
      "Epoch 30, batch 41/93, loss: 0.2766\n",
      "Epoch 30, batch 46/93, loss: 0.2200\n",
      "Epoch 30, batch 51/93, loss: 0.3199\n",
      "Epoch 30, batch 56/93, loss: 0.1687\n",
      "Epoch 30, batch 61/93, loss: 0.2603\n",
      "Epoch 30, batch 66/93, loss: 0.2063\n",
      "Epoch 30, batch 71/93, loss: 0.2308\n",
      "Epoch 30, batch 76/93, loss: 0.3058\n",
      "Epoch 30, batch 81/93, loss: 0.2115\n",
      "Epoch 30, batch 86/93, loss: 0.1690\n",
      "Epoch 30, batch 91/93, loss: 0.2515\n",
      "Epoch 30, train loss: 0.2492, train accuracy: 91.44%\n",
      "Epoch 30, val loss: 0.5361, val accuracy: 84.24%\n",
      "Epoch 31, batch 1/93, loss: 0.1393\n",
      "Epoch 31, batch 6/93, loss: 0.2272\n",
      "Epoch 31, batch 11/93, loss: 0.2293\n",
      "Epoch 31, batch 16/93, loss: 0.2883\n",
      "Epoch 31, batch 21/93, loss: 0.2612\n",
      "Epoch 31, batch 26/93, loss: 0.4302\n",
      "Epoch 31, batch 31/93, loss: 0.2109\n",
      "Epoch 31, batch 36/93, loss: 0.1893\n",
      "Epoch 31, batch 41/93, loss: 0.2347\n",
      "Epoch 31, batch 46/93, loss: 0.2428\n",
      "Epoch 31, batch 51/93, loss: 0.2235\n",
      "Epoch 31, batch 56/93, loss: 0.2090\n",
      "Epoch 31, batch 61/93, loss: 0.2141\n",
      "Epoch 31, batch 66/93, loss: 0.1624\n",
      "Epoch 31, batch 71/93, loss: 0.3080\n",
      "Epoch 31, batch 76/93, loss: 0.2179\n",
      "Epoch 31, batch 81/93, loss: 0.1801\n",
      "Epoch 31, batch 86/93, loss: 0.2949\n",
      "Epoch 31, batch 91/93, loss: 0.2495\n",
      "Epoch 31, train loss: 0.2460, train accuracy: 91.68%\n",
      "Epoch 31, val loss: 0.5560, val accuracy: 83.38%\n",
      "Epoch 32, batch 1/93, loss: 0.2110\n",
      "Epoch 32, batch 6/93, loss: 0.2502\n",
      "Epoch 32, batch 11/93, loss: 0.3631\n",
      "Epoch 32, batch 16/93, loss: 0.2359\n",
      "Epoch 32, batch 21/93, loss: 0.3452\n",
      "Epoch 32, batch 26/93, loss: 0.2093\n",
      "Epoch 32, batch 31/93, loss: 0.2345\n",
      "Epoch 32, batch 36/93, loss: 0.3292\n",
      "Epoch 32, batch 41/93, loss: 0.2847\n",
      "Epoch 32, batch 46/93, loss: 0.2718\n",
      "Epoch 32, batch 51/93, loss: 0.2284\n",
      "Epoch 32, batch 56/93, loss: 0.3455\n",
      "Epoch 32, batch 61/93, loss: 0.3108\n",
      "Epoch 32, batch 66/93, loss: 0.1725\n",
      "Epoch 32, batch 71/93, loss: 0.2628\n",
      "Epoch 32, batch 76/93, loss: 0.1730\n",
      "Epoch 32, batch 81/93, loss: 0.3118\n",
      "Epoch 32, batch 86/93, loss: 0.2261\n",
      "Epoch 32, batch 91/93, loss: 0.3207\n",
      "Epoch 32, train loss: 0.2591, train accuracy: 91.49%\n",
      "Epoch 32, val loss: 0.5205, val accuracy: 84.11%\n",
      "Epoch 33, batch 1/93, loss: 0.1172\n",
      "Epoch 33, batch 6/93, loss: 0.2022\n",
      "Epoch 33, batch 11/93, loss: 0.1753\n",
      "Epoch 33, batch 16/93, loss: 0.2752\n",
      "Epoch 33, batch 21/93, loss: 0.3574\n",
      "Epoch 33, batch 26/93, loss: 0.1675\n",
      "Epoch 33, batch 31/93, loss: 0.2632\n",
      "Epoch 33, batch 36/93, loss: 0.2060\n",
      "Epoch 33, batch 41/93, loss: 0.3290\n",
      "Epoch 33, batch 46/93, loss: 0.2770\n",
      "Epoch 33, batch 51/93, loss: 0.2075\n",
      "Epoch 33, batch 56/93, loss: 0.3383\n",
      "Epoch 33, batch 61/93, loss: 0.2867\n",
      "Epoch 33, batch 66/93, loss: 0.1706\n",
      "Epoch 33, batch 71/93, loss: 0.1948\n",
      "Epoch 33, batch 76/93, loss: 0.2330\n",
      "Epoch 33, batch 81/93, loss: 0.2693\n",
      "Epoch 33, batch 86/93, loss: 0.2661\n",
      "Epoch 33, batch 91/93, loss: 0.2789\n",
      "Epoch 33, train loss: 0.2343, train accuracy: 92.31%\n",
      "Epoch 33, val loss: 0.5755, val accuracy: 84.33%\n",
      "Epoch 34, batch 1/93, loss: 0.1273\n",
      "Epoch 34, batch 6/93, loss: 0.3091\n",
      "Epoch 34, batch 11/93, loss: 0.2714\n",
      "Epoch 34, batch 16/93, loss: 0.2861\n",
      "Epoch 34, batch 21/93, loss: 0.2375\n",
      "Epoch 34, batch 26/93, loss: 0.3106\n",
      "Epoch 34, batch 31/93, loss: 0.4103\n",
      "Epoch 34, batch 36/93, loss: 0.3055\n",
      "Epoch 34, batch 41/93, loss: 0.2671\n",
      "Epoch 34, batch 46/93, loss: 0.1294\n",
      "Epoch 34, batch 51/93, loss: 0.3096\n",
      "Epoch 34, batch 56/93, loss: 0.3655\n",
      "Epoch 34, batch 61/93, loss: 0.1742\n",
      "Epoch 34, batch 66/93, loss: 0.2784\n",
      "Epoch 34, batch 71/93, loss: 0.1636\n",
      "Epoch 34, batch 76/93, loss: 0.3018\n",
      "Epoch 34, batch 81/93, loss: 0.2194\n",
      "Epoch 34, batch 86/93, loss: 0.2665\n",
      "Epoch 34, batch 91/93, loss: 0.2991\n",
      "Epoch 34, train loss: 0.2666, train accuracy: 90.88%\n",
      "Epoch 34, val loss: 0.5840, val accuracy: 82.16%\n",
      "Epoch 35, batch 1/93, loss: 0.2745\n",
      "Epoch 35, batch 6/93, loss: 0.2235\n",
      "Epoch 35, batch 11/93, loss: 0.3055\n",
      "Epoch 35, batch 16/93, loss: 0.2397\n",
      "Epoch 35, batch 21/93, loss: 0.2756\n",
      "Epoch 35, batch 26/93, loss: 0.2816\n",
      "Epoch 35, batch 31/93, loss: 0.1966\n",
      "Epoch 35, batch 36/93, loss: 0.2457\n",
      "Epoch 35, batch 41/93, loss: 0.2174\n",
      "Epoch 35, batch 46/93, loss: 0.1965\n",
      "Epoch 35, batch 51/93, loss: 0.1731\n",
      "Epoch 35, batch 56/93, loss: 0.2783\n",
      "Epoch 35, batch 61/93, loss: 0.1935\n",
      "Epoch 35, batch 66/93, loss: 0.1994\n",
      "Epoch 35, batch 71/93, loss: 0.1668\n",
      "Epoch 35, batch 76/93, loss: 0.1646\n",
      "Epoch 35, batch 81/93, loss: 0.1714\n",
      "Epoch 35, batch 86/93, loss: 0.2317\n",
      "Epoch 35, batch 91/93, loss: 0.2463\n",
      "Epoch 35, train loss: 0.2425, train accuracy: 92.01%\n",
      "Epoch 35, val loss: 0.5745, val accuracy: 84.11%\n",
      "Epoch 36, batch 1/93, loss: 0.1166\n",
      "Epoch 36, batch 6/93, loss: 0.1472\n",
      "Epoch 36, batch 11/93, loss: 0.3118\n",
      "Epoch 36, batch 16/93, loss: 0.1595\n",
      "Epoch 36, batch 21/93, loss: 0.2866\n",
      "Epoch 36, batch 26/93, loss: 0.1506\n",
      "Epoch 36, batch 31/93, loss: 0.2122\n",
      "Epoch 36, batch 36/93, loss: 0.1874\n",
      "Epoch 36, batch 41/93, loss: 0.1748\n",
      "Epoch 36, batch 46/93, loss: 0.3107\n",
      "Epoch 36, batch 51/93, loss: 0.1977\n",
      "Epoch 36, batch 56/93, loss: 0.1986\n",
      "Epoch 36, batch 61/93, loss: 0.2374\n",
      "Epoch 36, batch 66/93, loss: 0.3114\n",
      "Epoch 36, batch 71/93, loss: 0.2070\n",
      "Epoch 36, batch 76/93, loss: 0.1639\n",
      "Epoch 36, batch 81/93, loss: 0.1593\n",
      "Epoch 36, batch 86/93, loss: 0.1530\n",
      "Epoch 36, batch 91/93, loss: 0.2725\n",
      "Epoch 36, train loss: 0.2132, train accuracy: 93.14%\n",
      "Epoch 36, val loss: 0.6127, val accuracy: 83.07%\n",
      "Epoch 37, batch 1/93, loss: 0.2139\n",
      "Epoch 37, batch 6/93, loss: 0.1538\n",
      "Epoch 37, batch 11/93, loss: 0.3051\n",
      "Epoch 37, batch 16/93, loss: 0.2289\n",
      "Epoch 37, batch 21/93, loss: 0.2105\n",
      "Epoch 37, batch 26/93, loss: 0.2910\n",
      "Epoch 37, batch 31/93, loss: 0.2333\n",
      "Epoch 37, batch 36/93, loss: 0.1281\n",
      "Epoch 37, batch 41/93, loss: 0.1467\n",
      "Epoch 37, batch 46/93, loss: 0.2397\n",
      "Epoch 37, batch 51/93, loss: 0.2533\n",
      "Epoch 37, batch 56/93, loss: 0.1590\n",
      "Epoch 37, batch 61/93, loss: 0.1440\n",
      "Epoch 37, batch 66/93, loss: 0.1677\n",
      "Epoch 37, batch 71/93, loss: 0.1276\n",
      "Epoch 37, batch 76/93, loss: 0.1843\n",
      "Epoch 37, batch 81/93, loss: 0.1991\n",
      "Epoch 37, batch 86/93, loss: 0.1396\n",
      "Epoch 37, batch 91/93, loss: 0.1326\n",
      "Epoch 37, train loss: 0.2168, train accuracy: 93.14%\n",
      "Epoch 37, val loss: 0.5730, val accuracy: 84.38%\n",
      "Epoch 38, batch 1/93, loss: 0.1062\n",
      "Epoch 38, batch 6/93, loss: 0.1732\n",
      "Epoch 38, batch 11/93, loss: 0.2099\n",
      "Epoch 38, batch 16/93, loss: 0.2538\n",
      "Epoch 38, batch 21/93, loss: 0.1436\n",
      "Epoch 38, batch 26/93, loss: 0.1278\n",
      "Epoch 38, batch 31/93, loss: 0.1104\n",
      "Epoch 38, batch 36/93, loss: 0.2117\n",
      "Epoch 38, batch 41/93, loss: 0.1393\n",
      "Epoch 38, batch 46/93, loss: 0.2655\n",
      "Epoch 38, batch 51/93, loss: 0.1431\n",
      "Epoch 38, batch 56/93, loss: 0.1007\n",
      "Epoch 38, batch 61/93, loss: 0.1672\n",
      "Epoch 38, batch 66/93, loss: 0.2659\n",
      "Epoch 38, batch 71/93, loss: 0.1873\n",
      "Epoch 38, batch 76/93, loss: 0.1774\n",
      "Epoch 38, batch 81/93, loss: 0.1464\n",
      "Epoch 38, batch 86/93, loss: 0.2404\n",
      "Epoch 38, batch 91/93, loss: 0.1710\n",
      "Epoch 38, train loss: 0.2065, train accuracy: 92.95%\n",
      "Epoch 38, val loss: 0.5202, val accuracy: 84.77%\n",
      "Epoch 39, batch 1/93, loss: 0.1758\n",
      "Epoch 39, batch 6/93, loss: 0.1790\n",
      "Epoch 39, batch 11/93, loss: 0.2013\n",
      "Epoch 39, batch 16/93, loss: 0.2160\n",
      "Epoch 39, batch 21/93, loss: 0.1043\n",
      "Epoch 39, batch 26/93, loss: 0.1158\n",
      "Epoch 39, batch 31/93, loss: 0.1153\n",
      "Epoch 39, batch 36/93, loss: 0.2452\n",
      "Epoch 39, batch 41/93, loss: 0.1537\n",
      "Epoch 39, batch 46/93, loss: 0.1560\n",
      "Epoch 39, batch 51/93, loss: 0.1991\n",
      "Epoch 39, batch 56/93, loss: 0.1446\n",
      "Epoch 39, batch 61/93, loss: 0.1144\n",
      "Epoch 39, batch 66/93, loss: 0.1165\n",
      "Epoch 39, batch 71/93, loss: 0.1837\n",
      "Epoch 39, batch 76/93, loss: 0.1609\n",
      "Epoch 39, batch 81/93, loss: 0.1444\n",
      "Epoch 39, batch 86/93, loss: 0.2951\n",
      "Epoch 39, batch 91/93, loss: 0.2646\n",
      "Epoch 39, train loss: 0.1805, train accuracy: 94.40%\n",
      "Epoch 39, val loss: 0.5427, val accuracy: 84.03%\n",
      "Epoch 40, batch 1/93, loss: 0.1400\n",
      "Epoch 40, batch 6/93, loss: 0.1995\n",
      "Epoch 40, batch 11/93, loss: 0.1860\n",
      "Epoch 40, batch 16/93, loss: 0.2826\n",
      "Epoch 40, batch 21/93, loss: 0.1340\n",
      "Epoch 40, batch 26/93, loss: 0.2063\n",
      "Epoch 40, batch 31/93, loss: 0.1490\n",
      "Epoch 40, batch 36/93, loss: 0.1620\n",
      "Epoch 40, batch 41/93, loss: 0.2095\n",
      "Epoch 40, batch 46/93, loss: 0.2206\n",
      "Epoch 40, batch 51/93, loss: 0.1150\n",
      "Epoch 40, batch 56/93, loss: 0.1924\n",
      "Epoch 40, batch 61/93, loss: 0.1929\n",
      "Epoch 40, batch 66/93, loss: 0.2751\n",
      "Epoch 40, batch 71/93, loss: 0.1395\n",
      "Epoch 40, batch 76/93, loss: 0.2878\n",
      "Epoch 40, batch 81/93, loss: 0.1987\n",
      "Epoch 40, batch 86/93, loss: 0.1497\n",
      "Epoch 40, batch 91/93, loss: 0.1836\n",
      "Epoch 40, train loss: 0.1949, train accuracy: 93.65%\n",
      "Epoch 40, val loss: 0.5500, val accuracy: 84.64%\n",
      "Epoch 41, batch 1/93, loss: 0.2003\n",
      "Epoch 41, batch 6/93, loss: 0.2432\n",
      "Epoch 41, batch 11/93, loss: 0.1812\n",
      "Epoch 41, batch 16/93, loss: 0.2837\n",
      "Epoch 41, batch 21/93, loss: 0.1831\n",
      "Epoch 41, batch 26/93, loss: 0.1236\n",
      "Epoch 41, batch 31/93, loss: 0.2338\n",
      "Epoch 41, batch 36/93, loss: 0.1882\n",
      "Epoch 41, batch 41/93, loss: 0.1626\n",
      "Epoch 41, batch 46/93, loss: 0.1570\n",
      "Epoch 41, batch 51/93, loss: 0.1852\n",
      "Epoch 41, batch 56/93, loss: 0.1858\n",
      "Epoch 41, batch 61/93, loss: 0.2193\n",
      "Epoch 41, batch 66/93, loss: 0.1368\n",
      "Epoch 41, batch 71/93, loss: 0.1362\n",
      "Epoch 41, batch 76/93, loss: 0.2193\n",
      "Epoch 41, batch 81/93, loss: 0.1495\n",
      "Epoch 41, batch 86/93, loss: 0.1823\n",
      "Epoch 41, batch 91/93, loss: 0.1531\n",
      "Epoch 41, train loss: 0.1762, train accuracy: 94.35%\n",
      "Epoch 41, val loss: 0.5741, val accuracy: 84.20%\n",
      "Epoch 42, batch 1/93, loss: 0.1639\n",
      "Epoch 42, batch 6/93, loss: 0.2484\n",
      "Epoch 42, batch 11/93, loss: 0.2057\n",
      "Epoch 42, batch 16/93, loss: 0.1914\n",
      "Epoch 42, batch 21/93, loss: 0.1729\n",
      "Epoch 42, batch 26/93, loss: 0.1873\n",
      "Epoch 42, batch 31/93, loss: 0.1138\n",
      "Epoch 42, batch 36/93, loss: 0.1948\n",
      "Epoch 42, batch 41/93, loss: 0.2127\n",
      "Epoch 42, batch 46/93, loss: 0.2679\n",
      "Epoch 42, batch 51/93, loss: 0.1059\n",
      "Epoch 42, batch 56/93, loss: 0.1511\n",
      "Epoch 42, batch 61/93, loss: 0.1704\n",
      "Epoch 42, batch 66/93, loss: 0.1681\n",
      "Epoch 42, batch 71/93, loss: 0.1229\n",
      "Epoch 42, batch 76/93, loss: 0.2254\n",
      "Epoch 42, batch 81/93, loss: 0.1441\n",
      "Epoch 42, batch 86/93, loss: 0.0923\n",
      "Epoch 42, batch 91/93, loss: 0.2703\n",
      "Epoch 42, train loss: 0.1719, train accuracy: 94.62%\n",
      "Epoch 42, val loss: 0.5575, val accuracy: 84.42%\n",
      "Epoch 43, batch 1/93, loss: 0.1573\n",
      "Epoch 43, batch 6/93, loss: 0.1039\n",
      "Epoch 43, batch 11/93, loss: 0.0947\n",
      "Epoch 43, batch 16/93, loss: 0.1776\n",
      "Epoch 43, batch 21/93, loss: 0.1260\n",
      "Epoch 43, batch 26/93, loss: 0.1266\n",
      "Epoch 43, batch 31/93, loss: 0.0750\n",
      "Epoch 43, batch 36/93, loss: 0.1447\n",
      "Epoch 43, batch 41/93, loss: 0.1547\n",
      "Epoch 43, batch 46/93, loss: 0.1131\n",
      "Epoch 43, batch 51/93, loss: 0.1039\n",
      "Epoch 43, batch 56/93, loss: 0.1263\n",
      "Epoch 43, batch 61/93, loss: 0.1372\n",
      "Epoch 43, batch 66/93, loss: 0.0971\n",
      "Epoch 43, batch 71/93, loss: 0.1837\n",
      "Epoch 43, batch 76/93, loss: 0.1167\n",
      "Epoch 43, batch 81/93, loss: 0.1392\n",
      "Epoch 43, batch 86/93, loss: 0.2536\n",
      "Epoch 43, batch 91/93, loss: 0.1915\n",
      "Epoch 43, train loss: 0.1512, train accuracy: 95.50%\n",
      "Epoch 43, val loss: 0.5505, val accuracy: 84.42%\n",
      "Epoch 44, batch 1/93, loss: 0.2086\n",
      "Epoch 44, batch 6/93, loss: 0.2642\n",
      "Epoch 44, batch 11/93, loss: 0.1511\n",
      "Epoch 44, batch 16/93, loss: 0.1404\n",
      "Epoch 44, batch 21/93, loss: 0.2100\n",
      "Epoch 44, batch 26/93, loss: 0.1102\n",
      "Epoch 44, batch 31/93, loss: 0.1253\n",
      "Epoch 44, batch 36/93, loss: 0.1372\n",
      "Epoch 44, batch 41/93, loss: 0.1181\n",
      "Epoch 44, batch 46/93, loss: 0.1943\n",
      "Epoch 44, batch 51/93, loss: 0.1474\n",
      "Epoch 44, batch 56/93, loss: 0.1006\n",
      "Epoch 44, batch 61/93, loss: 0.0743\n",
      "Epoch 44, batch 66/93, loss: 0.0715\n",
      "Epoch 44, batch 71/93, loss: 0.0996\n",
      "Epoch 44, batch 76/93, loss: 0.1143\n",
      "Epoch 44, batch 81/93, loss: 0.2080\n",
      "Epoch 44, batch 86/93, loss: 0.2313\n",
      "Epoch 44, batch 91/93, loss: 0.1719\n",
      "Epoch 44, train loss: 0.1508, train accuracy: 95.16%\n",
      "Epoch 44, val loss: 0.5215, val accuracy: 84.29%\n",
      "Epoch 45, batch 1/93, loss: 0.1165\n",
      "Epoch 45, batch 6/93, loss: 0.3742\n",
      "Epoch 45, batch 11/93, loss: 0.1679\n",
      "Epoch 45, batch 16/93, loss: 0.1074\n",
      "Epoch 45, batch 21/93, loss: 0.1469\n",
      "Epoch 45, batch 26/93, loss: 0.1806\n",
      "Epoch 45, batch 31/93, loss: 0.0766\n",
      "Epoch 45, batch 36/93, loss: 0.1251\n",
      "Epoch 45, batch 41/93, loss: 0.1639\n",
      "Epoch 45, batch 46/93, loss: 0.1740\n",
      "Epoch 45, batch 51/93, loss: 0.1365\n",
      "Epoch 45, batch 56/93, loss: 0.1656\n",
      "Epoch 45, batch 61/93, loss: 0.2907\n",
      "Epoch 45, batch 66/93, loss: 0.0916\n",
      "Epoch 45, batch 71/93, loss: 0.2080\n",
      "Epoch 45, batch 76/93, loss: 0.1055\n",
      "Epoch 45, batch 81/93, loss: 0.1576\n",
      "Epoch 45, batch 86/93, loss: 0.1715\n",
      "Epoch 45, batch 91/93, loss: 0.1764\n",
      "Epoch 45, train loss: 0.1663, train accuracy: 94.82%\n",
      "Epoch 45, val loss: 0.5412, val accuracy: 85.03%\n",
      "Epoch 46, batch 1/93, loss: 0.1089\n",
      "Epoch 46, batch 6/93, loss: 0.1166\n",
      "Epoch 46, batch 11/93, loss: 0.0990\n",
      "Epoch 46, batch 16/93, loss: 0.0834\n",
      "Epoch 46, batch 21/93, loss: 0.3061\n",
      "Epoch 46, batch 26/93, loss: 0.0770\n",
      "Epoch 46, batch 31/93, loss: 0.1746\n",
      "Epoch 46, batch 36/93, loss: 0.1046\n",
      "Epoch 46, batch 41/93, loss: 0.1618\n",
      "Epoch 46, batch 46/93, loss: 0.2056\n",
      "Epoch 46, batch 51/93, loss: 0.0975\n",
      "Epoch 46, batch 56/93, loss: 0.1848\n",
      "Epoch 46, batch 61/93, loss: 0.1573\n",
      "Epoch 46, batch 66/93, loss: 0.1449\n",
      "Epoch 46, batch 71/93, loss: 0.0722\n",
      "Epoch 46, batch 76/93, loss: 0.1358\n",
      "Epoch 46, batch 81/93, loss: 0.1318\n",
      "Epoch 46, batch 86/93, loss: 0.1186\n",
      "Epoch 46, batch 91/93, loss: 0.0774\n",
      "Epoch 46, train loss: 0.1286, train accuracy: 96.29%\n",
      "Epoch 46, val loss: 0.5477, val accuracy: 85.24%\n",
      "Epoch 47, batch 1/93, loss: 0.0866\n",
      "Epoch 47, batch 6/93, loss: 0.2268\n",
      "Epoch 47, batch 11/93, loss: 0.2256\n",
      "Epoch 47, batch 16/93, loss: 0.2167\n",
      "Epoch 47, batch 21/93, loss: 0.1220\n",
      "Epoch 47, batch 26/93, loss: 0.1959\n",
      "Epoch 47, batch 31/93, loss: 0.1308\n",
      "Epoch 47, batch 36/93, loss: 0.1918\n",
      "Epoch 47, batch 41/93, loss: 0.1350\n",
      "Epoch 47, batch 46/93, loss: 0.1431\n",
      "Epoch 47, batch 51/93, loss: 0.0970\n",
      "Epoch 47, batch 56/93, loss: 0.1171\n",
      "Epoch 47, batch 61/93, loss: 0.2334\n",
      "Epoch 47, batch 66/93, loss: 0.1127\n",
      "Epoch 47, batch 71/93, loss: 0.1636\n",
      "Epoch 47, batch 76/93, loss: 0.0718\n",
      "Epoch 47, batch 81/93, loss: 0.1155\n",
      "Epoch 47, batch 86/93, loss: 0.0755\n",
      "Epoch 47, batch 91/93, loss: 0.1031\n",
      "Epoch 47, train loss: 0.1466, train accuracy: 95.02%\n",
      "Epoch 47, val loss: 0.5937, val accuracy: 85.11%\n",
      "Epoch 48, batch 1/93, loss: 0.0818\n",
      "Epoch 48, batch 6/93, loss: 0.1307\n",
      "Epoch 48, batch 11/93, loss: 0.1259\n",
      "Epoch 48, batch 16/93, loss: 0.0944\n",
      "Epoch 48, batch 21/93, loss: 0.1806\n",
      "Epoch 48, batch 26/93, loss: 0.0571\n",
      "Epoch 48, batch 31/93, loss: 0.1429\n",
      "Epoch 48, batch 36/93, loss: 0.1020\n",
      "Epoch 48, batch 41/93, loss: 0.1267\n",
      "Epoch 48, batch 46/93, loss: 0.1129\n",
      "Epoch 48, batch 51/93, loss: 0.1110\n",
      "Epoch 48, batch 56/93, loss: 0.1486\n",
      "Epoch 48, batch 61/93, loss: 0.1891\n",
      "Epoch 48, batch 66/93, loss: 0.0843\n",
      "Epoch 48, batch 71/93, loss: 0.1085\n",
      "Epoch 48, batch 76/93, loss: 0.0960\n",
      "Epoch 48, batch 81/93, loss: 0.1446\n",
      "Epoch 48, batch 86/93, loss: 0.0778\n",
      "Epoch 48, batch 91/93, loss: 0.0815\n",
      "Epoch 48, train loss: 0.1190, train accuracy: 96.18%\n",
      "Epoch 48, val loss: 0.5554, val accuracy: 84.81%\n",
      "Epoch 49, batch 1/93, loss: 0.1380\n",
      "Epoch 49, batch 6/93, loss: 0.1304\n",
      "Epoch 49, batch 11/93, loss: 0.0875\n",
      "Epoch 49, batch 16/93, loss: 0.0948\n",
      "Epoch 49, batch 21/93, loss: 0.0493\n",
      "Epoch 49, batch 26/93, loss: 0.0843\n",
      "Epoch 49, batch 31/93, loss: 0.0968\n",
      "Epoch 49, batch 36/93, loss: 0.1052\n",
      "Epoch 49, batch 41/93, loss: 0.0563\n",
      "Epoch 49, batch 46/93, loss: 0.1145\n",
      "Epoch 49, batch 51/93, loss: 0.1492\n",
      "Epoch 49, batch 56/93, loss: 0.1711\n",
      "Epoch 49, batch 61/93, loss: 0.1415\n",
      "Epoch 49, batch 66/93, loss: 0.1395\n",
      "Epoch 49, batch 71/93, loss: 0.0976\n",
      "Epoch 49, batch 76/93, loss: 0.1600\n",
      "Epoch 49, batch 81/93, loss: 0.0763\n",
      "Epoch 49, batch 86/93, loss: 0.1652\n",
      "Epoch 49, batch 91/93, loss: 0.0852\n",
      "Epoch 49, train loss: 0.1256, train accuracy: 96.40%\n",
      "Epoch 49, val loss: 0.6132, val accuracy: 84.33%\n",
      "Epoch 50, batch 1/93, loss: 0.1390\n",
      "Epoch 50, batch 6/93, loss: 0.1295\n",
      "Epoch 50, batch 11/93, loss: 0.0752\n",
      "Epoch 50, batch 16/93, loss: 0.1915\n",
      "Epoch 50, batch 21/93, loss: 0.0762\n",
      "Epoch 50, batch 26/93, loss: 0.2339\n",
      "Epoch 50, batch 31/93, loss: 0.1515\n",
      "Epoch 50, batch 36/93, loss: 0.1867\n",
      "Epoch 50, batch 41/93, loss: 0.1647\n",
      "Epoch 50, batch 46/93, loss: 0.1013\n",
      "Epoch 50, batch 51/93, loss: 0.1130\n",
      "Epoch 50, batch 56/93, loss: 0.1047\n",
      "Epoch 50, batch 61/93, loss: 0.0893\n",
      "Epoch 50, batch 66/93, loss: 0.1158\n",
      "Epoch 50, batch 71/93, loss: 0.0771\n",
      "Epoch 50, batch 76/93, loss: 0.0940\n",
      "Epoch 50, batch 81/93, loss: 0.1456\n",
      "Epoch 50, batch 86/93, loss: 0.1492\n",
      "Epoch 50, batch 91/93, loss: 0.0968\n",
      "Epoch 50, train loss: 0.1300, train accuracy: 95.68%\n",
      "Epoch 50, val loss: 0.6237, val accuracy: 84.85%\n",
      "Epoch 51, batch 1/93, loss: 0.1486\n",
      "Epoch 51, batch 6/93, loss: 0.0640\n",
      "Epoch 51, batch 11/93, loss: 0.1214\n",
      "Epoch 51, batch 16/93, loss: 0.1460\n",
      "Epoch 51, batch 21/93, loss: 0.1736\n",
      "Epoch 51, batch 26/93, loss: 0.1320\n",
      "Epoch 51, batch 31/93, loss: 0.1408\n",
      "Epoch 51, batch 36/93, loss: 0.1535\n",
      "Epoch 51, batch 41/93, loss: 0.0381\n",
      "Epoch 51, batch 46/93, loss: 0.1306\n",
      "Epoch 51, batch 51/93, loss: 0.0820\n",
      "Epoch 51, batch 56/93, loss: 0.1258\n",
      "Epoch 51, batch 61/93, loss: 0.1234\n",
      "Epoch 51, batch 66/93, loss: 0.1083\n",
      "Epoch 51, batch 71/93, loss: 0.1825\n",
      "Epoch 51, batch 76/93, loss: 0.0807\n",
      "Epoch 51, batch 81/93, loss: 0.0966\n",
      "Epoch 51, batch 86/93, loss: 0.0953\n",
      "Epoch 51, batch 91/93, loss: 0.1131\n",
      "Epoch 51, train loss: 0.1212, train accuracy: 96.22%\n",
      "Epoch 51, val loss: 0.6234, val accuracy: 84.33%\n",
      "Epoch 52, batch 1/93, loss: 0.0667\n",
      "Epoch 52, batch 6/93, loss: 0.0839\n",
      "Epoch 52, batch 11/93, loss: 0.1202\n",
      "Epoch 52, batch 16/93, loss: 0.1976\n",
      "Epoch 52, batch 21/93, loss: 0.2055\n",
      "Epoch 52, batch 26/93, loss: 0.1205\n",
      "Epoch 52, batch 31/93, loss: 0.1020\n",
      "Epoch 52, batch 36/93, loss: 0.0839\n",
      "Epoch 52, batch 41/93, loss: 0.2519\n",
      "Epoch 52, batch 46/93, loss: 0.1003\n",
      "Epoch 52, batch 51/93, loss: 0.0984\n",
      "Epoch 52, batch 56/93, loss: 0.0762\n",
      "Epoch 52, batch 61/93, loss: 0.0924\n",
      "Epoch 52, batch 66/93, loss: 0.0804\n",
      "Epoch 52, batch 71/93, loss: 0.1101\n",
      "Epoch 52, batch 76/93, loss: 0.1148\n",
      "Epoch 52, batch 81/93, loss: 0.0847\n",
      "Epoch 52, batch 86/93, loss: 0.0691\n",
      "Epoch 52, batch 91/93, loss: 0.0485\n",
      "Epoch 52, train loss: 0.1238, train accuracy: 96.26%\n",
      "Epoch 52, val loss: 0.5755, val accuracy: 85.72%\n",
      "Epoch 53, batch 1/93, loss: 0.2011\n",
      "Epoch 53, batch 6/93, loss: 0.1114\n",
      "Epoch 53, batch 11/93, loss: 0.1870\n",
      "Epoch 53, batch 16/93, loss: 0.1837\n",
      "Epoch 53, batch 21/93, loss: 0.0504\n",
      "Epoch 53, batch 26/93, loss: 0.0805\n",
      "Epoch 53, batch 31/93, loss: 0.4322\n",
      "Epoch 53, batch 36/93, loss: 0.0879\n",
      "Epoch 53, batch 41/93, loss: 0.0706\n",
      "Epoch 53, batch 46/93, loss: 0.0734\n",
      "Epoch 53, batch 51/93, loss: 0.0534\n",
      "Epoch 53, batch 56/93, loss: 0.1363\n",
      "Epoch 53, batch 61/93, loss: 0.1250\n",
      "Epoch 53, batch 66/93, loss: 0.1134\n",
      "Epoch 53, batch 71/93, loss: 0.1351\n",
      "Epoch 53, batch 76/93, loss: 0.0589\n",
      "Epoch 53, batch 81/93, loss: 0.1348\n",
      "Epoch 53, batch 86/93, loss: 0.0703\n",
      "Epoch 53, batch 91/93, loss: 0.0904\n",
      "Epoch 53, train loss: 0.1200, train accuracy: 96.23%\n",
      "Epoch 53, val loss: 0.6559, val accuracy: 84.29%\n",
      "Epoch 54, batch 1/93, loss: 0.0425\n",
      "Epoch 54, batch 6/93, loss: 0.1284\n",
      "Epoch 54, batch 11/93, loss: 0.2305\n",
      "Epoch 54, batch 16/93, loss: 0.0762\n",
      "Epoch 54, batch 21/93, loss: 0.0642\n",
      "Epoch 54, batch 26/93, loss: 0.0540\n",
      "Epoch 54, batch 31/93, loss: 0.0528\n",
      "Epoch 54, batch 36/93, loss: 0.1149\n",
      "Epoch 54, batch 41/93, loss: 0.2394\n",
      "Epoch 54, batch 46/93, loss: 0.1533\n",
      "Epoch 54, batch 51/93, loss: 0.0955\n",
      "Epoch 54, batch 56/93, loss: 0.0702\n",
      "Epoch 54, batch 61/93, loss: 0.1706\n",
      "Epoch 54, batch 66/93, loss: 0.0749\n",
      "Epoch 54, batch 71/93, loss: 0.1242\n",
      "Epoch 54, batch 76/93, loss: 0.1155\n",
      "Epoch 54, batch 81/93, loss: 0.0634\n",
      "Epoch 54, batch 86/93, loss: 0.1045\n",
      "Epoch 54, batch 91/93, loss: 0.1491\n",
      "Epoch 54, train loss: 0.1027, train accuracy: 96.86%\n",
      "Epoch 54, val loss: 0.5703, val accuracy: 84.68%\n",
      "Epoch 55, batch 1/93, loss: 0.0501\n",
      "Epoch 55, batch 6/93, loss: 0.1679\n",
      "Epoch 55, batch 11/93, loss: 0.0912\n",
      "Epoch 55, batch 16/93, loss: 0.1432\n",
      "Epoch 55, batch 21/93, loss: 0.0482\n",
      "Epoch 55, batch 26/93, loss: 0.1232\n",
      "Epoch 55, batch 31/93, loss: 0.1440\n",
      "Epoch 55, batch 36/93, loss: 0.0931\n",
      "Epoch 55, batch 41/93, loss: 0.0581\n",
      "Epoch 55, batch 46/93, loss: 0.0964\n",
      "Epoch 55, batch 51/93, loss: 0.1311\n",
      "Epoch 55, batch 56/93, loss: 0.0538\n",
      "Epoch 55, batch 61/93, loss: 0.0827\n",
      "Epoch 55, batch 66/93, loss: 0.1169\n",
      "Epoch 55, batch 71/93, loss: 0.0906\n",
      "Epoch 55, batch 76/93, loss: 0.1408\n",
      "Epoch 55, batch 81/93, loss: 0.1744\n",
      "Epoch 55, batch 86/93, loss: 0.0828\n",
      "Epoch 55, batch 91/93, loss: 0.0954\n",
      "Epoch 55, train loss: 0.1152, train accuracy: 96.56%\n",
      "Epoch 55, val loss: 0.6069, val accuracy: 84.72%\n",
      "Epoch 56, batch 1/93, loss: 0.1352\n",
      "Epoch 56, batch 6/93, loss: 0.2388\n",
      "Epoch 56, batch 11/93, loss: 0.1990\n",
      "Epoch 56, batch 16/93, loss: 0.2029\n",
      "Epoch 56, batch 21/93, loss: 0.1572\n",
      "Epoch 56, batch 26/93, loss: 0.0942\n",
      "Epoch 56, batch 31/93, loss: 0.2023\n",
      "Epoch 56, batch 36/93, loss: 0.1033\n",
      "Epoch 56, batch 41/93, loss: 0.0782\n",
      "Epoch 56, batch 46/93, loss: 0.0864\n",
      "Epoch 56, batch 51/93, loss: 0.1123\n",
      "Epoch 56, batch 56/93, loss: 0.1379\n",
      "Epoch 56, batch 61/93, loss: 0.0750\n",
      "Epoch 56, batch 66/93, loss: 0.1233\n",
      "Epoch 56, batch 71/93, loss: 0.1008\n",
      "Epoch 56, batch 76/93, loss: 0.0972\n",
      "Epoch 56, batch 81/93, loss: 0.0942\n",
      "Epoch 56, batch 86/93, loss: 0.1722\n",
      "Epoch 56, batch 91/93, loss: 0.0827\n",
      "Epoch 56, train loss: 0.1269, train accuracy: 95.84%\n",
      "Epoch 56, val loss: 0.6347, val accuracy: 84.11%\n",
      "Epoch 57, batch 1/93, loss: 0.0593\n",
      "Epoch 57, batch 6/93, loss: 0.1488\n",
      "Epoch 57, batch 11/93, loss: 0.1877\n",
      "Epoch 57, batch 16/93, loss: 0.1427\n",
      "Epoch 57, batch 21/93, loss: 0.1202\n",
      "Epoch 57, batch 26/93, loss: 0.1581\n",
      "Epoch 57, batch 31/93, loss: 0.1137\n",
      "Epoch 57, batch 36/93, loss: 0.1300\n",
      "Epoch 57, batch 41/93, loss: 0.0543\n",
      "Epoch 57, batch 46/93, loss: 0.1033\n",
      "Epoch 57, batch 51/93, loss: 0.0676\n",
      "Epoch 57, batch 56/93, loss: 0.1211\n",
      "Epoch 57, batch 61/93, loss: 0.0857\n",
      "Epoch 57, batch 66/93, loss: 0.0623\n",
      "Epoch 57, batch 71/93, loss: 0.0452\n",
      "Epoch 57, batch 76/93, loss: 0.1417\n",
      "Epoch 57, batch 81/93, loss: 0.0736\n",
      "Epoch 57, batch 86/93, loss: 0.0691\n",
      "Epoch 57, batch 91/93, loss: 0.1242\n",
      "Epoch 57, train loss: 0.1202, train accuracy: 96.43%\n",
      "Epoch 57, val loss: 0.5821, val accuracy: 85.81%\n",
      "Epoch 58, batch 1/93, loss: 0.0706\n",
      "Epoch 58, batch 6/93, loss: 0.1410\n",
      "Epoch 58, batch 11/93, loss: 0.2066\n",
      "Epoch 58, batch 16/93, loss: 0.2302\n",
      "Epoch 58, batch 21/93, loss: 0.2079\n",
      "Epoch 58, batch 26/93, loss: 0.0814\n",
      "Epoch 58, batch 31/93, loss: 0.0408\n",
      "Epoch 58, batch 36/93, loss: 0.1212\n",
      "Epoch 58, batch 41/93, loss: 0.0807\n",
      "Epoch 58, batch 46/93, loss: 0.1300\n",
      "Epoch 58, batch 51/93, loss: 0.1961\n",
      "Epoch 58, batch 56/93, loss: 0.1429\n",
      "Epoch 58, batch 61/93, loss: 0.1219\n",
      "Epoch 58, batch 66/93, loss: 0.0803\n",
      "Epoch 58, batch 71/93, loss: 0.1140\n",
      "Epoch 58, batch 76/93, loss: 0.0790\n",
      "Epoch 58, batch 81/93, loss: 0.0807\n",
      "Epoch 58, batch 86/93, loss: 0.0537\n",
      "Epoch 58, batch 91/93, loss: 0.0566\n",
      "Epoch 58, train loss: 0.1182, train accuracy: 96.60%\n",
      "Epoch 58, val loss: 0.6018, val accuracy: 84.77%\n",
      "Epoch 59, batch 1/93, loss: 0.1285\n",
      "Epoch 59, batch 6/93, loss: 0.3995\n",
      "Epoch 59, batch 11/93, loss: 0.2824\n",
      "Epoch 59, batch 16/93, loss: 0.2975\n",
      "Epoch 59, batch 21/93, loss: 0.1639\n",
      "Epoch 59, batch 26/93, loss: 0.2395\n",
      "Epoch 59, batch 31/93, loss: 0.2232\n",
      "Epoch 59, batch 36/93, loss: 0.3317\n",
      "Epoch 59, batch 41/93, loss: 0.1749\n",
      "Epoch 59, batch 46/93, loss: 0.0890\n",
      "Epoch 59, batch 51/93, loss: 0.1712\n",
      "Epoch 59, batch 56/93, loss: 0.1161\n",
      "Epoch 59, batch 61/93, loss: 0.1848\n",
      "Epoch 59, batch 66/93, loss: 0.0533\n",
      "Epoch 59, batch 71/93, loss: 0.1520\n",
      "Epoch 59, batch 76/93, loss: 0.1094\n",
      "Epoch 59, batch 81/93, loss: 0.0282\n",
      "Epoch 59, batch 86/93, loss: 0.0824\n",
      "Epoch 59, batch 91/93, loss: 0.1062\n",
      "Epoch 59, train loss: 0.1816, train accuracy: 93.76%\n",
      "Epoch 59, val loss: 0.6192, val accuracy: 84.68%\n",
      "Epoch 60, batch 1/93, loss: 0.1316\n",
      "Epoch 60, batch 6/93, loss: 0.1438\n",
      "Epoch 60, batch 11/93, loss: 0.1246\n",
      "Epoch 60, batch 16/93, loss: 0.1644\n",
      "Epoch 60, batch 21/93, loss: 0.1089\n",
      "Epoch 60, batch 26/93, loss: 0.0798\n",
      "Epoch 60, batch 31/93, loss: 0.0837\n",
      "Epoch 60, batch 36/93, loss: 0.0527\n",
      "Epoch 60, batch 41/93, loss: 0.1617\n",
      "Epoch 60, batch 46/93, loss: 0.1228\n",
      "Epoch 60, batch 51/93, loss: 0.0296\n",
      "Epoch 60, batch 56/93, loss: 0.1007\n",
      "Epoch 60, batch 61/93, loss: 0.1864\n",
      "Epoch 60, batch 66/93, loss: 0.0733\n",
      "Epoch 60, batch 71/93, loss: 0.0879\n",
      "Epoch 60, batch 76/93, loss: 0.1276\n",
      "Epoch 60, batch 81/93, loss: 0.2048\n",
      "Epoch 60, batch 86/93, loss: 0.0443\n",
      "Epoch 60, batch 91/93, loss: 0.1070\n",
      "Epoch 60, train loss: 0.1167, train accuracy: 96.47%\n",
      "Epoch 60, val loss: 0.6314, val accuracy: 84.29%\n",
      "Epoch 61, batch 1/93, loss: 0.0516\n",
      "Epoch 61, batch 6/93, loss: 0.1861\n",
      "Epoch 61, batch 11/93, loss: 0.0751\n",
      "Epoch 61, batch 16/93, loss: 0.1525\n",
      "Epoch 61, batch 21/93, loss: 0.2485\n",
      "Epoch 61, batch 26/93, loss: 0.1326\n",
      "Epoch 61, batch 31/93, loss: 0.1079\n",
      "Epoch 61, batch 36/93, loss: 0.0649\n",
      "Epoch 61, batch 41/93, loss: 0.0773\n",
      "Epoch 61, batch 46/93, loss: 0.1176\n",
      "Epoch 61, batch 51/93, loss: 0.1108\n",
      "Epoch 61, batch 56/93, loss: 0.2211\n",
      "Epoch 61, batch 61/93, loss: 0.0868\n",
      "Epoch 61, batch 66/93, loss: 0.2252\n",
      "Epoch 61, batch 71/93, loss: 0.0823\n",
      "Epoch 61, batch 76/93, loss: 0.1366\n",
      "Epoch 61, batch 81/93, loss: 0.0734\n",
      "Epoch 61, batch 86/93, loss: 0.0962\n",
      "Epoch 61, batch 91/93, loss: 0.0957\n",
      "Epoch 61, train loss: 0.1342, train accuracy: 95.74%\n",
      "Epoch 61, val loss: 0.6092, val accuracy: 85.20%\n",
      "Epoch 62, batch 1/93, loss: 0.0397\n",
      "Epoch 62, batch 6/93, loss: 0.1002\n",
      "Epoch 62, batch 11/93, loss: 0.1113\n",
      "Epoch 62, batch 16/93, loss: 0.0397\n",
      "Epoch 62, batch 21/93, loss: 0.1558\n",
      "Epoch 62, batch 26/93, loss: 0.0413\n",
      "Epoch 62, batch 31/93, loss: 0.1093\n",
      "Epoch 62, batch 36/93, loss: 0.0830\n",
      "Epoch 62, batch 41/93, loss: 0.0434\n",
      "Epoch 62, batch 46/93, loss: 0.0626\n",
      "Epoch 62, batch 51/93, loss: 0.0374\n",
      "Epoch 62, batch 56/93, loss: 0.0957\n",
      "Epoch 62, batch 61/93, loss: 0.0746\n",
      "Epoch 62, batch 66/93, loss: 0.0601\n",
      "Epoch 62, batch 71/93, loss: 0.0956\n",
      "Epoch 62, batch 76/93, loss: 0.0464\n",
      "Epoch 62, batch 81/93, loss: 0.0359\n",
      "Epoch 62, batch 86/93, loss: 0.2106\n",
      "Epoch 62, batch 91/93, loss: 0.0743\n",
      "Epoch 62, train loss: 0.0779, train accuracy: 97.78%\n",
      "Epoch 62, val loss: 0.6217, val accuracy: 85.89%\n",
      "Epoch 63, batch 1/93, loss: 0.1366\n",
      "Epoch 63, batch 6/93, loss: 0.1062\n",
      "Epoch 63, batch 11/93, loss: 0.0918\n",
      "Epoch 63, batch 16/93, loss: 0.0912\n",
      "Epoch 63, batch 21/93, loss: 0.0327\n",
      "Epoch 63, batch 26/93, loss: 0.1191\n",
      "Epoch 63, batch 31/93, loss: 0.1102\n",
      "Epoch 63, batch 36/93, loss: 0.0657\n",
      "Epoch 63, batch 41/93, loss: 0.0540\n",
      "Epoch 63, batch 46/93, loss: 0.0788\n",
      "Epoch 63, batch 51/93, loss: 0.0727\n",
      "Epoch 63, batch 56/93, loss: 0.0286\n",
      "Epoch 63, batch 61/93, loss: 0.0246\n",
      "Epoch 63, batch 66/93, loss: 0.0448\n",
      "Epoch 63, batch 71/93, loss: 0.1342\n",
      "Epoch 63, batch 76/93, loss: 0.0408\n",
      "Epoch 63, batch 81/93, loss: 0.0543\n",
      "Epoch 63, batch 86/93, loss: 0.0744\n",
      "Epoch 63, batch 91/93, loss: 0.0498\n",
      "Epoch 63, train loss: 0.0746, train accuracy: 97.79%\n",
      "Epoch 63, val loss: 0.5554, val accuracy: 85.76%\n",
      "Epoch 64, batch 1/93, loss: 0.0630\n",
      "Epoch 64, batch 6/93, loss: 0.1497\n",
      "Epoch 64, batch 11/93, loss: 0.0597\n",
      "Epoch 64, batch 16/93, loss: 0.0424\n",
      "Epoch 64, batch 21/93, loss: 0.0557\n",
      "Epoch 64, batch 26/93, loss: 0.0472\n",
      "Epoch 64, batch 31/93, loss: 0.0541\n",
      "Epoch 64, batch 36/93, loss: 0.0705\n",
      "Epoch 64, batch 41/93, loss: 0.0685\n",
      "Epoch 64, batch 46/93, loss: 0.0758\n",
      "Epoch 64, batch 51/93, loss: 0.0756\n",
      "Epoch 64, batch 56/93, loss: 0.0358\n",
      "Epoch 64, batch 61/93, loss: 0.0878\n",
      "Epoch 64, batch 66/93, loss: 0.0668\n",
      "Epoch 64, batch 71/93, loss: 0.0754\n",
      "Epoch 64, batch 76/93, loss: 0.0489\n",
      "Epoch 64, batch 81/93, loss: 0.0951\n",
      "Epoch 64, batch 86/93, loss: 0.0355\n",
      "Epoch 64, batch 91/93, loss: 0.0659\n",
      "Epoch 64, train loss: 0.0718, train accuracy: 98.05%\n",
      "Epoch 64, val loss: 0.6232, val accuracy: 85.20%\n",
      "Epoch 65, batch 1/93, loss: 0.0397\n",
      "Epoch 65, batch 6/93, loss: 0.0897\n",
      "Epoch 65, batch 11/93, loss: 0.0772\n",
      "Epoch 65, batch 16/93, loss: 0.0510\n",
      "Epoch 65, batch 21/93, loss: 0.0426\n",
      "Epoch 65, batch 26/93, loss: 0.0940\n",
      "Epoch 65, batch 31/93, loss: 0.0592\n",
      "Epoch 65, batch 36/93, loss: 0.1576\n",
      "Epoch 65, batch 41/93, loss: 0.1103\n",
      "Epoch 65, batch 46/93, loss: 0.0998\n",
      "Epoch 65, batch 51/93, loss: 0.0369\n",
      "Epoch 65, batch 56/93, loss: 0.1326\n",
      "Epoch 65, batch 61/93, loss: 0.1275\n",
      "Epoch 65, batch 66/93, loss: 0.0973\n",
      "Epoch 65, batch 71/93, loss: 0.1147\n",
      "Epoch 65, batch 76/93, loss: 0.0668\n",
      "Epoch 65, batch 81/93, loss: 0.1379\n",
      "Epoch 65, batch 86/93, loss: 0.1073\n",
      "Epoch 65, batch 91/93, loss: 0.1969\n",
      "Epoch 65, train loss: 0.0938, train accuracy: 97.21%\n",
      "Epoch 65, val loss: 0.6076, val accuracy: 84.38%\n",
      "Epoch 66, batch 1/93, loss: 0.0670\n",
      "Epoch 66, batch 6/93, loss: 0.3559\n",
      "Epoch 66, batch 11/93, loss: 0.0807\n",
      "Epoch 66, batch 16/93, loss: 0.1414\n",
      "Epoch 66, batch 21/93, loss: 0.1362\n",
      "Epoch 66, batch 26/93, loss: 0.1161\n",
      "Epoch 66, batch 31/93, loss: 0.1590\n",
      "Epoch 66, batch 36/93, loss: 0.0935\n",
      "Epoch 66, batch 41/93, loss: 0.2104\n",
      "Epoch 66, batch 46/93, loss: 0.0911\n",
      "Epoch 66, batch 51/93, loss: 0.0793\n",
      "Epoch 66, batch 56/93, loss: 0.0522\n",
      "Epoch 66, batch 61/93, loss: 0.0538\n",
      "Epoch 66, batch 66/93, loss: 0.0968\n",
      "Epoch 66, batch 71/93, loss: 0.0675\n",
      "Epoch 66, batch 76/93, loss: 0.1578\n",
      "Epoch 66, batch 81/93, loss: 0.0715\n",
      "Epoch 66, batch 86/93, loss: 0.0668\n",
      "Epoch 66, batch 91/93, loss: 0.0616\n",
      "Epoch 66, train loss: 0.1192, train accuracy: 96.17%\n",
      "Epoch 66, val loss: 0.5910, val accuracy: 84.68%\n",
      "Epoch 67, batch 1/93, loss: 0.1120\n",
      "Epoch 67, batch 6/93, loss: 0.0658\n",
      "Epoch 67, batch 11/93, loss: 0.0954\n",
      "Epoch 67, batch 16/93, loss: 0.1599\n",
      "Epoch 67, batch 21/93, loss: 0.1375\n",
      "Epoch 67, batch 26/93, loss: 0.1477\n",
      "Epoch 67, batch 31/93, loss: 0.1035\n",
      "Epoch 67, batch 36/93, loss: 0.1078\n",
      "Epoch 67, batch 41/93, loss: 0.0702\n",
      "Epoch 67, batch 46/93, loss: 0.1313\n",
      "Epoch 67, batch 51/93, loss: 0.0444\n",
      "Epoch 67, batch 56/93, loss: 0.0945\n",
      "Epoch 67, batch 61/93, loss: 0.0590\n",
      "Epoch 67, batch 66/93, loss: 0.0403\n",
      "Epoch 67, batch 71/93, loss: 0.0708\n",
      "Epoch 67, batch 76/93, loss: 0.0836\n",
      "Epoch 67, batch 81/93, loss: 0.0953\n",
      "Epoch 67, batch 86/93, loss: 0.0939\n",
      "Epoch 67, batch 91/93, loss: 0.1644\n",
      "Epoch 67, train loss: 0.0863, train accuracy: 97.37%\n",
      "Epoch 67, val loss: 0.5668, val accuracy: 85.20%\n",
      "Epoch 68, batch 1/93, loss: 0.0665\n",
      "Epoch 68, batch 6/93, loss: 0.1182\n",
      "Epoch 68, batch 11/93, loss: 0.1278\n",
      "Epoch 68, batch 16/93, loss: 0.1165\n",
      "Epoch 68, batch 21/93, loss: 0.1965\n",
      "Epoch 68, batch 26/93, loss: 0.1099\n",
      "Epoch 68, batch 31/93, loss: 0.0981\n",
      "Epoch 68, batch 36/93, loss: 0.0728\n",
      "Epoch 68, batch 41/93, loss: 0.0965\n",
      "Epoch 68, batch 46/93, loss: 0.1442\n",
      "Epoch 68, batch 51/93, loss: 0.0379\n",
      "Epoch 68, batch 56/93, loss: 0.0844\n",
      "Epoch 68, batch 61/93, loss: 0.1207\n",
      "Epoch 68, batch 66/93, loss: 0.0921\n",
      "Epoch 68, batch 71/93, loss: 0.1042\n",
      "Epoch 68, batch 76/93, loss: 0.1074\n",
      "Epoch 68, batch 81/93, loss: 0.0492\n",
      "Epoch 68, batch 86/93, loss: 0.0262\n",
      "Epoch 68, batch 91/93, loss: 0.0363\n",
      "Epoch 68, train loss: 0.0925, train accuracy: 97.20%\n",
      "Epoch 68, val loss: 0.5978, val accuracy: 84.98%\n",
      "Epoch 69, batch 1/93, loss: 0.0672\n",
      "Epoch 69, batch 6/93, loss: 0.2396\n",
      "Epoch 69, batch 11/93, loss: 0.0401\n",
      "Epoch 69, batch 16/93, loss: 0.0866\n",
      "Epoch 69, batch 21/93, loss: 0.1024\n",
      "Epoch 69, batch 26/93, loss: 0.0841\n",
      "Epoch 69, batch 31/93, loss: 0.0844\n",
      "Epoch 69, batch 36/93, loss: 0.0761\n",
      "Epoch 69, batch 41/93, loss: 0.1462\n",
      "Epoch 69, batch 46/93, loss: 0.1100\n",
      "Epoch 69, batch 51/93, loss: 0.0537\n",
      "Epoch 69, batch 56/93, loss: 0.0377\n",
      "Epoch 69, batch 61/93, loss: 0.1393\n",
      "Epoch 69, batch 66/93, loss: 0.0307\n",
      "Epoch 69, batch 71/93, loss: 0.0939\n",
      "Epoch 69, batch 76/93, loss: 0.0874\n",
      "Epoch 69, batch 81/93, loss: 0.0518\n",
      "Epoch 69, batch 86/93, loss: 0.0330\n",
      "Epoch 69, batch 91/93, loss: 0.0484\n",
      "Epoch 69, train loss: 0.0788, train accuracy: 97.63%\n",
      "Epoch 69, val loss: 0.5513, val accuracy: 85.68%\n",
      "Epoch 70, batch 1/93, loss: 0.0417\n",
      "Epoch 70, batch 6/93, loss: 0.0590\n",
      "Epoch 70, batch 11/93, loss: 0.0915\n",
      "Epoch 70, batch 16/93, loss: 0.0637\n",
      "Epoch 70, batch 21/93, loss: 0.0468\n",
      "Epoch 70, batch 26/93, loss: 0.0336\n",
      "Epoch 70, batch 31/93, loss: 0.0513\n",
      "Epoch 70, batch 36/93, loss: 0.0745\n",
      "Epoch 70, batch 41/93, loss: 0.0952\n",
      "Epoch 70, batch 46/93, loss: 0.1127\n",
      "Epoch 70, batch 51/93, loss: 0.0810\n",
      "Epoch 70, batch 56/93, loss: 0.0603\n",
      "Epoch 70, batch 61/93, loss: 0.0498\n",
      "Epoch 70, batch 66/93, loss: 0.0332\n",
      "Epoch 70, batch 71/93, loss: 0.0371\n",
      "Epoch 70, batch 76/93, loss: 0.0364\n",
      "Epoch 70, batch 81/93, loss: 0.0422\n",
      "Epoch 70, batch 86/93, loss: 0.0635\n",
      "Epoch 70, batch 91/93, loss: 0.0351\n",
      "Epoch 70, train loss: 0.0676, train accuracy: 98.32%\n",
      "Epoch 70, val loss: 0.6484, val accuracy: 85.76%\n",
      "Epoch 71, batch 1/93, loss: 0.0625\n",
      "Epoch 71, batch 6/93, loss: 0.0463\n",
      "Epoch 71, batch 11/93, loss: 0.0834\n",
      "Epoch 71, batch 16/93, loss: 0.0854\n",
      "Epoch 71, batch 21/93, loss: 0.0736\n",
      "Epoch 71, batch 26/93, loss: 0.0607\n",
      "Epoch 71, batch 31/93, loss: 0.0416\n",
      "Epoch 71, batch 36/93, loss: 0.0787\n",
      "Epoch 71, batch 41/93, loss: 0.0835\n",
      "Epoch 71, batch 46/93, loss: 0.0376\n",
      "Epoch 71, batch 51/93, loss: 0.0863\n",
      "Epoch 71, batch 56/93, loss: 0.0838\n",
      "Epoch 71, batch 61/93, loss: 0.0502\n",
      "Epoch 71, batch 66/93, loss: 0.0618\n",
      "Epoch 71, batch 71/93, loss: 0.1298\n",
      "Epoch 71, batch 76/93, loss: 0.1012\n",
      "Epoch 71, batch 81/93, loss: 0.0607\n",
      "Epoch 71, batch 86/93, loss: 0.0275\n",
      "Epoch 71, batch 91/93, loss: 0.1114\n",
      "Epoch 71, train loss: 0.0718, train accuracy: 98.01%\n",
      "Epoch 71, val loss: 0.6126, val accuracy: 85.37%\n",
      "Epoch 72, batch 1/93, loss: 0.0615\n",
      "Epoch 72, batch 6/93, loss: 0.1504\n",
      "Epoch 72, batch 11/93, loss: 0.1051\n",
      "Epoch 72, batch 16/93, loss: 0.2014\n",
      "Epoch 72, batch 21/93, loss: 0.1123\n",
      "Epoch 72, batch 26/93, loss: 0.0832\n",
      "Epoch 72, batch 31/93, loss: 0.1764\n",
      "Epoch 72, batch 36/93, loss: 0.0740\n",
      "Epoch 72, batch 41/93, loss: 0.0459\n",
      "Epoch 72, batch 46/93, loss: 0.0450\n",
      "Epoch 72, batch 51/93, loss: 0.0804\n",
      "Epoch 72, batch 56/93, loss: 0.0366\n",
      "Epoch 72, batch 61/93, loss: 0.0433\n",
      "Epoch 72, batch 66/93, loss: 0.0650\n",
      "Epoch 72, batch 71/93, loss: 0.0556\n",
      "Epoch 72, batch 76/93, loss: 0.0887\n",
      "Epoch 72, batch 81/93, loss: 0.1164\n",
      "Epoch 72, batch 86/93, loss: 0.1171\n",
      "Epoch 72, batch 91/93, loss: 0.0335\n",
      "Epoch 72, train loss: 0.0827, train accuracy: 97.45%\n",
      "Epoch 72, val loss: 0.6392, val accuracy: 85.59%\n",
      "Epoch 73, batch 1/93, loss: 0.0970\n",
      "Epoch 73, batch 6/93, loss: 0.0432\n",
      "Epoch 73, batch 11/93, loss: 0.1440\n",
      "Epoch 73, batch 16/93, loss: 0.0490\n",
      "Epoch 73, batch 21/93, loss: 0.1007\n",
      "Epoch 73, batch 26/93, loss: 0.0563\n",
      "Epoch 73, batch 31/93, loss: 0.0568\n",
      "Epoch 73, batch 36/93, loss: 0.0711\n",
      "Epoch 73, batch 41/93, loss: 0.1350\n",
      "Epoch 73, batch 46/93, loss: 0.0276\n",
      "Epoch 73, batch 51/93, loss: 0.0483\n",
      "Epoch 73, batch 56/93, loss: 0.0515\n",
      "Epoch 73, batch 61/93, loss: 0.0807\n",
      "Epoch 73, batch 66/93, loss: 0.0845\n",
      "Epoch 73, batch 71/93, loss: 0.0379\n",
      "Epoch 73, batch 76/93, loss: 0.0388\n",
      "Epoch 73, batch 81/93, loss: 0.0315\n",
      "Epoch 73, batch 86/93, loss: 0.0548\n",
      "Epoch 73, batch 91/93, loss: 0.0454\n",
      "Epoch 73, train loss: 0.0698, train accuracy: 97.92%\n",
      "Epoch 73, val loss: 0.6395, val accuracy: 85.55%\n",
      "Epoch 74, batch 1/93, loss: 0.0313\n",
      "Epoch 74, batch 6/93, loss: 0.0829\n",
      "Epoch 74, batch 11/93, loss: 0.0558\n",
      "Epoch 74, batch 16/93, loss: 0.0886\n",
      "Epoch 74, batch 21/93, loss: 0.0722\n",
      "Epoch 74, batch 26/93, loss: 0.0617\n",
      "Epoch 74, batch 31/93, loss: 0.0289\n",
      "Epoch 74, batch 36/93, loss: 0.0501\n",
      "Epoch 74, batch 41/93, loss: 0.0363\n",
      "Epoch 74, batch 46/93, loss: 0.0361\n",
      "Epoch 74, batch 51/93, loss: 0.0503\n",
      "Epoch 74, batch 56/93, loss: 0.0571\n",
      "Epoch 74, batch 61/93, loss: 0.0664\n",
      "Epoch 74, batch 66/93, loss: 0.0605\n",
      "Epoch 74, batch 71/93, loss: 0.0382\n",
      "Epoch 74, batch 76/93, loss: 0.0406\n",
      "Epoch 74, batch 81/93, loss: 0.0394\n",
      "Epoch 74, batch 86/93, loss: 0.0209\n",
      "Epoch 74, batch 91/93, loss: 0.0494\n",
      "Epoch 74, train loss: 0.0601, train accuracy: 98.38%\n",
      "Epoch 74, val loss: 0.5922, val accuracy: 85.24%\n",
      "Epoch 75, batch 1/93, loss: 0.0484\n",
      "Epoch 75, batch 6/93, loss: 0.0774\n",
      "Epoch 75, batch 11/93, loss: 0.1049\n",
      "Epoch 75, batch 16/93, loss: 0.0845\n",
      "Epoch 75, batch 21/93, loss: 0.0292\n",
      "Epoch 75, batch 26/93, loss: 0.0571\n",
      "Epoch 75, batch 31/93, loss: 0.0469\n",
      "Epoch 75, batch 36/93, loss: 0.0500\n",
      "Epoch 75, batch 41/93, loss: 0.1105\n",
      "Epoch 75, batch 46/93, loss: 0.0604\n",
      "Epoch 75, batch 51/93, loss: 0.0584\n",
      "Epoch 75, batch 56/93, loss: 0.0509\n",
      "Epoch 75, batch 61/93, loss: 0.0504\n",
      "Epoch 75, batch 66/93, loss: 0.0457\n",
      "Epoch 75, batch 71/93, loss: 0.0329\n",
      "Epoch 75, batch 76/93, loss: 0.0249\n",
      "Epoch 75, batch 81/93, loss: 0.0552\n",
      "Epoch 75, batch 86/93, loss: 0.0545\n",
      "Epoch 75, batch 91/93, loss: 0.0381\n",
      "Epoch 75, train loss: 0.0578, train accuracy: 98.35%\n",
      "Epoch 75, val loss: 0.6611, val accuracy: 84.16%\n",
      "Epoch 76, batch 1/93, loss: 0.0533\n",
      "Epoch 76, batch 6/93, loss: 0.0501\n",
      "Epoch 76, batch 11/93, loss: 0.0681\n",
      "Epoch 76, batch 16/93, loss: 0.0512\n",
      "Epoch 76, batch 21/93, loss: 0.0552\n",
      "Epoch 76, batch 26/93, loss: 0.0723\n",
      "Epoch 76, batch 31/93, loss: 0.0571\n",
      "Epoch 76, batch 36/93, loss: 0.0724\n",
      "Epoch 76, batch 41/93, loss: 0.0355\n",
      "Epoch 76, batch 46/93, loss: 0.0632\n",
      "Epoch 76, batch 51/93, loss: 0.0306\n",
      "Epoch 76, batch 56/93, loss: 0.1412\n",
      "Epoch 76, batch 61/93, loss: 0.0421\n",
      "Epoch 76, batch 66/93, loss: 0.0435\n",
      "Epoch 76, batch 71/93, loss: 0.0406\n",
      "Epoch 76, batch 76/93, loss: 0.0901\n",
      "Epoch 76, batch 81/93, loss: 0.0818\n",
      "Epoch 76, batch 86/93, loss: 0.0506\n",
      "Epoch 76, batch 91/93, loss: 0.0471\n",
      "Epoch 76, train loss: 0.0690, train accuracy: 98.13%\n",
      "Epoch 76, val loss: 0.6565, val accuracy: 85.33%\n",
      "Epoch 77, batch 1/93, loss: 0.0287\n",
      "Epoch 77, batch 6/93, loss: 0.1449\n",
      "Epoch 77, batch 11/93, loss: 0.3401\n",
      "Epoch 77, batch 16/93, loss: 0.1360\n",
      "Epoch 77, batch 21/93, loss: 0.1479\n",
      "Epoch 77, batch 26/93, loss: 0.1408\n",
      "Epoch 77, batch 31/93, loss: 0.2134\n",
      "Epoch 77, batch 36/93, loss: 0.1684\n",
      "Epoch 77, batch 41/93, loss: 0.0803\n",
      "Epoch 77, batch 46/93, loss: 0.1521\n",
      "Epoch 77, batch 51/93, loss: 0.0779\n",
      "Epoch 77, batch 56/93, loss: 0.0411\n",
      "Epoch 77, batch 61/93, loss: 0.0389\n",
      "Epoch 77, batch 66/93, loss: 0.0453\n",
      "Epoch 77, batch 71/93, loss: 0.0539\n",
      "Epoch 77, batch 76/93, loss: 0.0659\n",
      "Epoch 77, batch 81/93, loss: 0.0629\n",
      "Epoch 77, batch 86/93, loss: 0.0653\n",
      "Epoch 77, batch 91/93, loss: 0.0363\n",
      "Epoch 77, train loss: 0.0998, train accuracy: 96.69%\n",
      "Epoch 77, val loss: 0.7268, val accuracy: 83.72%\n",
      "Epoch 78, batch 1/93, loss: 0.0249\n",
      "Epoch 78, batch 6/93, loss: 0.0287\n",
      "Epoch 78, batch 11/93, loss: 0.0728\n",
      "Epoch 78, batch 16/93, loss: 0.0458\n",
      "Epoch 78, batch 21/93, loss: 0.0560\n",
      "Epoch 78, batch 26/93, loss: 0.0120\n",
      "Epoch 78, batch 31/93, loss: 0.0961\n",
      "Epoch 78, batch 36/93, loss: 0.1484\n",
      "Epoch 78, batch 41/93, loss: 0.0329\n",
      "Epoch 78, batch 46/93, loss: 0.0476\n",
      "Epoch 78, batch 51/93, loss: 0.0484\n",
      "Epoch 78, batch 56/93, loss: 0.0439\n",
      "Epoch 78, batch 61/93, loss: 0.0256\n",
      "Epoch 78, batch 66/93, loss: 0.0609\n",
      "Epoch 78, batch 71/93, loss: 0.0393\n",
      "Epoch 78, batch 76/93, loss: 0.0969\n",
      "Epoch 78, batch 81/93, loss: 0.0502\n",
      "Epoch 78, batch 86/93, loss: 0.0684\n",
      "Epoch 78, batch 91/93, loss: 0.1794\n",
      "Epoch 78, train loss: 0.0654, train accuracy: 98.09%\n",
      "Epoch 78, val loss: 0.6565, val accuracy: 85.16%\n",
      "Epoch 79, batch 1/93, loss: 0.0343\n",
      "Epoch 79, batch 6/93, loss: 0.0553\n",
      "Epoch 79, batch 11/93, loss: 0.0344\n",
      "Epoch 79, batch 16/93, loss: 0.0321\n",
      "Epoch 79, batch 21/93, loss: 0.0278\n",
      "Epoch 79, batch 26/93, loss: 0.0902\n",
      "Epoch 79, batch 31/93, loss: 0.0487\n",
      "Epoch 79, batch 36/93, loss: 0.0846\n",
      "Epoch 79, batch 41/93, loss: 0.0339\n",
      "Epoch 79, batch 46/93, loss: 0.0436\n",
      "Epoch 79, batch 51/93, loss: 0.0314\n",
      "Epoch 79, batch 56/93, loss: 0.0226\n",
      "Epoch 79, batch 61/93, loss: 0.0159\n",
      "Epoch 79, batch 66/93, loss: 0.0712\n",
      "Epoch 79, batch 71/93, loss: 0.0329\n",
      "Epoch 79, batch 76/93, loss: 0.0612\n",
      "Epoch 79, batch 81/93, loss: 0.0760\n",
      "Epoch 79, batch 86/93, loss: 0.0219\n",
      "Epoch 79, batch 91/93, loss: 0.0586\n",
      "Epoch 79, train loss: 0.0518, train accuracy: 98.58%\n",
      "Epoch 79, val loss: 0.6615, val accuracy: 86.02%\n",
      "Epoch 80, batch 1/93, loss: 0.0228\n",
      "Epoch 80, batch 6/93, loss: 0.0201\n",
      "Epoch 80, batch 11/93, loss: 0.0884\n",
      "Epoch 80, batch 16/93, loss: 0.0263\n",
      "Epoch 80, batch 21/93, loss: 0.0212\n",
      "Epoch 80, batch 26/93, loss: 0.0183\n",
      "Epoch 80, batch 31/93, loss: 0.0538\n",
      "Epoch 80, batch 36/93, loss: 0.0764\n",
      "Epoch 80, batch 41/93, loss: 0.0187\n",
      "Epoch 80, batch 46/93, loss: 0.0496\n",
      "Epoch 80, batch 51/93, loss: 0.0592\n",
      "Epoch 80, batch 56/93, loss: 0.0545\n",
      "Epoch 80, batch 61/93, loss: 0.0137\n",
      "Epoch 80, batch 66/93, loss: 0.0365\n",
      "Epoch 80, batch 71/93, loss: 0.0403\n",
      "Epoch 80, batch 76/93, loss: 0.0257\n",
      "Epoch 80, batch 81/93, loss: 0.0589\n",
      "Epoch 80, batch 86/93, loss: 0.0244\n",
      "Epoch 80, batch 91/93, loss: 0.0651\n",
      "Epoch 80, train loss: 0.0466, train accuracy: 98.85%\n",
      "Epoch 80, val loss: 0.6980, val accuracy: 85.63%\n",
      "Epoch 81, batch 1/93, loss: 0.0181\n",
      "Epoch 81, batch 6/93, loss: 0.1098\n",
      "Epoch 81, batch 11/93, loss: 0.0617\n",
      "Epoch 81, batch 16/93, loss: 0.0277\n",
      "Epoch 81, batch 21/93, loss: 0.0441\n",
      "Epoch 81, batch 26/93, loss: 0.0346\n",
      "Epoch 81, batch 31/93, loss: 0.0264\n",
      "Epoch 81, batch 36/93, loss: 0.0353\n",
      "Epoch 81, batch 41/93, loss: 0.0198\n",
      "Epoch 81, batch 46/93, loss: 0.1149\n",
      "Epoch 81, batch 51/93, loss: 0.0215\n",
      "Epoch 81, batch 56/93, loss: 0.0415\n",
      "Epoch 81, batch 61/93, loss: 0.0385\n",
      "Epoch 81, batch 66/93, loss: 0.0393\n",
      "Epoch 81, batch 71/93, loss: 0.0301\n",
      "Epoch 81, batch 76/93, loss: 0.0326\n",
      "Epoch 81, batch 81/93, loss: 0.0608\n",
      "Epoch 81, batch 86/93, loss: 0.0234\n",
      "Epoch 81, batch 91/93, loss: 0.0122\n",
      "Epoch 81, train loss: 0.0464, train accuracy: 98.85%\n",
      "Epoch 81, val loss: 0.7008, val accuracy: 85.37%\n",
      "Epoch 82, batch 1/93, loss: 0.0316\n",
      "Epoch 82, batch 6/93, loss: 0.0902\n",
      "Epoch 82, batch 11/93, loss: 0.1106\n",
      "Epoch 82, batch 16/93, loss: 0.0438\n",
      "Epoch 82, batch 21/93, loss: 0.1033\n",
      "Epoch 82, batch 26/93, loss: 0.0447\n",
      "Epoch 82, batch 31/93, loss: 0.0777\n",
      "Epoch 82, batch 36/93, loss: 0.0605\n",
      "Epoch 82, batch 41/93, loss: 0.0665\n",
      "Epoch 82, batch 46/93, loss: 0.0364\n",
      "Epoch 82, batch 51/93, loss: 0.0643\n",
      "Epoch 82, batch 56/93, loss: 0.0751\n",
      "Epoch 82, batch 61/93, loss: 0.0493\n",
      "Epoch 82, batch 66/93, loss: 0.0762\n",
      "Epoch 82, batch 71/93, loss: 0.0543\n",
      "Epoch 82, batch 76/93, loss: 0.0430\n",
      "Epoch 82, batch 81/93, loss: 0.0538\n",
      "Epoch 82, batch 86/93, loss: 0.0328\n",
      "Epoch 82, batch 91/93, loss: 0.0891\n",
      "Epoch 82, train loss: 0.0789, train accuracy: 97.83%\n",
      "Epoch 82, val loss: 0.7841, val accuracy: 84.20%\n",
      "Epoch 83, batch 1/93, loss: 0.0409\n",
      "Epoch 83, batch 6/93, loss: 0.0675\n",
      "Epoch 83, batch 11/93, loss: 0.2684\n",
      "Epoch 83, batch 16/93, loss: 0.1785\n",
      "Epoch 83, batch 21/93, loss: 0.1065\n",
      "Epoch 83, batch 26/93, loss: 0.2250\n",
      "Epoch 83, batch 31/93, loss: 0.2150\n",
      "Epoch 83, batch 36/93, loss: 0.1738\n",
      "Epoch 83, batch 41/93, loss: 0.0969\n",
      "Epoch 83, batch 46/93, loss: 0.0956\n",
      "Epoch 83, batch 51/93, loss: 0.0992\n",
      "Epoch 83, batch 56/93, loss: 0.1342\n",
      "Epoch 83, batch 61/93, loss: 0.0787\n",
      "Epoch 83, batch 66/93, loss: 0.0671\n",
      "Epoch 83, batch 71/93, loss: 0.0957\n",
      "Epoch 83, batch 76/93, loss: 0.0538\n",
      "Epoch 83, batch 81/93, loss: 0.0736\n",
      "Epoch 83, batch 86/93, loss: 0.0560\n",
      "Epoch 83, batch 91/93, loss: 0.0605\n",
      "Epoch 83, train loss: 0.1374, train accuracy: 95.53%\n",
      "Epoch 83, val loss: 0.6781, val accuracy: 83.42%\n",
      "Epoch 84, batch 1/93, loss: 0.0492\n",
      "Epoch 84, batch 6/93, loss: 0.0911\n",
      "Epoch 84, batch 11/93, loss: 0.0434\n",
      "Epoch 84, batch 16/93, loss: 0.0433\n",
      "Epoch 84, batch 21/93, loss: 0.0779\n",
      "Epoch 84, batch 26/93, loss: 0.0504\n",
      "Epoch 84, batch 31/93, loss: 0.0957\n",
      "Epoch 84, batch 36/93, loss: 0.0639\n",
      "Epoch 84, batch 41/93, loss: 0.0368\n",
      "Epoch 84, batch 46/93, loss: 0.0843\n",
      "Epoch 84, batch 51/93, loss: 0.0552\n",
      "Epoch 84, batch 56/93, loss: 0.0933\n",
      "Epoch 84, batch 61/93, loss: 0.0955\n",
      "Epoch 84, batch 66/93, loss: 0.1133\n",
      "Epoch 84, batch 71/93, loss: 0.0410\n",
      "Epoch 84, batch 76/93, loss: 0.0562\n",
      "Epoch 84, batch 81/93, loss: 0.0785\n",
      "Epoch 84, batch 86/93, loss: 0.0507\n",
      "Epoch 84, batch 91/93, loss: 0.0371\n",
      "Epoch 84, train loss: 0.0702, train accuracy: 97.88%\n",
      "Epoch 84, val loss: 0.6874, val accuracy: 84.90%\n",
      "Epoch 85, batch 1/93, loss: 0.0449\n",
      "Epoch 85, batch 6/93, loss: 0.1586\n",
      "Epoch 85, batch 11/93, loss: 0.1031\n",
      "Epoch 85, batch 16/93, loss: 0.1106\n",
      "Epoch 85, batch 21/93, loss: 0.1253\n",
      "Epoch 85, batch 26/93, loss: 0.1114\n",
      "Epoch 85, batch 31/93, loss: 0.0884\n",
      "Epoch 85, batch 36/93, loss: 0.1394\n",
      "Epoch 85, batch 41/93, loss: 0.0577\n",
      "Epoch 85, batch 46/93, loss: 0.0944\n",
      "Epoch 85, batch 51/93, loss: 0.0579\n",
      "Epoch 85, batch 56/93, loss: 0.0304\n",
      "Epoch 85, batch 61/93, loss: 0.0477\n",
      "Epoch 85, batch 66/93, loss: 0.0574\n",
      "Epoch 85, batch 71/93, loss: 0.0645\n",
      "Epoch 85, batch 76/93, loss: 0.0526\n",
      "Epoch 85, batch 81/93, loss: 0.0626\n",
      "Epoch 85, batch 86/93, loss: 0.0780\n",
      "Epoch 85, batch 91/93, loss: 0.0447\n",
      "Epoch 85, train loss: 0.0804, train accuracy: 97.48%\n",
      "Epoch 85, val loss: 0.6974, val accuracy: 84.55%\n",
      "Epoch 86, batch 1/93, loss: 0.0830\n",
      "Epoch 86, batch 6/93, loss: 0.2777\n",
      "Epoch 86, batch 11/93, loss: 0.0513\n",
      "Epoch 86, batch 16/93, loss: 0.1619\n",
      "Epoch 86, batch 21/93, loss: 0.3485\n",
      "Epoch 86, batch 26/93, loss: 0.1157\n",
      "Epoch 86, batch 31/93, loss: 0.1902\n",
      "Epoch 86, batch 36/93, loss: 0.1710\n",
      "Epoch 86, batch 41/93, loss: 0.1571\n",
      "Epoch 86, batch 46/93, loss: 0.2593\n",
      "Epoch 86, batch 51/93, loss: 0.0973\n",
      "Epoch 86, batch 56/93, loss: 0.0878\n",
      "Epoch 86, batch 61/93, loss: 0.1229\n",
      "Epoch 86, batch 66/93, loss: 0.1110\n",
      "Epoch 86, batch 71/93, loss: 0.0355\n",
      "Epoch 86, batch 76/93, loss: 0.0853\n",
      "Epoch 86, batch 81/93, loss: 0.0423\n",
      "Epoch 86, batch 86/93, loss: 0.0721\n",
      "Epoch 86, batch 91/93, loss: 0.0293\n",
      "Epoch 86, train loss: 0.1184, train accuracy: 96.41%\n",
      "Epoch 86, val loss: 0.7011, val accuracy: 84.11%\n",
      "Epoch 87, batch 1/93, loss: 0.0263\n",
      "Epoch 87, batch 6/93, loss: 0.0336\n",
      "Epoch 87, batch 11/93, loss: 0.1120\n",
      "Epoch 87, batch 16/93, loss: 0.1055\n",
      "Epoch 87, batch 21/93, loss: 0.0859\n",
      "Epoch 87, batch 26/93, loss: 0.1469\n",
      "Epoch 87, batch 31/93, loss: 0.0314\n",
      "Epoch 87, batch 36/93, loss: 0.1325\n",
      "Epoch 87, batch 41/93, loss: 0.1090\n",
      "Epoch 87, batch 46/93, loss: 0.0587\n",
      "Epoch 87, batch 51/93, loss: 0.0459\n",
      "Epoch 87, batch 56/93, loss: 0.0263\n",
      "Epoch 87, batch 61/93, loss: 0.0660\n",
      "Epoch 87, batch 66/93, loss: 0.0319\n",
      "Epoch 87, batch 71/93, loss: 0.0317\n",
      "Epoch 87, batch 76/93, loss: 0.0386\n",
      "Epoch 87, batch 81/93, loss: 0.0296\n",
      "Epoch 87, batch 86/93, loss: 0.0417\n",
      "Epoch 87, batch 91/93, loss: 0.0709\n",
      "Epoch 87, train loss: 0.0752, train accuracy: 97.71%\n",
      "Epoch 87, val loss: 0.7113, val accuracy: 84.77%\n",
      "Epoch 88, batch 1/93, loss: 0.0348\n",
      "Epoch 88, batch 6/93, loss: 0.0278\n",
      "Epoch 88, batch 11/93, loss: 0.0860\n",
      "Epoch 88, batch 16/93, loss: 0.1203\n",
      "Epoch 88, batch 21/93, loss: 0.0522\n",
      "Epoch 88, batch 26/93, loss: 0.0645\n",
      "Epoch 88, batch 31/93, loss: 0.1011\n",
      "Epoch 88, batch 36/93, loss: 0.1240\n",
      "Epoch 88, batch 41/93, loss: 0.1760\n",
      "Epoch 88, batch 46/93, loss: 0.0551\n",
      "Epoch 88, batch 51/93, loss: 0.0372\n",
      "Epoch 88, batch 56/93, loss: 0.0475\n",
      "Epoch 88, batch 61/93, loss: 0.0332\n",
      "Epoch 88, batch 66/93, loss: 0.0903\n",
      "Epoch 88, batch 71/93, loss: 0.0341\n",
      "Epoch 88, batch 76/93, loss: 0.0436\n",
      "Epoch 88, batch 81/93, loss: 0.0247\n",
      "Epoch 88, batch 86/93, loss: 0.0225\n",
      "Epoch 88, batch 91/93, loss: 0.0369\n",
      "Epoch 88, train loss: 0.0654, train accuracy: 97.94%\n",
      "Epoch 88, val loss: 0.6691, val accuracy: 85.81%\n",
      "Epoch 89, batch 1/93, loss: 0.0457\n",
      "Epoch 89, batch 6/93, loss: 0.0298\n",
      "Epoch 89, batch 11/93, loss: 0.0832\n",
      "Epoch 89, batch 16/93, loss: 0.0911\n",
      "Epoch 89, batch 21/93, loss: 0.0422\n",
      "Epoch 89, batch 26/93, loss: 0.0966\n",
      "Epoch 89, batch 31/93, loss: 0.0389\n",
      "Epoch 89, batch 36/93, loss: 0.0699\n",
      "Epoch 89, batch 41/93, loss: 0.0853\n",
      "Epoch 89, batch 46/93, loss: 0.0505\n",
      "Epoch 89, batch 51/93, loss: 0.0400\n",
      "Epoch 89, batch 56/93, loss: 0.0165\n",
      "Epoch 89, batch 61/93, loss: 0.0372\n",
      "Epoch 89, batch 66/93, loss: 0.0766\n",
      "Epoch 89, batch 71/93, loss: 0.0482\n",
      "Epoch 89, batch 76/93, loss: 0.0334\n",
      "Epoch 89, batch 81/93, loss: 0.0411\n",
      "Epoch 89, batch 86/93, loss: 0.0729\n",
      "Epoch 89, batch 91/93, loss: 0.0578\n",
      "Epoch 89, train loss: 0.0489, train accuracy: 98.63%\n",
      "Epoch 89, val loss: 0.7296, val accuracy: 85.24%\n",
      "Epoch 90, batch 1/93, loss: 0.0169\n",
      "Epoch 90, batch 6/93, loss: 0.0766\n",
      "Epoch 90, batch 11/93, loss: 0.0957\n",
      "Epoch 90, batch 16/93, loss: 0.0711\n",
      "Epoch 90, batch 21/93, loss: 0.0686\n",
      "Epoch 90, batch 26/93, loss: 0.0268\n",
      "Epoch 90, batch 31/93, loss: 0.0917\n",
      "Epoch 90, batch 36/93, loss: 0.0430\n",
      "Epoch 90, batch 41/93, loss: 0.0633\n",
      "Epoch 90, batch 46/93, loss: 0.0401\n",
      "Epoch 90, batch 51/93, loss: 0.0330\n",
      "Epoch 90, batch 56/93, loss: 0.0376\n",
      "Epoch 90, batch 61/93, loss: 0.0650\n",
      "Epoch 90, batch 66/93, loss: 0.0317\n",
      "Epoch 90, batch 71/93, loss: 0.0512\n",
      "Epoch 90, batch 76/93, loss: 0.0219\n",
      "Epoch 90, batch 81/93, loss: 0.0389\n",
      "Epoch 90, batch 86/93, loss: 0.0361\n",
      "Epoch 90, batch 91/93, loss: 0.0481\n",
      "Epoch 90, train loss: 0.0471, train accuracy: 98.61%\n",
      "Epoch 90, val loss: 0.7540, val accuracy: 85.07%\n",
      "Epoch 91, batch 1/93, loss: 0.0245\n",
      "Epoch 91, batch 6/93, loss: 0.0460\n",
      "Epoch 91, batch 11/93, loss: 0.0387\n",
      "Epoch 91, batch 16/93, loss: 0.0246\n",
      "Epoch 91, batch 21/93, loss: 0.0511\n",
      "Epoch 91, batch 26/93, loss: 0.0444\n",
      "Epoch 91, batch 31/93, loss: 0.0459\n",
      "Epoch 91, batch 36/93, loss: 0.0521\n",
      "Epoch 91, batch 41/93, loss: 0.0119\n",
      "Epoch 91, batch 46/93, loss: 0.0305\n",
      "Epoch 91, batch 51/93, loss: 0.0283\n",
      "Epoch 91, batch 56/93, loss: 0.0304\n",
      "Epoch 91, batch 61/93, loss: 0.0674\n",
      "Epoch 91, batch 66/93, loss: 0.0471\n",
      "Epoch 91, batch 71/93, loss: 0.0153\n",
      "Epoch 91, batch 76/93, loss: 0.0307\n",
      "Epoch 91, batch 81/93, loss: 0.0319\n",
      "Epoch 91, batch 86/93, loss: 0.0207\n",
      "Epoch 91, batch 91/93, loss: 0.0192\n",
      "Epoch 91, train loss: 0.0342, train accuracy: 99.16%\n",
      "Epoch 91, val loss: 0.6474, val accuracy: 85.46%\n",
      "Epoch 92, batch 1/93, loss: 0.0245\n",
      "Epoch 92, batch 6/93, loss: 0.0296\n",
      "Epoch 92, batch 11/93, loss: 0.0606\n",
      "Epoch 92, batch 16/93, loss: 0.0171\n",
      "Epoch 92, batch 21/93, loss: 0.0343\n",
      "Epoch 92, batch 26/93, loss: 0.0484\n",
      "Epoch 92, batch 31/93, loss: 0.0607\n",
      "Epoch 92, batch 36/93, loss: 0.0400\n",
      "Epoch 92, batch 41/93, loss: 0.0608\n",
      "Epoch 92, batch 46/93, loss: 0.0221\n",
      "Epoch 92, batch 51/93, loss: 0.0782\n",
      "Epoch 92, batch 56/93, loss: 0.0225\n",
      "Epoch 92, batch 61/93, loss: 0.0846\n",
      "Epoch 92, batch 66/93, loss: 0.0509\n",
      "Epoch 92, batch 71/93, loss: 0.0413\n",
      "Epoch 92, batch 76/93, loss: 0.0584\n",
      "Epoch 92, batch 81/93, loss: 0.0209\n",
      "Epoch 92, batch 86/93, loss: 0.0258\n",
      "Epoch 92, batch 91/93, loss: 0.0659\n",
      "Epoch 92, train loss: 0.0351, train accuracy: 99.11%\n",
      "Epoch 92, val loss: 0.7163, val accuracy: 85.55%\n",
      "Epoch 93, batch 1/93, loss: 0.0127\n",
      "Epoch 93, batch 6/93, loss: 0.0790\n",
      "Epoch 93, batch 11/93, loss: 0.0232\n",
      "Epoch 93, batch 16/93, loss: 0.0266\n",
      "Epoch 93, batch 21/93, loss: 0.0492\n",
      "Epoch 93, batch 26/93, loss: 0.0682\n",
      "Epoch 93, batch 31/93, loss: 0.0424\n",
      "Epoch 93, batch 36/93, loss: 0.0406\n",
      "Epoch 93, batch 41/93, loss: 0.0472\n",
      "Epoch 93, batch 46/93, loss: 0.0220\n",
      "Epoch 93, batch 51/93, loss: 0.0493\n",
      "Epoch 93, batch 56/93, loss: 0.0078\n",
      "Epoch 93, batch 61/93, loss: 0.0244\n",
      "Epoch 93, batch 66/93, loss: 0.0318\n",
      "Epoch 93, batch 71/93, loss: 0.0187\n",
      "Epoch 93, batch 76/93, loss: 0.0179\n",
      "Epoch 93, batch 81/93, loss: 0.0286\n",
      "Epoch 93, batch 86/93, loss: 0.0281\n",
      "Epoch 93, batch 91/93, loss: 0.0358\n",
      "Epoch 93, train loss: 0.0380, train accuracy: 98.99%\n",
      "Epoch 93, val loss: 0.7238, val accuracy: 86.02%\n",
      "Epoch 94, batch 1/93, loss: 0.0123\n",
      "Epoch 94, batch 6/93, loss: 0.1449\n",
      "Epoch 94, batch 11/93, loss: 0.0531\n",
      "Epoch 94, batch 16/93, loss: 0.0330\n",
      "Epoch 94, batch 21/93, loss: 0.0597\n",
      "Epoch 94, batch 26/93, loss: 0.0273\n",
      "Epoch 94, batch 31/93, loss: 0.0406\n",
      "Epoch 94, batch 36/93, loss: 0.0432\n",
      "Epoch 94, batch 41/93, loss: 0.0250\n",
      "Epoch 94, batch 46/93, loss: 0.0473\n",
      "Epoch 94, batch 51/93, loss: 0.0292\n",
      "Epoch 94, batch 56/93, loss: 0.0246\n",
      "Epoch 94, batch 61/93, loss: 0.0600\n",
      "Epoch 94, batch 66/93, loss: 0.0264\n",
      "Epoch 94, batch 71/93, loss: 0.0331\n",
      "Epoch 94, batch 76/93, loss: 0.0245\n",
      "Epoch 94, batch 81/93, loss: 0.0580\n",
      "Epoch 94, batch 86/93, loss: 0.0245\n",
      "Epoch 94, batch 91/93, loss: 0.0382\n",
      "Epoch 94, train loss: 0.0498, train accuracy: 99.11%\n",
      "Epoch 94, val loss: 0.7632, val accuracy: 84.42%\n",
      "Epoch 95, batch 1/93, loss: 0.1367\n",
      "Epoch 95, batch 6/93, loss: 0.1636\n",
      "Epoch 95, batch 11/93, loss: 0.1270\n",
      "Epoch 95, batch 16/93, loss: 0.0682\n",
      "Epoch 95, batch 21/93, loss: 0.1309\n",
      "Epoch 95, batch 26/93, loss: 0.0429\n",
      "Epoch 95, batch 31/93, loss: 0.0495\n",
      "Epoch 95, batch 36/93, loss: 0.0737\n",
      "Epoch 95, batch 41/93, loss: 0.0134\n",
      "Epoch 95, batch 46/93, loss: 0.0386\n",
      "Epoch 95, batch 51/93, loss: 0.0362\n",
      "Epoch 95, batch 56/93, loss: 0.1448\n",
      "Epoch 95, batch 61/93, loss: 0.0944\n",
      "Epoch 95, batch 66/93, loss: 0.0385\n",
      "Epoch 95, batch 71/93, loss: 0.0558\n",
      "Epoch 95, batch 76/93, loss: 0.0812\n",
      "Epoch 95, batch 81/93, loss: 0.0327\n",
      "Epoch 95, batch 86/93, loss: 0.0298\n",
      "Epoch 95, batch 91/93, loss: 0.0415\n",
      "Epoch 95, train loss: 0.0629, train accuracy: 98.19%\n",
      "Epoch 95, val loss: 0.7028, val accuracy: 85.68%\n",
      "Epoch 96, batch 1/93, loss: 0.0274\n",
      "Epoch 96, batch 6/93, loss: 0.0451\n",
      "Epoch 96, batch 11/93, loss: 0.0677\n",
      "Epoch 96, batch 16/93, loss: 0.0212\n",
      "Epoch 96, batch 21/93, loss: 0.0164\n",
      "Epoch 96, batch 26/93, loss: 0.0382\n",
      "Epoch 96, batch 31/93, loss: 0.0651\n",
      "Epoch 96, batch 36/93, loss: 0.0187\n",
      "Epoch 96, batch 41/93, loss: 0.0287\n",
      "Epoch 96, batch 46/93, loss: 0.0430\n",
      "Epoch 96, batch 51/93, loss: 0.0227\n",
      "Epoch 96, batch 56/93, loss: 0.0304\n",
      "Epoch 96, batch 61/93, loss: 0.0467\n",
      "Epoch 96, batch 66/93, loss: 0.0442\n",
      "Epoch 96, batch 71/93, loss: 0.0291\n",
      "Epoch 96, batch 76/93, loss: 0.0131\n",
      "Epoch 96, batch 81/93, loss: 0.0370\n",
      "Epoch 96, batch 86/93, loss: 0.0090\n",
      "Epoch 96, batch 91/93, loss: 0.1074\n",
      "Epoch 96, train loss: 0.0446, train accuracy: 98.70%\n",
      "Epoch 96, val loss: 0.7123, val accuracy: 83.64%\n",
      "Epoch 97, batch 1/93, loss: 0.0629\n",
      "Epoch 97, batch 6/93, loss: 0.0459\n",
      "Epoch 97, batch 11/93, loss: 0.0647\n",
      "Epoch 97, batch 16/93, loss: 0.0363\n",
      "Epoch 97, batch 21/93, loss: 0.0295\n",
      "Epoch 97, batch 26/93, loss: 0.0335\n",
      "Epoch 97, batch 31/93, loss: 0.0318\n",
      "Epoch 97, batch 36/93, loss: 0.0424\n",
      "Epoch 97, batch 41/93, loss: 0.0152\n",
      "Epoch 97, batch 46/93, loss: 0.0640\n",
      "Epoch 97, batch 51/93, loss: 0.0343\n",
      "Epoch 97, batch 56/93, loss: 0.0124\n",
      "Epoch 97, batch 61/93, loss: 0.0392\n",
      "Epoch 97, batch 66/93, loss: 0.0344\n",
      "Epoch 97, batch 71/93, loss: 0.0327\n",
      "Epoch 97, batch 76/93, loss: 0.0239\n",
      "Epoch 97, batch 81/93, loss: 0.0208\n",
      "Epoch 97, batch 86/93, loss: 0.0215\n",
      "Epoch 97, batch 91/93, loss: 0.0327\n",
      "Epoch 97, train loss: 0.0457, train accuracy: 98.71%\n",
      "Epoch 97, val loss: 0.6521, val accuracy: 84.64%\n",
      "Epoch 98, batch 1/93, loss: 0.0337\n",
      "Epoch 98, batch 6/93, loss: 0.0275\n",
      "Epoch 98, batch 11/93, loss: 0.0959\n",
      "Epoch 98, batch 16/93, loss: 0.0416\n",
      "Epoch 98, batch 21/93, loss: 0.0787\n",
      "Epoch 98, batch 26/93, loss: 0.0430\n",
      "Epoch 98, batch 31/93, loss: 0.0504\n",
      "Epoch 98, batch 36/93, loss: 0.0396\n",
      "Epoch 98, batch 41/93, loss: 0.0154\n",
      "Epoch 98, batch 46/93, loss: 0.0313\n",
      "Epoch 98, batch 51/93, loss: 0.0321\n",
      "Epoch 98, batch 56/93, loss: 0.0325\n",
      "Epoch 98, batch 61/93, loss: 0.0260\n",
      "Epoch 98, batch 66/93, loss: 0.0265\n",
      "Epoch 98, batch 71/93, loss: 0.0137\n",
      "Epoch 98, batch 76/93, loss: 0.0207\n",
      "Epoch 98, batch 81/93, loss: 0.0137\n",
      "Epoch 98, batch 86/93, loss: 0.0225\n",
      "Epoch 98, batch 91/93, loss: 0.0178\n",
      "Epoch 98, train loss: 0.0332, train accuracy: 99.13%\n",
      "Epoch 98, val loss: 0.6839, val accuracy: 85.85%\n",
      "Epoch 99, batch 1/93, loss: 0.0311\n",
      "Epoch 99, batch 6/93, loss: 0.0316\n",
      "Epoch 99, batch 11/93, loss: 0.0118\n",
      "Epoch 99, batch 16/93, loss: 0.0138\n",
      "Epoch 99, batch 21/93, loss: 0.0112\n",
      "Epoch 99, batch 26/93, loss: 0.0213\n",
      "Epoch 99, batch 31/93, loss: 0.0405\n",
      "Epoch 99, batch 36/93, loss: 0.0259\n",
      "Epoch 99, batch 41/93, loss: 0.0270\n",
      "Epoch 99, batch 46/93, loss: 0.0237\n",
      "Epoch 99, batch 51/93, loss: 0.0213\n",
      "Epoch 99, batch 56/93, loss: 0.0283\n",
      "Epoch 99, batch 61/93, loss: 0.0427\n",
      "Epoch 99, batch 66/93, loss: 0.0415\n",
      "Epoch 99, batch 71/93, loss: 0.0454\n",
      "Epoch 99, batch 76/93, loss: 0.0158\n",
      "Epoch 99, batch 81/93, loss: 0.0448\n",
      "Epoch 99, batch 86/93, loss: 0.0261\n",
      "Epoch 99, batch 91/93, loss: 0.0728\n",
      "Epoch 99, train loss: 0.0373, train accuracy: 99.16%\n",
      "Epoch 99, val loss: 0.7126, val accuracy: 84.98%\n",
      "Epoch 100, batch 1/93, loss: 0.0132\n",
      "Epoch 100, batch 6/93, loss: 0.1542\n",
      "Epoch 100, batch 11/93, loss: 0.0887\n",
      "Epoch 100, batch 16/93, loss: 0.1483\n",
      "Epoch 100, batch 21/93, loss: 0.0653\n",
      "Epoch 100, batch 26/93, loss: 0.0397\n",
      "Epoch 100, batch 31/93, loss: 0.0902\n",
      "Epoch 100, batch 36/93, loss: 0.0679\n",
      "Epoch 100, batch 41/93, loss: 0.0205\n",
      "Epoch 100, batch 46/93, loss: 0.0329\n",
      "Epoch 100, batch 51/93, loss: 0.0464\n",
      "Epoch 100, batch 56/93, loss: 0.0667\n",
      "Epoch 100, batch 61/93, loss: 0.0456\n",
      "Epoch 100, batch 66/93, loss: 0.0310\n",
      "Epoch 100, batch 71/93, loss: 0.0418\n",
      "Epoch 100, batch 76/93, loss: 0.0525\n",
      "Epoch 100, batch 81/93, loss: 0.0332\n",
      "Epoch 100, batch 86/93, loss: 0.0660\n",
      "Epoch 100, batch 91/93, loss: 0.0383\n",
      "Epoch 100, train loss: 0.0650, train accuracy: 97.95%\n",
      "Epoch 100, val loss: 0.7817, val accuracy: 85.20%\n",
      "Epoch 101, batch 1/93, loss: 0.0614\n",
      "Epoch 101, batch 6/93, loss: 0.0807\n",
      "Epoch 101, batch 11/93, loss: 0.0467\n",
      "Epoch 101, batch 16/93, loss: 0.0875\n",
      "Epoch 101, batch 21/93, loss: 0.0321\n",
      "Epoch 101, batch 26/93, loss: 0.0729\n",
      "Epoch 101, batch 31/93, loss: 0.0260\n",
      "Epoch 101, batch 36/93, loss: 0.0407\n",
      "Epoch 101, batch 41/93, loss: 0.0271\n",
      "Epoch 101, batch 46/93, loss: 0.0857\n",
      "Epoch 101, batch 51/93, loss: 0.0199\n",
      "Epoch 101, batch 56/93, loss: 0.0341\n",
      "Epoch 101, batch 61/93, loss: 0.0429\n",
      "Epoch 101, batch 66/93, loss: 0.0516\n",
      "Epoch 101, batch 71/93, loss: 0.0964\n",
      "Epoch 101, batch 76/93, loss: 0.0650\n",
      "Epoch 101, batch 81/93, loss: 0.0299\n",
      "Epoch 101, batch 86/93, loss: 0.0732\n",
      "Epoch 101, batch 91/93, loss: 0.0165\n",
      "Epoch 101, train loss: 0.0552, train accuracy: 98.38%\n",
      "Epoch 101, val loss: 0.7032, val accuracy: 85.76%\n",
      "Epoch 102, batch 1/93, loss: 0.0249\n",
      "Epoch 102, batch 6/93, loss: 0.0514\n",
      "Epoch 102, batch 11/93, loss: 0.0905\n",
      "Epoch 102, batch 16/93, loss: 0.0525\n",
      "Epoch 102, batch 21/93, loss: 0.0612\n",
      "Epoch 102, batch 26/93, loss: 0.0749\n",
      "Epoch 102, batch 31/93, loss: 0.0319\n",
      "Epoch 102, batch 36/93, loss: 0.0773\n",
      "Epoch 102, batch 41/93, loss: 0.0447\n",
      "Epoch 102, batch 46/93, loss: 0.1402\n",
      "Epoch 102, batch 51/93, loss: 0.0860\n",
      "Epoch 102, batch 56/93, loss: 0.0296\n",
      "Epoch 102, batch 61/93, loss: 0.0132\n",
      "Epoch 102, batch 66/93, loss: 0.0413\n",
      "Epoch 102, batch 71/93, loss: 0.0296\n",
      "Epoch 102, batch 76/93, loss: 0.0102\n",
      "Epoch 102, batch 81/93, loss: 0.0167\n",
      "Epoch 102, batch 86/93, loss: 0.0482\n",
      "Epoch 102, batch 91/93, loss: 0.0648\n",
      "Epoch 102, train loss: 0.0437, train accuracy: 98.85%\n",
      "Epoch 102, val loss: 0.8369, val accuracy: 85.20%\n",
      "Epoch 103, batch 1/93, loss: 0.0522\n",
      "Epoch 103, batch 6/93, loss: 0.0243\n",
      "Epoch 103, batch 11/93, loss: 0.0447\n",
      "Epoch 103, batch 16/93, loss: 0.1055\n",
      "Epoch 103, batch 21/93, loss: 0.0252\n",
      "Epoch 103, batch 26/93, loss: 0.0446\n",
      "Epoch 103, batch 31/93, loss: 0.0121\n",
      "Epoch 103, batch 36/93, loss: 0.0186\n",
      "Epoch 103, batch 41/93, loss: 0.0109\n",
      "Epoch 103, batch 46/93, loss: 0.0366\n",
      "Epoch 103, batch 51/93, loss: 0.0259\n",
      "Epoch 103, batch 56/93, loss: 0.0225\n",
      "Epoch 103, batch 61/93, loss: 0.0286\n",
      "Epoch 103, batch 66/93, loss: 0.0391\n",
      "Epoch 103, batch 71/93, loss: 0.0135\n",
      "Epoch 103, batch 76/93, loss: 0.0261\n",
      "Epoch 103, batch 81/93, loss: 0.0309\n",
      "Epoch 103, batch 86/93, loss: 0.0193\n",
      "Epoch 103, batch 91/93, loss: 0.0258\n",
      "Epoch 103, train loss: 0.0311, train accuracy: 99.25%\n",
      "Epoch 103, val loss: 0.7834, val accuracy: 85.55%\n",
      "Epoch 104, batch 1/93, loss: 0.0118\n",
      "Epoch 104, batch 6/93, loss: 0.0541\n",
      "Epoch 104, batch 11/93, loss: 0.0227\n",
      "Epoch 104, batch 16/93, loss: 0.0251\n",
      "Epoch 104, batch 21/93, loss: 0.0228\n",
      "Epoch 104, batch 26/93, loss: 0.0337\n",
      "Epoch 104, batch 31/93, loss: 0.0341\n",
      "Epoch 104, batch 36/93, loss: 0.0181\n",
      "Epoch 104, batch 41/93, loss: 0.0181\n",
      "Epoch 104, batch 46/93, loss: 0.0525\n",
      "Epoch 104, batch 51/93, loss: 0.0288\n",
      "Epoch 104, batch 56/93, loss: 0.0178\n",
      "Epoch 104, batch 61/93, loss: 0.0356\n",
      "Epoch 104, batch 66/93, loss: 0.0215\n",
      "Epoch 104, batch 71/93, loss: 0.0368\n",
      "Epoch 104, batch 76/93, loss: 0.0413\n",
      "Epoch 104, batch 81/93, loss: 0.0513\n",
      "Epoch 104, batch 86/93, loss: 0.0492\n",
      "Epoch 104, batch 91/93, loss: 0.0534\n",
      "Epoch 104, train loss: 0.0337, train accuracy: 99.13%\n",
      "Epoch 104, val loss: 0.7577, val accuracy: 85.50%\n",
      "Epoch 105, batch 1/93, loss: 0.0154\n",
      "Epoch 105, batch 6/93, loss: 0.0435\n",
      "Epoch 105, batch 11/93, loss: 0.0475\n",
      "Epoch 105, batch 16/93, loss: 0.0163\n",
      "Epoch 105, batch 21/93, loss: 0.0142\n",
      "Epoch 105, batch 26/93, loss: 0.0273\n",
      "Epoch 105, batch 31/93, loss: 0.0460\n",
      "Epoch 105, batch 36/93, loss: 0.0175\n",
      "Epoch 105, batch 41/93, loss: 0.0316\n",
      "Epoch 105, batch 46/93, loss: 0.0151\n",
      "Epoch 105, batch 51/93, loss: 0.0187\n",
      "Epoch 105, batch 56/93, loss: 0.0749\n",
      "Epoch 105, batch 61/93, loss: 0.0116\n",
      "Epoch 105, batch 66/93, loss: 0.0220\n",
      "Epoch 105, batch 71/93, loss: 0.0347\n",
      "Epoch 105, batch 76/93, loss: 0.0202\n",
      "Epoch 105, batch 81/93, loss: 0.0165\n",
      "Epoch 105, batch 86/93, loss: 0.0200\n",
      "Epoch 105, batch 91/93, loss: 0.0676\n",
      "Epoch 105, train loss: 0.0370, train accuracy: 99.15%\n",
      "Epoch 105, val loss: 0.7015, val accuracy: 85.29%\n",
      "Epoch 106, batch 1/93, loss: 0.0303\n",
      "Epoch 106, batch 6/93, loss: 0.1578\n",
      "Epoch 106, batch 11/93, loss: 0.0834\n",
      "Epoch 106, batch 16/93, loss: 0.1025\n",
      "Epoch 106, batch 21/93, loss: 0.0283\n",
      "Epoch 106, batch 26/93, loss: 0.1081\n",
      "Epoch 106, batch 31/93, loss: 0.0504\n",
      "Epoch 106, batch 36/93, loss: 0.0279\n",
      "Epoch 106, batch 41/93, loss: 0.0460\n",
      "Epoch 106, batch 46/93, loss: 0.0308\n",
      "Epoch 106, batch 51/93, loss: 0.0924\n",
      "Epoch 106, batch 56/93, loss: 0.0427\n",
      "Epoch 106, batch 61/93, loss: 0.0590\n",
      "Epoch 106, batch 66/93, loss: 0.0291\n",
      "Epoch 106, batch 71/93, loss: 0.0878\n",
      "Epoch 106, batch 76/93, loss: 0.0385\n",
      "Epoch 106, batch 81/93, loss: 0.0298\n",
      "Epoch 106, batch 86/93, loss: 0.0396\n",
      "Epoch 106, batch 91/93, loss: 0.0345\n",
      "Epoch 106, train loss: 0.0629, train accuracy: 97.94%\n",
      "Epoch 106, val loss: 0.7600, val accuracy: 84.94%\n",
      "Epoch 107, batch 1/93, loss: 0.0380\n",
      "Epoch 107, batch 6/93, loss: 0.1181\n",
      "Epoch 107, batch 11/93, loss: 0.0172\n",
      "Epoch 107, batch 16/93, loss: 0.0416\n",
      "Epoch 107, batch 21/93, loss: 0.0676\n",
      "Epoch 107, batch 26/93, loss: 0.0907\n",
      "Epoch 107, batch 31/93, loss: 0.0215\n",
      "Epoch 107, batch 36/93, loss: 0.0446\n",
      "Epoch 107, batch 41/93, loss: 0.0244\n",
      "Epoch 107, batch 46/93, loss: 0.0380\n",
      "Epoch 107, batch 51/93, loss: 0.1474\n",
      "Epoch 107, batch 56/93, loss: 0.0154\n",
      "Epoch 107, batch 61/93, loss: 0.0219\n",
      "Epoch 107, batch 66/93, loss: 0.0457\n",
      "Epoch 107, batch 71/93, loss: 0.0327\n",
      "Epoch 107, batch 76/93, loss: 0.0307\n",
      "Epoch 107, batch 81/93, loss: 0.0213\n",
      "Epoch 107, batch 86/93, loss: 0.0450\n",
      "Epoch 107, batch 91/93, loss: 0.0429\n",
      "Epoch 107, train loss: 0.0467, train accuracy: 98.64%\n",
      "Epoch 107, val loss: 0.7877, val accuracy: 84.85%\n",
      "Epoch 108, batch 1/93, loss: 0.0552\n",
      "Epoch 108, batch 6/93, loss: 0.0746\n",
      "Epoch 108, batch 11/93, loss: 0.0624\n",
      "Epoch 108, batch 16/93, loss: 0.0335\n",
      "Epoch 108, batch 21/93, loss: 0.0806\n",
      "Epoch 108, batch 26/93, loss: 0.0392\n",
      "Epoch 108, batch 31/93, loss: 0.0238\n",
      "Epoch 108, batch 36/93, loss: 0.0451\n",
      "Epoch 108, batch 41/93, loss: 0.0555\n",
      "Epoch 108, batch 46/93, loss: 0.0314\n",
      "Epoch 108, batch 51/93, loss: 0.0788\n",
      "Epoch 108, batch 56/93, loss: 0.0352\n",
      "Epoch 108, batch 61/93, loss: 0.0130\n",
      "Epoch 108, batch 66/93, loss: 0.0662\n",
      "Epoch 108, batch 71/93, loss: 0.0099\n",
      "Epoch 108, batch 76/93, loss: 0.0468\n",
      "Epoch 108, batch 81/93, loss: 0.0106\n",
      "Epoch 108, batch 86/93, loss: 0.0146\n",
      "Epoch 108, batch 91/93, loss: 0.0517\n",
      "Epoch 108, train loss: 0.0460, train accuracy: 98.64%\n",
      "Epoch 108, val loss: 0.7714, val accuracy: 84.98%\n",
      "Epoch 109, batch 1/93, loss: 0.0322\n",
      "Epoch 109, batch 6/93, loss: 0.0326\n",
      "Epoch 109, batch 11/93, loss: 0.0690\n",
      "Epoch 109, batch 16/93, loss: 0.0424\n",
      "Epoch 109, batch 21/93, loss: 0.0115\n",
      "Epoch 109, batch 26/93, loss: 0.0194\n",
      "Epoch 109, batch 31/93, loss: 0.0271\n",
      "Epoch 109, batch 36/93, loss: 0.0226\n",
      "Epoch 109, batch 41/93, loss: 0.0195\n",
      "Epoch 109, batch 46/93, loss: 0.0195\n",
      "Epoch 109, batch 51/93, loss: 0.0413\n",
      "Epoch 109, batch 56/93, loss: 0.0141\n",
      "Epoch 109, batch 61/93, loss: 0.0247\n",
      "Epoch 109, batch 66/93, loss: 0.0322\n",
      "Epoch 109, batch 71/93, loss: 0.0054\n",
      "Epoch 109, batch 76/93, loss: 0.0240\n",
      "Epoch 109, batch 81/93, loss: 0.0458\n",
      "Epoch 109, batch 86/93, loss: 0.0270\n",
      "Epoch 109, batch 91/93, loss: 0.0108\n",
      "Epoch 109, train loss: 0.0286, train accuracy: 99.26%\n",
      "Epoch 109, val loss: 0.7554, val accuracy: 85.29%\n",
      "Epoch 110, batch 1/93, loss: 0.0105\n",
      "Epoch 110, batch 6/93, loss: 0.0103\n",
      "Epoch 110, batch 11/93, loss: 0.0239\n",
      "Epoch 110, batch 16/93, loss: 0.0265\n",
      "Epoch 110, batch 21/93, loss: 0.0169\n",
      "Epoch 110, batch 26/93, loss: 0.0217\n",
      "Epoch 110, batch 31/93, loss: 0.0204\n",
      "Epoch 110, batch 36/93, loss: 0.0354\n",
      "Epoch 110, batch 41/93, loss: 0.0550\n",
      "Epoch 110, batch 46/93, loss: 0.0080\n",
      "Epoch 110, batch 51/93, loss: 0.0352\n",
      "Epoch 110, batch 56/93, loss: 0.0214\n",
      "Epoch 110, batch 61/93, loss: 0.0505\n",
      "Epoch 110, batch 66/93, loss: 0.0180\n",
      "Epoch 110, batch 71/93, loss: 0.0282\n",
      "Epoch 110, batch 76/93, loss: 0.0116\n",
      "Epoch 110, batch 81/93, loss: 0.0586\n",
      "Epoch 110, batch 86/93, loss: 0.0207\n",
      "Epoch 110, batch 91/93, loss: 0.0154\n",
      "Epoch 110, train loss: 0.0334, train accuracy: 99.13%\n",
      "Epoch 110, val loss: 0.7201, val accuracy: 83.72%\n",
      "Epoch 111, batch 1/93, loss: 0.0205\n",
      "Epoch 111, batch 6/93, loss: 0.1471\n",
      "Epoch 111, batch 11/93, loss: 0.1684\n",
      "Epoch 111, batch 16/93, loss: 0.0628\n",
      "Epoch 111, batch 21/93, loss: 0.0328\n",
      "Epoch 111, batch 26/93, loss: 0.0334\n",
      "Epoch 111, batch 31/93, loss: 0.0413\n",
      "Epoch 111, batch 36/93, loss: 0.0568\n",
      "Epoch 111, batch 41/93, loss: 0.0491\n",
      "Epoch 111, batch 46/93, loss: 0.0682\n",
      "Epoch 111, batch 51/93, loss: 0.1149\n",
      "Epoch 111, batch 56/93, loss: 0.0618\n",
      "Epoch 111, batch 61/93, loss: 0.0310\n",
      "Epoch 111, batch 66/93, loss: 0.0192\n",
      "Epoch 111, batch 71/93, loss: 0.0866\n",
      "Epoch 111, batch 76/93, loss: 0.0253\n",
      "Epoch 111, batch 81/93, loss: 0.0701\n",
      "Epoch 111, batch 86/93, loss: 0.0331\n",
      "Epoch 111, batch 91/93, loss: 0.0354\n",
      "Epoch 111, train loss: 0.0674, train accuracy: 97.81%\n",
      "Epoch 111, val loss: 0.7712, val accuracy: 84.68%\n",
      "Epoch 112, batch 1/93, loss: 0.0184\n",
      "Epoch 112, batch 6/93, loss: 0.0229\n",
      "Epoch 112, batch 11/93, loss: 0.0489\n",
      "Epoch 112, batch 16/93, loss: 0.0224\n",
      "Epoch 112, batch 21/93, loss: 0.0458\n",
      "Epoch 112, batch 26/93, loss: 0.0425\n",
      "Epoch 112, batch 31/93, loss: 0.0140\n",
      "Epoch 112, batch 36/93, loss: 0.0106\n",
      "Epoch 112, batch 41/93, loss: 0.0254\n",
      "Epoch 112, batch 46/93, loss: 0.0253\n",
      "Epoch 112, batch 51/93, loss: 0.0194\n",
      "Epoch 112, batch 56/93, loss: 0.0113\n",
      "Epoch 112, batch 61/93, loss: 0.0656\n",
      "Epoch 112, batch 66/93, loss: 0.0055\n",
      "Epoch 112, batch 71/93, loss: 0.0268\n",
      "Epoch 112, batch 76/93, loss: 0.0130\n",
      "Epoch 112, batch 81/93, loss: 0.0216\n",
      "Epoch 112, batch 86/93, loss: 0.0298\n",
      "Epoch 112, batch 91/93, loss: 0.0175\n",
      "Epoch 112, train loss: 0.0391, train accuracy: 99.13%\n",
      "Epoch 112, val loss: 0.7678, val accuracy: 84.68%\n",
      "Epoch 113, batch 1/93, loss: 0.0611\n",
      "Epoch 113, batch 6/93, loss: 0.1458\n",
      "Epoch 113, batch 11/93, loss: 0.2119\n",
      "Epoch 113, batch 16/93, loss: 0.1809\n",
      "Epoch 113, batch 21/93, loss: 0.1896\n",
      "Epoch 113, batch 26/93, loss: 0.1845\n",
      "Epoch 113, batch 31/93, loss: 0.1662\n",
      "Epoch 113, batch 36/93, loss: 0.1128\n",
      "Epoch 113, batch 41/93, loss: 0.0322\n",
      "Epoch 113, batch 46/93, loss: 0.1168\n",
      "Epoch 113, batch 51/93, loss: 0.1953\n",
      "Epoch 113, batch 56/93, loss: 0.0361\n",
      "Epoch 113, batch 61/93, loss: 0.0157\n",
      "Epoch 113, batch 66/93, loss: 0.0782\n",
      "Epoch 113, batch 71/93, loss: 0.0473\n",
      "Epoch 113, batch 76/93, loss: 0.0209\n",
      "Epoch 113, batch 81/93, loss: 0.0514\n",
      "Epoch 113, batch 86/93, loss: 0.0458\n",
      "Epoch 113, batch 91/93, loss: 0.0307\n",
      "Epoch 113, train loss: 0.1092, train accuracy: 96.60%\n",
      "Epoch 113, val loss: 0.7724, val accuracy: 84.11%\n",
      "Epoch 114, batch 1/93, loss: 0.0263\n",
      "Epoch 114, batch 6/93, loss: 0.1040\n",
      "Epoch 114, batch 11/93, loss: 0.0963\n",
      "Epoch 114, batch 16/93, loss: 0.0524\n",
      "Epoch 114, batch 21/93, loss: 0.1247\n",
      "Epoch 114, batch 26/93, loss: 0.0753\n",
      "Epoch 114, batch 31/93, loss: 0.0247\n",
      "Epoch 114, batch 36/93, loss: 0.0684\n",
      "Epoch 114, batch 41/93, loss: 0.0827\n",
      "Epoch 114, batch 46/93, loss: 0.0925\n",
      "Epoch 114, batch 51/93, loss: 0.0416\n",
      "Epoch 114, batch 56/93, loss: 0.0583\n",
      "Epoch 114, batch 61/93, loss: 0.0309\n",
      "Epoch 114, batch 66/93, loss: 0.0730\n",
      "Epoch 114, batch 71/93, loss: 0.0280\n",
      "Epoch 114, batch 76/93, loss: 0.0894\n",
      "Epoch 114, batch 81/93, loss: 0.0181\n",
      "Epoch 114, batch 86/93, loss: 0.0260\n",
      "Epoch 114, batch 91/93, loss: 0.0227\n",
      "Epoch 114, train loss: 0.0666, train accuracy: 97.94%\n",
      "Epoch 114, val loss: 0.6834, val accuracy: 84.03%\n",
      "Epoch 115, batch 1/93, loss: 0.0359\n",
      "Epoch 115, batch 6/93, loss: 0.0698\n",
      "Epoch 115, batch 11/93, loss: 0.0552\n",
      "Epoch 115, batch 16/93, loss: 0.0422\n",
      "Epoch 115, batch 21/93, loss: 0.0415\n",
      "Epoch 115, batch 26/93, loss: 0.0438\n",
      "Epoch 115, batch 31/93, loss: 0.0346\n",
      "Epoch 115, batch 36/93, loss: 0.0605\n",
      "Epoch 115, batch 41/93, loss: 0.1253\n",
      "Epoch 115, batch 46/93, loss: 0.0232\n",
      "Epoch 115, batch 51/93, loss: 0.0549\n",
      "Epoch 115, batch 56/93, loss: 0.0132\n",
      "Epoch 115, batch 61/93, loss: 0.0229\n",
      "Epoch 115, batch 66/93, loss: 0.0306\n",
      "Epoch 115, batch 71/93, loss: 0.0342\n",
      "Epoch 115, batch 76/93, loss: 0.0515\n",
      "Epoch 115, batch 81/93, loss: 0.0311\n",
      "Epoch 115, batch 86/93, loss: 0.0410\n",
      "Epoch 115, batch 91/93, loss: 0.0378\n",
      "Epoch 115, train loss: 0.0631, train accuracy: 98.51%\n",
      "Epoch 115, val loss: 0.6946, val accuracy: 84.85%\n",
      "Epoch 116, batch 1/93, loss: 0.0288\n",
      "Epoch 116, batch 6/93, loss: 0.1561\n",
      "Epoch 116, batch 11/93, loss: 0.1151\n",
      "Epoch 116, batch 16/93, loss: 0.0659\n",
      "Epoch 116, batch 21/93, loss: 0.3093\n",
      "Epoch 116, batch 26/93, loss: 0.1750\n",
      "Epoch 116, batch 31/93, loss: 0.1389\n",
      "Epoch 116, batch 36/93, loss: 0.1288\n",
      "Epoch 116, batch 41/93, loss: 0.1039\n",
      "Epoch 116, batch 46/93, loss: 0.1056\n",
      "Epoch 116, batch 51/93, loss: 0.0733\n",
      "Epoch 116, batch 56/93, loss: 0.0499\n",
      "Epoch 116, batch 61/93, loss: 0.1046\n",
      "Epoch 116, batch 66/93, loss: 0.0406\n",
      "Epoch 116, batch 71/93, loss: 0.0732\n",
      "Epoch 116, batch 76/93, loss: 0.0357\n",
      "Epoch 116, batch 81/93, loss: 0.0540\n",
      "Epoch 116, batch 86/93, loss: 0.0424\n",
      "Epoch 116, batch 91/93, loss: 0.0310\n",
      "Epoch 116, train loss: 0.0816, train accuracy: 97.14%\n",
      "Epoch 116, val loss: 0.7559, val accuracy: 85.89%\n",
      "Epoch 117, batch 1/93, loss: 0.0123\n",
      "Epoch 117, batch 6/93, loss: 0.0769\n",
      "Epoch 117, batch 11/93, loss: 0.0420\n",
      "Epoch 117, batch 16/93, loss: 0.0357\n",
      "Epoch 117, batch 21/93, loss: 0.1361\n",
      "Epoch 117, batch 26/93, loss: 0.0274\n",
      "Epoch 117, batch 31/93, loss: 0.0612\n",
      "Epoch 117, batch 36/93, loss: 0.0249\n",
      "Epoch 117, batch 41/93, loss: 0.0299\n",
      "Epoch 117, batch 46/93, loss: 0.0233\n",
      "Epoch 117, batch 51/93, loss: 0.0623\n",
      "Epoch 117, batch 56/93, loss: 0.0698\n",
      "Epoch 117, batch 61/93, loss: 0.0235\n",
      "Epoch 117, batch 66/93, loss: 0.0318\n",
      "Epoch 117, batch 71/93, loss: 0.0118\n",
      "Epoch 117, batch 76/93, loss: 0.0187\n",
      "Epoch 117, batch 81/93, loss: 0.0319\n",
      "Epoch 117, batch 86/93, loss: 0.0388\n",
      "Epoch 117, batch 91/93, loss: 0.0355\n",
      "Epoch 117, train loss: 0.0411, train accuracy: 99.01%\n",
      "Epoch 117, val loss: 0.6892, val accuracy: 85.68%\n",
      "Epoch 118, batch 1/93, loss: 0.0198\n",
      "Epoch 118, batch 6/93, loss: 0.0208\n",
      "Epoch 118, batch 11/93, loss: 0.0474\n",
      "Epoch 118, batch 16/93, loss: 0.0714\n",
      "Epoch 118, batch 21/93, loss: 0.0391\n",
      "Epoch 118, batch 26/93, loss: 0.0814\n",
      "Epoch 118, batch 31/93, loss: 0.0862\n",
      "Epoch 118, batch 36/93, loss: 0.0123\n",
      "Epoch 118, batch 41/93, loss: 0.0508\n",
      "Epoch 118, batch 46/93, loss: 0.0121\n",
      "Epoch 118, batch 51/93, loss: 0.0136\n",
      "Epoch 118, batch 56/93, loss: 0.0258\n",
      "Epoch 118, batch 61/93, loss: 0.0511\n",
      "Epoch 118, batch 66/93, loss: 0.0363\n",
      "Epoch 118, batch 71/93, loss: 0.0274\n",
      "Epoch 118, batch 76/93, loss: 0.0179\n",
      "Epoch 118, batch 81/93, loss: 0.0462\n",
      "Epoch 118, batch 86/93, loss: 0.0448\n",
      "Epoch 118, batch 91/93, loss: 0.0291\n",
      "Epoch 118, train loss: 0.0342, train accuracy: 98.95%\n",
      "Epoch 118, val loss: 0.7576, val accuracy: 86.07%\n",
      "Epoch 119, batch 1/93, loss: 0.0052\n",
      "Epoch 119, batch 6/93, loss: 0.0104\n",
      "Epoch 119, batch 11/93, loss: 0.0091\n",
      "Epoch 119, batch 16/93, loss: 0.0233\n",
      "Epoch 119, batch 21/93, loss: 0.0129\n",
      "Epoch 119, batch 26/93, loss: 0.0065\n",
      "Epoch 119, batch 31/93, loss: 0.0229\n",
      "Epoch 119, batch 36/93, loss: 0.0993\n",
      "Epoch 119, batch 41/93, loss: 0.0370\n",
      "Epoch 119, batch 46/93, loss: 0.0335\n",
      "Epoch 119, batch 51/93, loss: 0.0110\n",
      "Epoch 119, batch 56/93, loss: 0.0095\n",
      "Epoch 119, batch 61/93, loss: 0.0189\n",
      "Epoch 119, batch 66/93, loss: 0.0062\n",
      "Epoch 119, batch 71/93, loss: 0.0128\n",
      "Epoch 119, batch 76/93, loss: 0.0223\n",
      "Epoch 119, batch 81/93, loss: 0.0147\n",
      "Epoch 119, batch 86/93, loss: 0.0161\n",
      "Epoch 119, batch 91/93, loss: 0.0361\n",
      "Epoch 119, train loss: 0.0244, train accuracy: 99.36%\n",
      "Epoch 119, val loss: 0.7720, val accuracy: 86.68%\n",
      "Epoch 120, batch 1/93, loss: 0.0187\n",
      "Epoch 120, batch 6/93, loss: 0.0492\n",
      "Epoch 120, batch 11/93, loss: 0.0234\n",
      "Epoch 120, batch 16/93, loss: 0.0154\n",
      "Epoch 120, batch 21/93, loss: 0.0092\n",
      "Epoch 120, batch 26/93, loss: 0.0173\n",
      "Epoch 120, batch 31/93, loss: 0.0146\n",
      "Epoch 120, batch 36/93, loss: 0.0070\n",
      "Epoch 120, batch 41/93, loss: 0.0338\n",
      "Epoch 120, batch 46/93, loss: 0.0138\n",
      "Epoch 120, batch 51/93, loss: 0.0202\n",
      "Epoch 120, batch 56/93, loss: 0.0228\n",
      "Epoch 120, batch 61/93, loss: 0.0204\n",
      "Epoch 120, batch 66/93, loss: 0.0224\n",
      "Epoch 120, batch 71/93, loss: 0.0100\n",
      "Epoch 120, batch 76/93, loss: 0.0103\n",
      "Epoch 120, batch 81/93, loss: 0.0375\n",
      "Epoch 120, batch 86/93, loss: 0.0277\n",
      "Epoch 120, batch 91/93, loss: 0.0045\n",
      "Epoch 120, train loss: 0.0271, train accuracy: 99.32%\n",
      "Epoch 120, val loss: 0.7299, val accuracy: 84.85%\n",
      "Epoch 121, batch 1/93, loss: 0.0203\n",
      "Epoch 121, batch 6/93, loss: 0.0776\n",
      "Epoch 121, batch 11/93, loss: 0.0415\n",
      "Epoch 121, batch 16/93, loss: 0.1411\n",
      "Epoch 121, batch 21/93, loss: 0.0405\n",
      "Epoch 121, batch 26/93, loss: 0.0212\n",
      "Epoch 121, batch 31/93, loss: 0.0159\n",
      "Epoch 121, batch 36/93, loss: 0.0202\n",
      "Epoch 121, batch 41/93, loss: 0.0142\n",
      "Epoch 121, batch 46/93, loss: 0.0278\n",
      "Epoch 121, batch 51/93, loss: 0.0095\n",
      "Epoch 121, batch 56/93, loss: 0.0117\n",
      "Epoch 121, batch 61/93, loss: 0.0306\n",
      "Epoch 121, batch 66/93, loss: 0.0291\n",
      "Epoch 121, batch 71/93, loss: 0.0739\n",
      "Epoch 121, batch 76/93, loss: 0.0158\n",
      "Epoch 121, batch 81/93, loss: 0.0195\n",
      "Epoch 121, batch 86/93, loss: 0.0637\n",
      "Epoch 121, batch 91/93, loss: 0.0264\n",
      "Epoch 121, train loss: 0.0418, train accuracy: 98.82%\n",
      "Epoch 121, val loss: 0.7946, val accuracy: 85.20%\n",
      "Epoch 122, batch 1/93, loss: 0.0248\n",
      "Epoch 122, batch 6/93, loss: 0.0130\n",
      "Epoch 122, batch 11/93, loss: 0.0051\n",
      "Epoch 122, batch 16/93, loss: 0.0285\n",
      "Epoch 122, batch 21/93, loss: 0.0396\n",
      "Epoch 122, batch 26/93, loss: 0.0244\n",
      "Epoch 122, batch 31/93, loss: 0.0251\n",
      "Epoch 122, batch 36/93, loss: 0.0058\n",
      "Epoch 122, batch 41/93, loss: 0.0322\n",
      "Epoch 122, batch 46/93, loss: 0.0127\n",
      "Epoch 122, batch 51/93, loss: 0.0227\n",
      "Epoch 122, batch 56/93, loss: 0.0182\n",
      "Epoch 122, batch 61/93, loss: 0.0159\n",
      "Epoch 122, batch 66/93, loss: 0.0116\n",
      "Epoch 122, batch 71/93, loss: 0.0310\n",
      "Epoch 122, batch 76/93, loss: 0.0234\n",
      "Epoch 122, batch 81/93, loss: 0.0075\n",
      "Epoch 122, batch 86/93, loss: 0.0130\n",
      "Epoch 122, batch 91/93, loss: 0.0372\n",
      "Epoch 122, train loss: 0.0248, train accuracy: 99.41%\n",
      "Epoch 122, val loss: 0.7354, val accuracy: 85.55%\n",
      "Epoch 123, batch 1/93, loss: 0.0074\n",
      "Epoch 123, batch 6/93, loss: 0.0164\n",
      "Epoch 123, batch 11/93, loss: 0.0347\n",
      "Epoch 123, batch 16/93, loss: 0.0151\n",
      "Epoch 123, batch 21/93, loss: 0.0085\n",
      "Epoch 123, batch 26/93, loss: 0.0120\n",
      "Epoch 123, batch 31/93, loss: 0.0314\n",
      "Epoch 123, batch 36/93, loss: 0.0124\n",
      "Epoch 123, batch 41/93, loss: 0.0244\n",
      "Epoch 123, batch 46/93, loss: 0.0148\n",
      "Epoch 123, batch 51/93, loss: 0.0285\n",
      "Epoch 123, batch 56/93, loss: 0.0199\n",
      "Epoch 123, batch 61/93, loss: 0.0243\n",
      "Epoch 123, batch 66/93, loss: 0.0229\n",
      "Epoch 123, batch 71/93, loss: 0.0159\n",
      "Epoch 123, batch 76/93, loss: 0.0222\n",
      "Epoch 123, batch 81/93, loss: 0.0078\n",
      "Epoch 123, batch 86/93, loss: 0.0253\n",
      "Epoch 123, batch 91/93, loss: 0.0755\n",
      "Epoch 123, train loss: 0.0230, train accuracy: 99.41%\n",
      "Epoch 123, val loss: 0.7390, val accuracy: 86.72%\n",
      "Epoch 124, batch 1/93, loss: 0.0323\n",
      "Epoch 124, batch 6/93, loss: 0.0169\n",
      "Epoch 124, batch 11/93, loss: 0.0046\n",
      "Epoch 124, batch 16/93, loss: 0.0290\n",
      "Epoch 124, batch 21/93, loss: 0.0126\n",
      "Epoch 124, batch 26/93, loss: 0.0457\n",
      "Epoch 124, batch 31/93, loss: 0.0060\n",
      "Epoch 124, batch 36/93, loss: 0.0234\n",
      "Epoch 124, batch 41/93, loss: 0.0265\n",
      "Epoch 124, batch 46/93, loss: 0.0078\n",
      "Epoch 124, batch 51/93, loss: 0.0208\n",
      "Epoch 124, batch 56/93, loss: 0.0298\n",
      "Epoch 124, batch 61/93, loss: 0.0939\n",
      "Epoch 124, batch 66/93, loss: 0.0044\n",
      "Epoch 124, batch 71/93, loss: 0.0121\n",
      "Epoch 124, batch 76/93, loss: 0.0406\n",
      "Epoch 124, batch 81/93, loss: 0.0089\n",
      "Epoch 124, batch 86/93, loss: 0.0330\n",
      "Epoch 124, batch 91/93, loss: 0.0177\n",
      "Epoch 124, train loss: 0.0259, train accuracy: 99.33%\n",
      "Epoch 124, val loss: 0.7554, val accuracy: 86.07%\n",
      "Epoch 125, batch 1/93, loss: 0.0076\n",
      "Epoch 125, batch 6/93, loss: 0.0188\n",
      "Epoch 125, batch 11/93, loss: 0.0393\n",
      "Epoch 125, batch 16/93, loss: 0.0819\n",
      "Epoch 125, batch 21/93, loss: 0.0446\n",
      "Epoch 125, batch 26/93, loss: 0.0830\n",
      "Epoch 125, batch 31/93, loss: 0.0123\n",
      "Epoch 125, batch 36/93, loss: 0.0113\n",
      "Epoch 125, batch 41/93, loss: 0.0314\n",
      "Epoch 125, batch 46/93, loss: 0.0614\n",
      "Epoch 125, batch 51/93, loss: 0.0370\n",
      "Epoch 125, batch 56/93, loss: 0.0157\n",
      "Epoch 125, batch 61/93, loss: 0.0241\n",
      "Epoch 125, batch 66/93, loss: 0.0159\n",
      "Epoch 125, batch 71/93, loss: 0.0179\n",
      "Epoch 125, batch 76/93, loss: 0.0145\n",
      "Epoch 125, batch 81/93, loss: 0.0103\n",
      "Epoch 125, batch 86/93, loss: 0.0085\n",
      "Epoch 125, batch 91/93, loss: 0.0069\n",
      "Epoch 125, train loss: 0.0332, train accuracy: 99.15%\n",
      "Epoch 125, val loss: 0.7221, val accuracy: 85.59%\n",
      "Epoch 126, batch 1/93, loss: 0.0067\n",
      "Epoch 126, batch 6/93, loss: 0.0745\n",
      "Epoch 126, batch 11/93, loss: 0.0207\n",
      "Epoch 126, batch 16/93, loss: 0.0107\n",
      "Epoch 126, batch 21/93, loss: 0.0792\n",
      "Epoch 126, batch 26/93, loss: 0.0058\n",
      "Epoch 126, batch 31/93, loss: 0.0367\n",
      "Epoch 126, batch 36/93, loss: 0.0284\n",
      "Epoch 126, batch 41/93, loss: 0.0199\n",
      "Epoch 126, batch 46/93, loss: 0.0327\n",
      "Epoch 126, batch 51/93, loss: 0.0137\n",
      "Epoch 126, batch 56/93, loss: 0.0341\n",
      "Epoch 126, batch 61/93, loss: 0.0118\n",
      "Epoch 126, batch 66/93, loss: 0.0225\n",
      "Epoch 126, batch 71/93, loss: 0.0138\n",
      "Epoch 126, batch 76/93, loss: 0.0152\n",
      "Epoch 126, batch 81/93, loss: 0.0410\n",
      "Epoch 126, batch 86/93, loss: 0.0145\n",
      "Epoch 126, batch 91/93, loss: 0.0102\n",
      "Epoch 126, train loss: 0.0295, train accuracy: 99.23%\n",
      "Epoch 126, val loss: 0.7051, val accuracy: 85.68%\n",
      "Epoch 127, batch 1/93, loss: 0.0376\n",
      "Epoch 127, batch 6/93, loss: 0.0051\n",
      "Epoch 127, batch 11/93, loss: 0.0223\n",
      "Epoch 127, batch 16/93, loss: 0.0207\n",
      "Epoch 127, batch 21/93, loss: 0.0178\n",
      "Epoch 127, batch 26/93, loss: 0.0058\n",
      "Epoch 127, batch 31/93, loss: 0.0106\n",
      "Epoch 127, batch 36/93, loss: 0.0211\n",
      "Epoch 127, batch 41/93, loss: 0.0608\n",
      "Epoch 127, batch 46/93, loss: 0.0156\n",
      "Epoch 127, batch 51/93, loss: 0.0095\n",
      "Epoch 127, batch 56/93, loss: 0.0072\n",
      "Epoch 127, batch 61/93, loss: 0.0131\n",
      "Epoch 127, batch 66/93, loss: 0.0083\n",
      "Epoch 127, batch 71/93, loss: 0.0311\n",
      "Epoch 127, batch 76/93, loss: 0.0052\n",
      "Epoch 127, batch 81/93, loss: 0.0854\n",
      "Epoch 127, batch 86/93, loss: 0.0121\n",
      "Epoch 127, batch 91/93, loss: 0.0312\n",
      "Epoch 127, train loss: 0.0311, train accuracy: 99.56%\n",
      "Epoch 127, val loss: 0.7865, val accuracy: 84.90%\n",
      "Epoch 128, batch 1/93, loss: 0.0109\n",
      "Epoch 128, batch 6/93, loss: 0.1846\n",
      "Epoch 128, batch 11/93, loss: 0.2019\n",
      "Epoch 128, batch 16/93, loss: 0.0912\n",
      "Epoch 128, batch 21/93, loss: 0.0810\n",
      "Epoch 128, batch 26/93, loss: 0.1261\n",
      "Epoch 128, batch 31/93, loss: 0.1157\n",
      "Epoch 128, batch 36/93, loss: 0.0760\n",
      "Epoch 128, batch 41/93, loss: 0.0389\n",
      "Epoch 128, batch 46/93, loss: 0.0605\n",
      "Epoch 128, batch 51/93, loss: 0.1607\n",
      "Epoch 128, batch 56/93, loss: 0.0387\n",
      "Epoch 128, batch 61/93, loss: 0.0494\n",
      "Epoch 128, batch 66/93, loss: 0.0465\n",
      "Epoch 128, batch 71/93, loss: 0.0284\n",
      "Epoch 128, batch 76/93, loss: 0.0182\n",
      "Epoch 128, batch 81/93, loss: 0.0202\n",
      "Epoch 128, batch 86/93, loss: 0.0362\n",
      "Epoch 128, batch 91/93, loss: 0.0334\n",
      "Epoch 128, train loss: 0.0832, train accuracy: 97.47%\n",
      "Epoch 128, val loss: 0.8011, val accuracy: 85.37%\n",
      "Epoch 129, batch 1/93, loss: 0.0211\n",
      "Epoch 129, batch 6/93, loss: 0.6801\n",
      "Epoch 129, batch 11/93, loss: 0.1282\n",
      "Epoch 129, batch 16/93, loss: 0.0957\n",
      "Epoch 129, batch 21/93, loss: 0.0343\n",
      "Epoch 129, batch 26/93, loss: 0.0647\n",
      "Epoch 129, batch 31/93, loss: 0.0420\n",
      "Epoch 129, batch 36/93, loss: 0.0575\n",
      "Epoch 129, batch 41/93, loss: 0.0407\n",
      "Epoch 129, batch 46/93, loss: 0.0746\n",
      "Epoch 129, batch 51/93, loss: 0.0410\n",
      "Epoch 129, batch 56/93, loss: 0.0422\n",
      "Epoch 129, batch 61/93, loss: 0.0149\n",
      "Epoch 129, batch 66/93, loss: 0.0322\n",
      "Epoch 129, batch 71/93, loss: 0.0107\n",
      "Epoch 129, batch 76/93, loss: 0.0357\n",
      "Epoch 129, batch 81/93, loss: 0.0086\n",
      "Epoch 129, batch 86/93, loss: 0.0904\n",
      "Epoch 129, batch 91/93, loss: 0.0105\n",
      "Epoch 129, train loss: 0.0736, train accuracy: 97.81%\n",
      "Epoch 129, val loss: 0.7580, val accuracy: 85.46%\n",
      "Epoch 130, batch 1/93, loss: 0.0512\n",
      "Epoch 130, batch 6/93, loss: 0.0387\n",
      "Epoch 130, batch 11/93, loss: 0.0230\n",
      "Epoch 130, batch 16/93, loss: 0.0424\n",
      "Epoch 130, batch 21/93, loss: 0.0329\n",
      "Epoch 130, batch 26/93, loss: 0.0065\n",
      "Epoch 130, batch 31/93, loss: 0.0812\n",
      "Epoch 130, batch 36/93, loss: 0.0350\n",
      "Epoch 130, batch 41/93, loss: 0.0210\n",
      "Epoch 130, batch 46/93, loss: 0.0338\n",
      "Epoch 130, batch 51/93, loss: 0.0197\n",
      "Epoch 130, batch 56/93, loss: 0.0283\n",
      "Epoch 130, batch 61/93, loss: 0.0233\n",
      "Epoch 130, batch 66/93, loss: 0.0316\n",
      "Epoch 130, batch 71/93, loss: 0.0275\n",
      "Epoch 130, batch 76/93, loss: 0.0131\n",
      "Epoch 130, batch 81/93, loss: 0.0095\n",
      "Epoch 130, batch 86/93, loss: 0.0088\n",
      "Epoch 130, batch 91/93, loss: 0.0184\n",
      "Epoch 130, train loss: 0.0552, train accuracy: 98.83%\n",
      "Epoch 130, val loss: 0.8705, val accuracy: 83.25%\n",
      "Epoch 131, batch 1/93, loss: 0.0244\n",
      "Epoch 131, batch 6/93, loss: 0.2030\n",
      "Epoch 131, batch 11/93, loss: 0.1808\n",
      "Epoch 131, batch 16/93, loss: 0.1431\n",
      "Epoch 131, batch 21/93, loss: 0.1429\n",
      "Epoch 131, batch 26/93, loss: 0.0977\n",
      "Epoch 131, batch 31/93, loss: 0.1772\n",
      "Epoch 131, batch 36/93, loss: 0.0426\n",
      "Epoch 131, batch 41/93, loss: 0.0990\n",
      "Epoch 131, batch 46/93, loss: 0.1149\n",
      "Epoch 131, batch 51/93, loss: 0.1062\n",
      "Epoch 131, batch 56/93, loss: 0.1198\n",
      "Epoch 131, batch 61/93, loss: 0.1389\n",
      "Epoch 131, batch 66/93, loss: 0.0362\n",
      "Epoch 131, batch 71/93, loss: 0.0239\n",
      "Epoch 131, batch 76/93, loss: 0.0233\n",
      "Epoch 131, batch 81/93, loss: 0.0417\n",
      "Epoch 131, batch 86/93, loss: 0.0973\n",
      "Epoch 131, batch 91/93, loss: 0.0387\n",
      "Epoch 131, train loss: 0.1181, train accuracy: 96.17%\n",
      "Epoch 131, val loss: 0.7621, val accuracy: 83.55%\n",
      "Epoch 132, batch 1/93, loss: 0.0874\n",
      "Epoch 132, batch 6/93, loss: 0.0140\n",
      "Epoch 132, batch 11/93, loss: 0.0834\n",
      "Epoch 132, batch 16/93, loss: 0.0127\n",
      "Epoch 132, batch 21/93, loss: 0.0761\n",
      "Epoch 132, batch 26/93, loss: 0.0241\n",
      "Epoch 132, batch 31/93, loss: 0.0259\n",
      "Epoch 132, batch 36/93, loss: 0.0702\n",
      "Epoch 132, batch 41/93, loss: 0.0209\n",
      "Epoch 132, batch 46/93, loss: 0.0247\n",
      "Epoch 132, batch 51/93, loss: 0.0076\n",
      "Epoch 132, batch 56/93, loss: 0.0146\n",
      "Epoch 132, batch 61/93, loss: 0.0232\n",
      "Epoch 132, batch 66/93, loss: 0.0190\n",
      "Epoch 132, batch 71/93, loss: 0.0391\n",
      "Epoch 132, batch 76/93, loss: 0.0557\n",
      "Epoch 132, batch 81/93, loss: 0.0059\n",
      "Epoch 132, batch 86/93, loss: 0.0551\n",
      "Epoch 132, batch 91/93, loss: 0.0210\n",
      "Epoch 132, train loss: 0.0450, train accuracy: 98.53%\n",
      "Epoch 132, val loss: 0.7357, val accuracy: 85.03%\n",
      "Epoch 133, batch 1/93, loss: 0.0427\n",
      "Epoch 133, batch 6/93, loss: 0.0316\n",
      "Epoch 133, batch 11/93, loss: 0.0190\n",
      "Epoch 133, batch 16/93, loss: 0.0254\n",
      "Epoch 133, batch 21/93, loss: 0.0465\n",
      "Epoch 133, batch 26/93, loss: 0.0442\n",
      "Epoch 133, batch 31/93, loss: 0.0242\n",
      "Epoch 133, batch 36/93, loss: 0.0067\n",
      "Epoch 133, batch 41/93, loss: 0.0275\n",
      "Epoch 133, batch 46/93, loss: 0.0404\n",
      "Epoch 133, batch 51/93, loss: 0.0347\n",
      "Epoch 133, batch 56/93, loss: 0.0390\n",
      "Epoch 133, batch 61/93, loss: 0.0244\n",
      "Epoch 133, batch 66/93, loss: 0.0130\n",
      "Epoch 133, batch 71/93, loss: 0.0288\n",
      "Epoch 133, batch 76/93, loss: 0.0553\n",
      "Epoch 133, batch 81/93, loss: 0.0130\n",
      "Epoch 133, batch 86/93, loss: 0.0236\n",
      "Epoch 133, batch 91/93, loss: 0.0199\n",
      "Epoch 133, train loss: 0.0417, train accuracy: 99.05%\n",
      "Epoch 133, val loss: 0.8216, val accuracy: 84.64%\n",
      "Epoch 134, batch 1/93, loss: 0.0461\n",
      "Epoch 134, batch 6/93, loss: 0.1436\n",
      "Epoch 134, batch 11/93, loss: 0.1758\n",
      "Epoch 134, batch 16/93, loss: 0.1923\n",
      "Epoch 134, batch 21/93, loss: 0.3696\n",
      "Epoch 134, batch 26/93, loss: 0.1170\n",
      "Epoch 134, batch 31/93, loss: 0.0737\n",
      "Epoch 134, batch 36/93, loss: 0.0621\n",
      "Epoch 134, batch 41/93, loss: 0.1008\n",
      "Epoch 134, batch 46/93, loss: 0.0486\n",
      "Epoch 134, batch 51/93, loss: 0.1418\n",
      "Epoch 134, batch 56/93, loss: 0.1315\n",
      "Epoch 134, batch 61/93, loss: 0.0552\n",
      "Epoch 134, batch 66/93, loss: 0.0798\n",
      "Epoch 134, batch 71/93, loss: 0.1450\n",
      "Epoch 134, batch 76/93, loss: 0.0301\n",
      "Epoch 134, batch 81/93, loss: 0.1333\n",
      "Epoch 134, batch 86/93, loss: 0.0301\n",
      "Epoch 134, batch 91/93, loss: 0.0123\n",
      "Epoch 134, train loss: 0.1111, train accuracy: 96.21%\n",
      "Epoch 134, val loss: 0.7362, val accuracy: 84.81%\n",
      "Epoch 135, batch 1/93, loss: 0.0273\n",
      "Epoch 135, batch 6/93, loss: 0.0314\n",
      "Epoch 135, batch 11/93, loss: 0.0559\n",
      "Epoch 135, batch 16/93, loss: 0.0196\n",
      "Epoch 135, batch 21/93, loss: 0.0343\n",
      "Epoch 135, batch 26/93, loss: 0.0458\n",
      "Epoch 135, batch 31/93, loss: 0.0278\n",
      "Epoch 135, batch 36/93, loss: 0.0174\n",
      "Epoch 135, batch 41/93, loss: 0.0730\n",
      "Epoch 135, batch 46/93, loss: 0.0529\n",
      "Epoch 135, batch 51/93, loss: 0.0129\n",
      "Epoch 135, batch 56/93, loss: 0.0247\n",
      "Epoch 135, batch 61/93, loss: 0.0580\n",
      "Epoch 135, batch 66/93, loss: 0.0126\n",
      "Epoch 135, batch 71/93, loss: 0.0257\n",
      "Epoch 135, batch 76/93, loss: 0.0126\n",
      "Epoch 135, batch 81/93, loss: 0.0370\n",
      "Epoch 135, batch 86/93, loss: 0.0235\n",
      "Epoch 135, batch 91/93, loss: 0.0228\n",
      "Epoch 135, train loss: 0.0327, train accuracy: 99.10%\n",
      "Epoch 135, val loss: 0.7240, val accuracy: 84.68%\n",
      "Epoch 136, batch 1/93, loss: 0.0447\n",
      "Epoch 136, batch 6/93, loss: 0.0389\n",
      "Epoch 136, batch 11/93, loss: 0.0078\n",
      "Epoch 136, batch 16/93, loss: 0.0175\n",
      "Epoch 136, batch 21/93, loss: 0.0099\n",
      "Epoch 136, batch 26/93, loss: 0.0226\n",
      "Epoch 136, batch 31/93, loss: 0.0158\n",
      "Epoch 136, batch 36/93, loss: 0.0122\n",
      "Epoch 136, batch 41/93, loss: 0.0108\n",
      "Epoch 136, batch 46/93, loss: 0.0540\n",
      "Epoch 136, batch 51/93, loss: 0.0210\n",
      "Epoch 136, batch 56/93, loss: 0.0159\n",
      "Epoch 136, batch 61/93, loss: 0.0366\n",
      "Epoch 136, batch 66/93, loss: 0.0210\n",
      "Epoch 136, batch 71/93, loss: 0.0207\n",
      "Epoch 136, batch 76/93, loss: 0.0594\n",
      "Epoch 136, batch 81/93, loss: 0.0283\n",
      "Epoch 136, batch 86/93, loss: 0.0293\n",
      "Epoch 136, batch 91/93, loss: 0.0183\n",
      "Epoch 136, train loss: 0.0202, train accuracy: 99.66%\n",
      "Epoch 136, val loss: 0.8082, val accuracy: 85.03%\n",
      "Epoch 137, batch 1/93, loss: 0.0152\n",
      "Epoch 137, batch 6/93, loss: 0.0127\n",
      "Epoch 137, batch 11/93, loss: 0.0067\n",
      "Epoch 137, batch 16/93, loss: 0.0185\n",
      "Epoch 137, batch 21/93, loss: 0.0096\n",
      "Epoch 137, batch 26/93, loss: 0.0104\n",
      "Epoch 137, batch 31/93, loss: 0.0224\n",
      "Epoch 137, batch 36/93, loss: 0.0167\n",
      "Epoch 137, batch 41/93, loss: 0.0303\n",
      "Epoch 137, batch 46/93, loss: 0.0346\n",
      "Epoch 137, batch 51/93, loss: 0.0198\n",
      "Epoch 137, batch 56/93, loss: 0.0571\n",
      "Epoch 137, batch 61/93, loss: 0.0152\n",
      "Epoch 137, batch 66/93, loss: 0.0299\n",
      "Epoch 137, batch 71/93, loss: 0.0057\n",
      "Epoch 137, batch 76/93, loss: 0.0323\n",
      "Epoch 137, batch 81/93, loss: 0.0133\n",
      "Epoch 137, batch 86/93, loss: 0.0200\n",
      "Epoch 137, batch 91/93, loss: 0.0334\n",
      "Epoch 137, train loss: 0.0245, train accuracy: 99.40%\n",
      "Epoch 137, val loss: 0.6929, val accuracy: 86.07%\n",
      "Epoch 138, batch 1/93, loss: 0.0334\n",
      "Epoch 138, batch 6/93, loss: 0.0092\n",
      "Epoch 138, batch 11/93, loss: 0.0041\n",
      "Epoch 138, batch 16/93, loss: 0.0173\n",
      "Epoch 138, batch 21/93, loss: 0.0125\n",
      "Epoch 138, batch 26/93, loss: 0.0220\n",
      "Epoch 138, batch 31/93, loss: 0.0154\n",
      "Epoch 138, batch 36/93, loss: 0.0077\n",
      "Epoch 138, batch 41/93, loss: 0.0237\n",
      "Epoch 138, batch 46/93, loss: 0.0232\n",
      "Epoch 138, batch 51/93, loss: 0.0195\n",
      "Epoch 138, batch 56/93, loss: 0.0154\n",
      "Epoch 138, batch 61/93, loss: 0.0042\n",
      "Epoch 138, batch 66/93, loss: 0.0154\n",
      "Epoch 138, batch 71/93, loss: 0.0183\n",
      "Epoch 138, batch 76/93, loss: 0.0168\n",
      "Epoch 138, batch 81/93, loss: 0.0188\n",
      "Epoch 138, batch 86/93, loss: 0.0087\n",
      "Epoch 138, batch 91/93, loss: 0.0241\n",
      "Epoch 138, train loss: 0.0231, train accuracy: 99.50%\n",
      "Epoch 138, val loss: 0.7266, val accuracy: 85.59%\n",
      "Epoch 139, batch 1/93, loss: 0.0168\n",
      "Epoch 139, batch 6/93, loss: 0.1385\n",
      "Epoch 139, batch 11/93, loss: 0.1866\n",
      "Epoch 139, batch 16/93, loss: 0.0969\n",
      "Epoch 139, batch 21/93, loss: 0.0589\n",
      "Epoch 139, batch 26/93, loss: 0.0314\n",
      "Epoch 139, batch 31/93, loss: 0.0371\n",
      "Epoch 139, batch 36/93, loss: 0.0269\n",
      "Epoch 139, batch 41/93, loss: 0.0278\n",
      "Epoch 139, batch 46/93, loss: 0.1037\n",
      "Epoch 139, batch 51/93, loss: 0.1110\n",
      "Epoch 139, batch 56/93, loss: 0.0579\n",
      "Epoch 139, batch 61/93, loss: 0.0103\n",
      "Epoch 139, batch 66/93, loss: 0.0645\n",
      "Epoch 139, batch 71/93, loss: 0.0246\n",
      "Epoch 139, batch 76/93, loss: 0.0263\n",
      "Epoch 139, batch 81/93, loss: 0.0179\n",
      "Epoch 139, batch 86/93, loss: 0.0352\n",
      "Epoch 139, batch 91/93, loss: 0.0128\n",
      "Epoch 139, train loss: 0.0549, train accuracy: 98.19%\n",
      "Epoch 139, val loss: 0.7375, val accuracy: 85.50%\n",
      "Epoch 140, batch 1/93, loss: 0.0503\n",
      "Epoch 140, batch 6/93, loss: 0.0125\n",
      "Epoch 140, batch 11/93, loss: 0.0521\n",
      "Epoch 140, batch 16/93, loss: 0.0436\n",
      "Epoch 140, batch 21/93, loss: 0.0754\n",
      "Epoch 140, batch 26/93, loss: 0.0220\n",
      "Epoch 140, batch 31/93, loss: 0.0280\n",
      "Epoch 140, batch 36/93, loss: 0.0116\n",
      "Epoch 140, batch 41/93, loss: 0.0119\n",
      "Epoch 140, batch 46/93, loss: 0.0148\n",
      "Epoch 140, batch 51/93, loss: 0.0082\n",
      "Epoch 140, batch 56/93, loss: 0.0172\n",
      "Epoch 140, batch 61/93, loss: 0.0185\n",
      "Epoch 140, batch 66/93, loss: 0.0201\n",
      "Epoch 140, batch 71/93, loss: 0.0167\n",
      "Epoch 140, batch 76/93, loss: 0.0226\n",
      "Epoch 140, batch 81/93, loss: 0.0333\n",
      "Epoch 140, batch 86/93, loss: 0.0048\n",
      "Epoch 140, batch 91/93, loss: 0.0116\n",
      "Epoch 140, train loss: 0.0272, train accuracy: 99.32%\n",
      "Epoch 140, val loss: 0.7658, val accuracy: 85.16%\n",
      "Epoch 141, batch 1/93, loss: 0.0448\n",
      "Epoch 141, batch 6/93, loss: 0.0542\n",
      "Epoch 141, batch 11/93, loss: 0.0105\n",
      "Epoch 141, batch 16/93, loss: 0.0263\n",
      "Epoch 141, batch 21/93, loss: 0.0181\n",
      "Epoch 141, batch 26/93, loss: 0.0214\n",
      "Epoch 141, batch 31/93, loss: 0.0358\n",
      "Epoch 141, batch 36/93, loss: 0.0127\n",
      "Epoch 141, batch 41/93, loss: 0.0086\n",
      "Epoch 141, batch 46/93, loss: 0.0288\n",
      "Epoch 141, batch 51/93, loss: 0.0133\n",
      "Epoch 141, batch 56/93, loss: 0.0121\n",
      "Epoch 141, batch 61/93, loss: 0.0208\n",
      "Epoch 141, batch 66/93, loss: 0.0146\n",
      "Epoch 141, batch 71/93, loss: 0.0174\n",
      "Epoch 141, batch 76/93, loss: 0.0276\n",
      "Epoch 141, batch 81/93, loss: 0.0143\n",
      "Epoch 141, batch 86/93, loss: 0.0392\n",
      "Epoch 141, batch 91/93, loss: 0.0112\n",
      "Epoch 141, train loss: 0.0242, train accuracy: 99.45%\n",
      "Epoch 141, val loss: 0.7714, val accuracy: 85.11%\n",
      "Epoch 142, batch 1/93, loss: 0.0087\n",
      "Epoch 142, batch 6/93, loss: 0.0435\n",
      "Epoch 142, batch 11/93, loss: 0.0124\n",
      "Epoch 142, batch 16/93, loss: 0.0307\n",
      "Epoch 142, batch 21/93, loss: 0.0464\n",
      "Epoch 142, batch 26/93, loss: 0.0334\n",
      "Epoch 142, batch 31/93, loss: 0.0132\n",
      "Epoch 142, batch 36/93, loss: 0.0351\n",
      "Epoch 142, batch 41/93, loss: 0.0165\n",
      "Epoch 142, batch 46/93, loss: 0.0291\n",
      "Epoch 142, batch 51/93, loss: 0.0365\n",
      "Epoch 142, batch 56/93, loss: 0.0953\n",
      "Epoch 142, batch 61/93, loss: 0.0046\n",
      "Epoch 142, batch 66/93, loss: 0.0078\n",
      "Epoch 142, batch 71/93, loss: 0.0207\n",
      "Epoch 142, batch 76/93, loss: 0.0045\n",
      "Epoch 142, batch 81/93, loss: 0.0104\n",
      "Epoch 142, batch 86/93, loss: 0.0056\n",
      "Epoch 142, batch 91/93, loss: 0.0085\n",
      "Epoch 142, train loss: 0.0207, train accuracy: 99.54%\n",
      "Epoch 142, val loss: 0.7731, val accuracy: 85.29%\n",
      "Epoch 143, batch 1/93, loss: 0.0346\n",
      "Epoch 143, batch 6/93, loss: 0.0191\n",
      "Epoch 143, batch 11/93, loss: 0.0294\n",
      "Epoch 143, batch 16/93, loss: 0.0170\n",
      "Epoch 143, batch 21/93, loss: 0.0205\n",
      "Epoch 143, batch 26/93, loss: 0.0260\n",
      "Epoch 143, batch 31/93, loss: 0.0172\n",
      "Epoch 143, batch 36/93, loss: 0.0125\n",
      "Epoch 143, batch 41/93, loss: 0.0240\n",
      "Epoch 143, batch 46/93, loss: 0.0092\n",
      "Epoch 143, batch 51/93, loss: 0.0135\n",
      "Epoch 143, batch 56/93, loss: 0.0142\n",
      "Epoch 143, batch 61/93, loss: 0.0144\n",
      "Epoch 143, batch 66/93, loss: 0.0066\n",
      "Epoch 143, batch 71/93, loss: 0.0249\n",
      "Epoch 143, batch 76/93, loss: 0.0162\n",
      "Epoch 143, batch 81/93, loss: 0.0314\n",
      "Epoch 143, batch 86/93, loss: 0.0278\n",
      "Epoch 143, batch 91/93, loss: 0.0288\n",
      "Epoch 143, train loss: 0.0175, train accuracy: 99.56%\n",
      "Epoch 143, val loss: 0.7808, val accuracy: 86.07%\n",
      "Epoch 144, batch 1/93, loss: 0.0111\n",
      "Epoch 144, batch 6/93, loss: 0.0053\n",
      "Epoch 144, batch 11/93, loss: 0.0080\n",
      "Epoch 144, batch 16/93, loss: 0.0122\n",
      "Epoch 144, batch 21/93, loss: 0.0036\n",
      "Epoch 144, batch 26/93, loss: 0.0036\n",
      "Epoch 144, batch 31/93, loss: 0.0084\n",
      "Epoch 144, batch 36/93, loss: 0.0036\n",
      "Epoch 144, batch 41/93, loss: 0.0082\n",
      "Epoch 144, batch 46/93, loss: 0.0064\n",
      "Epoch 144, batch 51/93, loss: 0.0153\n",
      "Epoch 144, batch 56/93, loss: 0.0097\n",
      "Epoch 144, batch 61/93, loss: 0.0057\n",
      "Epoch 144, batch 66/93, loss: 0.0495\n",
      "Epoch 144, batch 71/93, loss: 0.0080\n",
      "Epoch 144, batch 76/93, loss: 0.0084\n",
      "Epoch 144, batch 81/93, loss: 0.0095\n",
      "Epoch 144, batch 86/93, loss: 0.0113\n",
      "Epoch 144, batch 91/93, loss: 0.0038\n",
      "Epoch 144, train loss: 0.0154, train accuracy: 99.71%\n",
      "Epoch 144, val loss: 0.7568, val accuracy: 85.81%\n",
      "Epoch 145, batch 1/93, loss: 0.0097\n",
      "Epoch 145, batch 6/93, loss: 0.0709\n",
      "Epoch 145, batch 11/93, loss: 0.0434\n",
      "Epoch 145, batch 16/93, loss: 0.0110\n",
      "Epoch 145, batch 21/93, loss: 0.0840\n",
      "Epoch 145, batch 26/93, loss: 0.0051\n",
      "Epoch 145, batch 31/93, loss: 0.0241\n",
      "Epoch 145, batch 36/93, loss: 0.0422\n",
      "Epoch 145, batch 41/93, loss: 0.0448\n",
      "Epoch 145, batch 46/93, loss: 0.0065\n",
      "Epoch 145, batch 51/93, loss: 0.0161\n",
      "Epoch 145, batch 56/93, loss: 0.0367\n",
      "Epoch 145, batch 61/93, loss: 0.0115\n",
      "Epoch 145, batch 66/93, loss: 0.0083\n",
      "Epoch 145, batch 71/93, loss: 0.0079\n",
      "Epoch 145, batch 76/93, loss: 0.0073\n",
      "Epoch 145, batch 81/93, loss: 0.0122\n",
      "Epoch 145, batch 86/93, loss: 0.0112\n",
      "Epoch 145, batch 91/93, loss: 0.0125\n",
      "Epoch 145, train loss: 0.0221, train accuracy: 99.51%\n",
      "Epoch 145, val loss: 0.7256, val accuracy: 85.85%\n",
      "Epoch 146, batch 1/93, loss: 0.0088\n",
      "Epoch 146, batch 6/93, loss: 0.0315\n",
      "Epoch 146, batch 11/93, loss: 0.0190\n",
      "Epoch 146, batch 16/93, loss: 0.0103\n",
      "Epoch 146, batch 21/93, loss: 0.0065\n",
      "Epoch 146, batch 26/93, loss: 0.0050\n",
      "Epoch 146, batch 31/93, loss: 0.0338\n",
      "Epoch 146, batch 36/93, loss: 0.0409\n",
      "Epoch 146, batch 41/93, loss: 0.0057\n",
      "Epoch 146, batch 46/93, loss: 0.0230\n",
      "Epoch 146, batch 51/93, loss: 0.0143\n",
      "Epoch 146, batch 56/93, loss: 0.0173\n",
      "Epoch 146, batch 61/93, loss: 0.0193\n",
      "Epoch 146, batch 66/93, loss: 0.0118\n",
      "Epoch 146, batch 71/93, loss: 0.0145\n",
      "Epoch 146, batch 76/93, loss: 0.0226\n",
      "Epoch 146, batch 81/93, loss: 0.0116\n",
      "Epoch 146, batch 86/93, loss: 0.0282\n",
      "Epoch 146, batch 91/93, loss: 0.0076\n",
      "Epoch 146, train loss: 0.0207, train accuracy: 99.54%\n",
      "Epoch 146, val loss: 0.7121, val accuracy: 85.85%\n",
      "Epoch 147, batch 1/93, loss: 0.0028\n",
      "Epoch 147, batch 6/93, loss: 0.0405\n",
      "Epoch 147, batch 11/93, loss: 0.1264\n",
      "Epoch 147, batch 16/93, loss: 0.0478\n",
      "Epoch 147, batch 21/93, loss: 0.0478\n",
      "Epoch 147, batch 26/93, loss: 0.0196\n",
      "Epoch 147, batch 31/93, loss: 0.0099\n",
      "Epoch 147, batch 36/93, loss: 0.0237\n",
      "Epoch 147, batch 41/93, loss: 0.0597\n",
      "Epoch 147, batch 46/93, loss: 0.0606\n",
      "Epoch 147, batch 51/93, loss: 0.0330\n",
      "Epoch 147, batch 56/93, loss: 0.0590\n",
      "Epoch 147, batch 61/93, loss: 0.0057\n",
      "Epoch 147, batch 66/93, loss: 0.0197\n",
      "Epoch 147, batch 71/93, loss: 0.0165\n",
      "Epoch 147, batch 76/93, loss: 0.0136\n",
      "Epoch 147, batch 81/93, loss: 0.0049\n",
      "Epoch 147, batch 86/93, loss: 0.0066\n",
      "Epoch 147, batch 91/93, loss: 0.0244\n",
      "Epoch 147, train loss: 0.0375, train accuracy: 98.95%\n",
      "Epoch 147, val loss: 0.7251, val accuracy: 86.07%\n",
      "Epoch 148, batch 1/93, loss: 0.0419\n",
      "Epoch 148, batch 6/93, loss: 0.0235\n",
      "Epoch 148, batch 11/93, loss: 0.0104\n",
      "Epoch 148, batch 16/93, loss: 0.0093\n",
      "Epoch 148, batch 21/93, loss: 0.0031\n",
      "Epoch 148, batch 26/93, loss: 0.0295\n",
      "Epoch 148, batch 31/93, loss: 0.0055\n",
      "Epoch 148, batch 36/93, loss: 0.0066\n",
      "Epoch 148, batch 41/93, loss: 0.0117\n",
      "Epoch 148, batch 46/93, loss: 0.0088\n",
      "Epoch 148, batch 51/93, loss: 0.0309\n",
      "Epoch 148, batch 56/93, loss: 0.0404\n",
      "Epoch 148, batch 61/93, loss: 0.0058\n",
      "Epoch 148, batch 66/93, loss: 0.0090\n",
      "Epoch 148, batch 71/93, loss: 0.0140\n",
      "Epoch 148, batch 76/93, loss: 0.0185\n",
      "Epoch 148, batch 81/93, loss: 0.0039\n",
      "Epoch 148, batch 86/93, loss: 0.0054\n",
      "Epoch 148, batch 91/93, loss: 0.0307\n",
      "Epoch 148, train loss: 0.0152, train accuracy: 99.61%\n",
      "Epoch 148, val loss: 0.7443, val accuracy: 85.98%\n",
      "Epoch 149, batch 1/93, loss: 0.0346\n",
      "Epoch 149, batch 6/93, loss: 0.0254\n",
      "Epoch 149, batch 11/93, loss: 0.0071\n",
      "Epoch 149, batch 16/93, loss: 0.0100\n",
      "Epoch 149, batch 21/93, loss: 0.0064\n",
      "Epoch 149, batch 26/93, loss: 0.0133\n",
      "Epoch 149, batch 31/93, loss: 0.0026\n",
      "Epoch 149, batch 36/93, loss: 0.0087\n",
      "Epoch 149, batch 41/93, loss: 0.0056\n",
      "Epoch 149, batch 46/93, loss: 0.0181\n",
      "Epoch 149, batch 51/93, loss: 0.0110\n",
      "Epoch 149, batch 56/93, loss: 0.0250\n",
      "Epoch 149, batch 61/93, loss: 0.0037\n",
      "Epoch 149, batch 66/93, loss: 0.0074\n",
      "Epoch 149, batch 71/93, loss: 0.0165\n",
      "Epoch 149, batch 76/93, loss: 0.0087\n",
      "Epoch 149, batch 81/93, loss: 0.0369\n",
      "Epoch 149, batch 86/93, loss: 0.0098\n",
      "Epoch 149, batch 91/93, loss: 0.0272\n",
      "Epoch 149, train loss: 0.0156, train accuracy: 99.64%\n",
      "Epoch 149, val loss: 0.8672, val accuracy: 85.46%\n",
      "Epoch 150, batch 1/93, loss: 0.0075\n",
      "Epoch 150, batch 6/93, loss: 0.1755\n",
      "Epoch 150, batch 11/93, loss: 0.0345\n",
      "Epoch 150, batch 16/93, loss: 0.0264\n",
      "Epoch 150, batch 21/93, loss: 0.0172\n",
      "Epoch 150, batch 26/93, loss: 0.0160\n",
      "Epoch 150, batch 31/93, loss: 0.0081\n",
      "Epoch 150, batch 36/93, loss: 0.0092\n",
      "Epoch 150, batch 41/93, loss: 0.0168\n",
      "Epoch 150, batch 46/93, loss: 0.0221\n",
      "Epoch 150, batch 51/93, loss: 0.0629\n",
      "Epoch 150, batch 56/93, loss: 0.0150\n",
      "Epoch 150, batch 61/93, loss: 0.0057\n",
      "Epoch 150, batch 66/93, loss: 0.0267\n",
      "Epoch 150, batch 71/93, loss: 0.0079\n",
      "Epoch 150, batch 76/93, loss: 0.0112\n",
      "Epoch 150, batch 81/93, loss: 0.0122\n",
      "Epoch 150, batch 86/93, loss: 0.0100\n",
      "Epoch 150, batch 91/93, loss: 0.0153\n",
      "Epoch 150, train loss: 0.0266, train accuracy: 99.28%\n",
      "Epoch 150, val loss: 0.7894, val accuracy: 85.59%\n"
     ]
    }
   ],
   "source": [
    "train_losses, train_accs, val_losses, val_accs = train(model, train_loader, val_loader, criterion, optimizer, device, num_epochs=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "61b66e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"best_model_v4.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e8a4296c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_loss_accuracy2(train_losses, val_losses, train_accs, val_accs):\n",
    "    # Create figure and axes\n",
    "    fig, ax1 = plt.subplots(figsize=(8, 4))\n",
    "    ax2 = ax1.twinx()\n",
    "\n",
    "    # Plot loss curves\n",
    "    ax1.plot(train_losses, label=\"Training Loss\", color=\"blue\")\n",
    "    ax1.plot(val_losses, label=\"Validation Loss\", color=\"orange\")\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    # Plot accuracy curves\n",
    "    ax2.plot(train_accs, label=\"Training Accuracy\", color=\"green\")\n",
    "    ax2.plot(val_accs, label=\"Validation Accuracy\", color=\"red\")\n",
    "    \n",
    "    \n",
    "    ax1.grid()\n",
    "    ax2.grid()\n",
    "\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper left\")\n",
    "    ax1.set_xlabel(\"Epoch\")\n",
    "    ax2.set_ylabel(\"Accuracy (%)\")\n",
    "    ax2.legend(loc=\"upper right\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b983a7d0-4506-4a87-90b5-3713e6c088b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss_accuracy2(train_losses, val_losses, train_accs, val_accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55958b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
